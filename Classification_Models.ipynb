{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "Fs1RmPQM1VuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import  ConfusionMatrixDisplay,confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "metadata": {
        "id": "A6XvSbuBybYI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Data"
      ],
      "metadata": {
        "id": "GFwGgL1R1ZDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vOXhSvEXxaKu",
        "outputId": "d6450a54-8e13-4fea-ade2-3b24848deaee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1       2       3       4         5         6   \\\n",
              "0       28.7967   16.0021  2.6449  0.3918  0.1982   27.7004   22.0110   \n",
              "1       31.6036   11.7235  2.5185  0.5303  0.3773   26.2722   23.8238   \n",
              "2      162.0520  136.0310  4.0612  0.0374  0.0187  116.7410  -64.8580   \n",
              "3       23.8172    9.5728  2.3385  0.6147  0.3922   27.2107   -6.4633   \n",
              "4       75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277   28.5525   \n",
              "...         ...       ...     ...     ...     ...       ...       ...   \n",
              "19015   21.3846   10.9170  2.6161  0.5857  0.3934   15.2618   11.5245   \n",
              "19016   28.9452    6.7020  2.2672  0.5351  0.2784   37.0816   13.1853   \n",
              "19017   75.4455   47.5305  3.4483  0.1417  0.0549   -9.3561   41.0562   \n",
              "19018  120.5135   76.9018  3.9939  0.0944  0.0683    5.8043  -93.5224   \n",
              "19019  187.1814   53.0014  3.2093  0.2876  0.1539 -167.3125 -168.4558   \n",
              "\n",
              "            7        8         9  10  \n",
              "0      -8.2027  40.0920   81.8828  g  \n",
              "1      -9.9574   6.3609  205.2610  g  \n",
              "2     -45.2160  76.9600  256.7880  g  \n",
              "3      -7.1513  10.4490  116.7370  g  \n",
              "4      21.8393   4.6480  356.4620  g  \n",
              "...        ...      ...       ... ..  \n",
              "19015   2.8766   2.4229  106.8258  h  \n",
              "19016  -2.9632  86.7975  247.4560  h  \n",
              "19017  -9.4662  30.2987  256.5166  h  \n",
              "19018 -63.8389  84.6874  408.3166  h  \n",
              "19019  31.4755  52.7310  272.3174  h  \n",
              "\n",
              "[19020 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83ae05cd-b5e3-4179-b72c-e96ff9a778fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28.7967</td>\n",
              "      <td>16.0021</td>\n",
              "      <td>2.6449</td>\n",
              "      <td>0.3918</td>\n",
              "      <td>0.1982</td>\n",
              "      <td>27.7004</td>\n",
              "      <td>22.0110</td>\n",
              "      <td>-8.2027</td>\n",
              "      <td>40.0920</td>\n",
              "      <td>81.8828</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31.6036</td>\n",
              "      <td>11.7235</td>\n",
              "      <td>2.5185</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.3773</td>\n",
              "      <td>26.2722</td>\n",
              "      <td>23.8238</td>\n",
              "      <td>-9.9574</td>\n",
              "      <td>6.3609</td>\n",
              "      <td>205.2610</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>162.0520</td>\n",
              "      <td>136.0310</td>\n",
              "      <td>4.0612</td>\n",
              "      <td>0.0374</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>116.7410</td>\n",
              "      <td>-64.8580</td>\n",
              "      <td>-45.2160</td>\n",
              "      <td>76.9600</td>\n",
              "      <td>256.7880</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.8172</td>\n",
              "      <td>9.5728</td>\n",
              "      <td>2.3385</td>\n",
              "      <td>0.6147</td>\n",
              "      <td>0.3922</td>\n",
              "      <td>27.2107</td>\n",
              "      <td>-6.4633</td>\n",
              "      <td>-7.1513</td>\n",
              "      <td>10.4490</td>\n",
              "      <td>116.7370</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>75.1362</td>\n",
              "      <td>30.9205</td>\n",
              "      <td>3.1611</td>\n",
              "      <td>0.3168</td>\n",
              "      <td>0.1832</td>\n",
              "      <td>-5.5277</td>\n",
              "      <td>28.5525</td>\n",
              "      <td>21.8393</td>\n",
              "      <td>4.6480</td>\n",
              "      <td>356.4620</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19015</th>\n",
              "      <td>21.3846</td>\n",
              "      <td>10.9170</td>\n",
              "      <td>2.6161</td>\n",
              "      <td>0.5857</td>\n",
              "      <td>0.3934</td>\n",
              "      <td>15.2618</td>\n",
              "      <td>11.5245</td>\n",
              "      <td>2.8766</td>\n",
              "      <td>2.4229</td>\n",
              "      <td>106.8258</td>\n",
              "      <td>h</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19016</th>\n",
              "      <td>28.9452</td>\n",
              "      <td>6.7020</td>\n",
              "      <td>2.2672</td>\n",
              "      <td>0.5351</td>\n",
              "      <td>0.2784</td>\n",
              "      <td>37.0816</td>\n",
              "      <td>13.1853</td>\n",
              "      <td>-2.9632</td>\n",
              "      <td>86.7975</td>\n",
              "      <td>247.4560</td>\n",
              "      <td>h</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19017</th>\n",
              "      <td>75.4455</td>\n",
              "      <td>47.5305</td>\n",
              "      <td>3.4483</td>\n",
              "      <td>0.1417</td>\n",
              "      <td>0.0549</td>\n",
              "      <td>-9.3561</td>\n",
              "      <td>41.0562</td>\n",
              "      <td>-9.4662</td>\n",
              "      <td>30.2987</td>\n",
              "      <td>256.5166</td>\n",
              "      <td>h</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19018</th>\n",
              "      <td>120.5135</td>\n",
              "      <td>76.9018</td>\n",
              "      <td>3.9939</td>\n",
              "      <td>0.0944</td>\n",
              "      <td>0.0683</td>\n",
              "      <td>5.8043</td>\n",
              "      <td>-93.5224</td>\n",
              "      <td>-63.8389</td>\n",
              "      <td>84.6874</td>\n",
              "      <td>408.3166</td>\n",
              "      <td>h</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19019</th>\n",
              "      <td>187.1814</td>\n",
              "      <td>53.0014</td>\n",
              "      <td>3.2093</td>\n",
              "      <td>0.2876</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>-167.3125</td>\n",
              "      <td>-168.4558</td>\n",
              "      <td>31.4755</td>\n",
              "      <td>52.7310</td>\n",
              "      <td>272.3174</td>\n",
              "      <td>h</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19020 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83ae05cd-b5e3-4179-b72c-e96ff9a778fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83ae05cd-b5e3-4179-b72c-e96ff9a778fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83ae05cd-b5e3-4179-b72c-e96ff9a778fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.read_csv('magic04.csv', header=None)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['fLength',\n",
        "              'fWidth',\n",
        "              'fSize',\n",
        "              'fConc',\n",
        "              'fConc1', \n",
        "              'fAsym',\n",
        "              'fM3Long',\n",
        "              'fM3Trans',\n",
        "              'fAlpha',\n",
        "              'fDist',\n",
        "              'classified']\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uikzZsZjyYuq",
        "outputId": "16a6577e-a821-458c-80f5-f42ff75106f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM3Long  fM3Trans  \\\n",
              "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
              "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
              "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
              "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
              "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
              "\n",
              "    fAlpha     fDist classified  \n",
              "0  40.0920   81.8828          g  \n",
              "1   6.3609  205.2610          g  \n",
              "2  76.9600  256.7880          g  \n",
              "3  10.4490  116.7370          g  \n",
              "4   4.6480  356.4620          g  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc4c6a6f-5e8d-4131-9400-c3c7ad00d815\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fLength</th>\n",
              "      <th>fWidth</th>\n",
              "      <th>fSize</th>\n",
              "      <th>fConc</th>\n",
              "      <th>fConc1</th>\n",
              "      <th>fAsym</th>\n",
              "      <th>fM3Long</th>\n",
              "      <th>fM3Trans</th>\n",
              "      <th>fAlpha</th>\n",
              "      <th>fDist</th>\n",
              "      <th>classified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28.7967</td>\n",
              "      <td>16.0021</td>\n",
              "      <td>2.6449</td>\n",
              "      <td>0.3918</td>\n",
              "      <td>0.1982</td>\n",
              "      <td>27.7004</td>\n",
              "      <td>22.0110</td>\n",
              "      <td>-8.2027</td>\n",
              "      <td>40.0920</td>\n",
              "      <td>81.8828</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31.6036</td>\n",
              "      <td>11.7235</td>\n",
              "      <td>2.5185</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.3773</td>\n",
              "      <td>26.2722</td>\n",
              "      <td>23.8238</td>\n",
              "      <td>-9.9574</td>\n",
              "      <td>6.3609</td>\n",
              "      <td>205.2610</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>162.0520</td>\n",
              "      <td>136.0310</td>\n",
              "      <td>4.0612</td>\n",
              "      <td>0.0374</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>116.7410</td>\n",
              "      <td>-64.8580</td>\n",
              "      <td>-45.2160</td>\n",
              "      <td>76.9600</td>\n",
              "      <td>256.7880</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.8172</td>\n",
              "      <td>9.5728</td>\n",
              "      <td>2.3385</td>\n",
              "      <td>0.6147</td>\n",
              "      <td>0.3922</td>\n",
              "      <td>27.2107</td>\n",
              "      <td>-6.4633</td>\n",
              "      <td>-7.1513</td>\n",
              "      <td>10.4490</td>\n",
              "      <td>116.7370</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>75.1362</td>\n",
              "      <td>30.9205</td>\n",
              "      <td>3.1611</td>\n",
              "      <td>0.3168</td>\n",
              "      <td>0.1832</td>\n",
              "      <td>-5.5277</td>\n",
              "      <td>28.5525</td>\n",
              "      <td>21.8393</td>\n",
              "      <td>4.6480</td>\n",
              "      <td>356.4620</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc4c6a6f-5e8d-4131-9400-c3c7ad00d815')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc4c6a6f-5e8d-4131-9400-c3c7ad00d815 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc4c6a6f-5e8d-4131-9400-c3c7ad00d815');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.classified.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWrUql5PyarU",
        "outputId": "1d18b5a1-1fac-449e-d42b-63422fe070a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "g    12332\n",
              "h     6688\n",
              "Name: classified, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Balancing Data"
      ],
      "metadata": {
        "id": "EV2d9uO11Shy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.groupby('classified')\n",
        "df = df.apply(lambda x: x.sample(df.size().min()).reset_index(drop=True))\n",
        "df.classified.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqVXwYBUyaur",
        "outputId": "1482a23b-628a-4385-a39e-90ab0f169315"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "g    6688\n",
              "h    6688\n",
              "Name: classified, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Splitting\n"
      ],
      "metadata": {
        "id": "jASrBVewNh7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['classified'])\n",
        "y= df['classified']"
      ],
      "metadata": {
        "id": "CS8rW73t14za"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = None)"
      ],
      "metadata": {
        "id": "7kHMRiIc2dAP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(X_test, y_test, model):\n",
        "    y_pred= model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    return y_pred, accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "yjdhQ8Uo33_L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Trees"
      ],
      "metadata": {
        "id": "umJHt9A22623"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtree = DecisionTreeClassifier()\n",
        "dtree.fit(X_train,y_train)\n",
        "dtree_y_pred, dtree_accuracy, dtree_precision, dtree_recall, dtree_f1 = evaluate(X_test,y_test, dtree)\n",
        "print(\"Decision Trees Accuracy: \", dtree_accuracy*100, \"%\")\n",
        "print(\"Decision Trees Precision: \", dtree_precision*100, \"%\")\n",
        "print(\"Decision Trees Recall: \", dtree_recall*100, \"%\")\n",
        "print(\"Decision Trees F1 score: \", dtree_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, dtree_y_pred))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "LEIW_LtL238I",
        "outputId": "4d04a2c1-ea13-4423-dd1c-176397b0674f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Trees Accuracy:  79.11786693246947 %\n",
            "Decision Trees Precision:  79.1671432473046 %\n",
            "Decision Trees Recall:  79.11786693246947 %\n",
            "Decision Trees F1 score:  79.11702667239217 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(dtree, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXI3S4TGFoME",
        "outputId": "f6107dde-e0b0-43c6-fd18-48438cebda92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7923118  0.79391351 0.78804058 0.79166667 0.78365385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes"
      ],
      "metadata": {
        "id": "zGtQGBI_-fxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Naive_Bayes = GaussianNB()\n",
        "Naive_Bayes.fit(X_train, y_train)\n",
        "naive_y_pred, naive_accuracy, naive_precision, naive_recall, naive_f1 = evaluate(X_test,y_test, Naive_Bayes)\n",
        "print(\"Naive Bayes Accuracy: \", naive_accuracy*100, \"%\")\n",
        "print(\"Naive Bayes Precision: \", naive_precision*100, \"%\")\n",
        "print(\"Naive Bayes Recall: \", naive_recall*100, \"%\")\n",
        "print(\"Naive Bayes F1 score: \", naive_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, naive_y_pred))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "mpEWGMS3-fa6",
        "outputId": "7b4fa916-a775-4d55-83ce-979febaeebf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy:  64.86419137802143 %\n",
            "Naive Bayes Precision:  69.95781423888899 %\n",
            "Naive Bayes Recall:  64.86419137802143 %\n",
            "Naive Bayes F1 score:  62.32665365135354 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEGCAYAAAAT05LOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxfVX3/8dc7k53sBEI2TZAADSiLERA0DaAQqP3F9kERxJoibUABQbqhtkZRrCsIrcamkB/gwiqWVDExIAguLEGUkkBgZMlCFrKHhCwz8+kf90zyzSQz872T7zez3PfTx33k3nPP995ziY9PzrnnnnMUEZiZFU239i6AmVl7cPAzs0Jy8DOzQnLwM7NCcvAzs0Lq3t4FKDV0SE2MGd2jvYthObzwTN/2LoLlsJXNbI9t2pdrnHnqAbFmbX1ZeZ96ZtvciJi8L/erlg4V/MaM7sETc0e3dzEshzNHHNveRbAcHo8H9/kaa9bW88Tct5SVt2b4i0NbOi9pFvABYFVEHJ3SjgW+C/QG6oBPRMQTkgTcAJwNbAH+JiJ+l34zFfiXdNkvRcStrZXNzV4zyyWAhjL/V4ZbgKY1w68BX4iIY4HPpWOAs4BxaZsGzACQNASYDpwInABMlzS4tRs7+JlZLkGwI+rL2lq9VsQjwNo9bgED0v5A4LW0PwW4LTKPAYMkDQfOBOZFxNqIWAfMY8+AuocO1ew1s86hzFodwFBJ80uOZ0bEzFZ+cyUwV9I3yCpoJ6f0kcCSknxLU1pz6S1y8DOzXIKgvvxhsasjYkLOW3wc+FRE/EjSucDNwPtyXqNVbvaaWW4NRFlbG00F7k37d5O9xwNYBpT2iI5Kac2lt8jBz8xyCaCeKGtro9eAP037pwEvpv3ZwEeVOQnYEBHLgbnAGZIGp46OM1Jai9zsNbPc9qFWtxtJtwOTyN4NLiXrtf074AZJ3YGtZD27APeTfeZSS/apy4UAEbFW0heBJ1O+ayKiaSfKHhz8zCyXAHZUaCq8iDi/mVPv3EveAC5t5jqzgFl57u3gZ2a5xL41aTsMBz8zyyegvvPHPgc/M8snG+HR+Tn4mVlOop59mhuhQ3DwM7Ncsg4PBz8zK5jsOz8HPzMroAbX/MysaFzzM7NCCkR9FxgZ6+BnZrm52WtmhROI7VHT3sXYZw5+ZpZL9pGzm71mVkDu8DCzwokQ9eGan5kVUINrfmZWNFmHR+cPHZ3/Ccxsv3KHh5kVVn0X+M6v84dvM9uvGkd4lLO1RtIsSaskPdsk/XJJz0taIOlrJemfllQraZGkM0vSJ6e0WklXl/McrvmZWW4NlevtvQX4D+C2xgRJpwJTgGMiYpukg1P6eOA84ChgBPCApMPTz74NvJ9swfInJc2OiIUt3djBz8xyySY2qEzwi4hHJI1pkvxx4CsRsS3lWZXSpwB3pPSXJdWya03f2oh4CUDSHSlvi8HPzV4zyyUQO6KmrI1sScr5Jdu01q4PHA68V9Ljkn4p6V0pfSSwpCTf0pTWXHqLXPMzs1wiyPOR8+qImJDzFt2BIcBJwLuAuyQdmvMaZd3EzCwHVfsj56XAvWmd3ickNQBDgWXA6JJ8o1IaLaQ3y81eM8slyGp+5Wxt9N/AqQCpQ6MnsBqYDZwnqZekscA44AngSWCcpLGSepJ1isxu7Sau+ZlZbpXq8JB0OzCJ7N3gUmA6MAuYlT5/2Q5MTbXABZLuIuvIqAMujYj6dJ3LgLlADTArIha0dm8HPzPLJVDFJjONiPObOfWRZvJfC1y7l/T7gfvz3NvBz8xyyZau7Pyho/M/gZntZ1603MwKKKjoCI924+BnZrm55mdmhRMh1/zMrHiyDg+v3mZmheM1PMysgLIOD7/zM7MCqtQIj/bk4GdmuVRyhEd7cvAzs9y8gJGZFU4E7Ghw8DOzgsmavQ5+ZlZAHuFRUN/81Ggef2AAg4bWMfOhRQD88dk+3Hj1KLZv7UZN9+Cyf1vKkcdt4TdzBnDb14cjQU334JIvLOPoEzcDMO+uwfzwhkMA+PAVK3j/ueva7ZmK5KrrFnPi+zaxfnV3Lj7tCAAOHf8ml39lKX0OaGDl0p589dK3sOWNGo44dgtXfD1bHkLA9755CL+ZM7AdS9/+usqnLlWtu7ZlLc3O4IwPreXaH7y0W9pNXxrOR65awYwHFvHRf1zOzV8aAcBx732DGQ8sYsYDi7jqusVc/w/ZbNsb19Xw/esO4YafvMCNP32B7193CJvWd/6v5juDn985hM9eMHa3tCu/sYRZXx7OJacfwa9/NoBzPp4tGPbKot5cNvlwPvH+I/jsBYdyxdeW0q0m2qPYHUjW7C1n68iqVjpJNWRraZ4FjAfOT+tudnpvP2kz/QfX75YmweZNWfDavLGGIcN2ANDngAaU/pHcuqXbzv2nHu7P8RM3MWBwPf0H1XP8xE3Mf6j/fnuGInv28X5sWrd7o2fUodv438cOAODpR/rznj/bAMC2N7vRUJ/9pfXo1UAUPe4lDWkdj9a2jqyaofkE0lqaEbEdaFxLs0u65Jpl3PTFEVzwzvH81xdH8LHPvLbz3K9/NpCL3nsk//rRQ7nqusUArF7Rg4NG7NiZZ+jwHaxe0WO/l9syr77Qm3dP3gjAez+wYbe/myOO28zMh57nP3/xAjf+86idwbCost7emrK21kiaJWlVmrK+6bm/lxSShqZjSboxtSSfkXR8Sd6pkl5M29RynqOawa+stTQlTWtc0/P1NfVNT3caP7l1KBd/YRk/eGohF3/+Na676i07z51y1gZufvR5Pj/rZW792vB2LKU157qrRvPnU1fzH3NeoE+/euq27wpwi54+gGmnHsnlZ43jvMtX0qNXQzuWtP01fuRczlaGW4DJTRMljQbOABaXJJ9FtmjROGAaMCPlHUK29seJZJWu6ZIGt3bjdm+UR8TMiJgQERMOOrDzvvOad/cQ3nN21lSa+OfreeH3fffI8/aTNrNicU82rKlh6CE7eP21XTW91ct7MPSQHXv8xvaPJbW9+cz5b+OyyYfz8H8PZvmrPfea583NNYw5Yms7lLBjqVSzNyIeAdbu5dT1wD+R9a80mgLcFpnHgEGShgNnAvMiYm1ErAPmsZeA2lQ1g19La2x2OQcO28Ezv+0HwO9/1Y8RY7cBsOzlnjvfE734TB92bBcDhtTzzkmbeOqX/dm0voZN62t46pf9eeekTe1V/MIbeGD2D48UfPiKlfzkewcCMGz0tp0dHAeP3M7ow7aycumegbFIGnt7y6z5DW1s2aVtWmvXlzQFWBYRf2hyqrnWZFmtzKaq+anLzrU0yYLeecCHq3i//ebfPv5WnvltPzas7c4F7xzPX//9Cq78+hJmfG4k9fWiZ68GrkyfR/zqp4N44J7BdO8Ovfo08JkZryLBgMH1XHDlSi4/+3AALvjUSgYM7rzN/s7k6u+8yjve/QYDh9Tx/fkL+d43h9GnbwN//jergewd7c/vGALA0Sds5kOXvUxdnWhoEP/+mVFsXOsvxHL05K6OiAnlZpbUF/gMWZO3qhRV7L6SdDbwLXatpbnHknOlJhzTO56YO7qlLNbBnDni2PYuguXweDzIxli7Tz02g488OE6bdU5Zee89ZcZTrQU/SWOAn0TE0ZLeDjwIbEmnRwGvkb3L+wLwcETcnn63iGzN30nApIi4OKX/Z2m+5lT1n7C2rKVpZh1ftT5yjoj/BQ5uPJb0CjAhIlZLmg1cJukOss6NDRGxXNJc4MslnRxnAJ9u7V6uv5tZLpUc4SHpdrKa21BJS4HpEXFzM9nvB84GaslqhhcCRMRaSV8ke9UGcE1E7K0TZTcOfmaWW6WCX0Sc38r5MSX7AVzaTL5ZwKw893bwM7NcPJmpmRVWRx+6Vg4HPzPLJQLqPJmpmRWRm71mVjh+52dmhRUOfmZWRO7wMLPCifA7PzMrJFHv3l4zKyK/8zOzwukqq7c5+JlZPkGXWMjJwc/McnNvr5kVTrjDw8yKys1eMysk9/aaWeFEOPiZWUF1hU9dOv9bSzPb7yLK21ojaZakVZKeLUn7uqTnJT0j6ceSBpWc+7SkWkmLJJ1Zkj45pdVKurqcZ3DwM7NcAtHQ0K2srQy3AJObpM0Djo6IdwAvkFZikzSebP3vo9JvviOpRlIN8G3gLGA8cH7K2yIHPzPLLcrcWr1OxCPA2iZpP4+IunT4GNnavQBTgDsiYltEvEy2itsJaauNiJciYjtwR8rbIgc/M8sndXiUs5EtSTm/ZJuW824fA36W9kcCS0rOLU1pzaW3yB0eZpZf+d/5rY6ICW25haTPAnXAD9ry+9Y4+JlZbtX+1EXS3wAfAE5P6/UCLANGl2QbldJoIb1ZzQY/Sf9OC/E9Ij7Z2sXNrOsJoKGhesFP0mTgn4A/jYgtJadmAz+UdB0wAhgHPAEIGCdpLFnQOw/4cGv3aanmN7+NZTezriyACtX8JN0OTCJ7N7gUmE7Wu9sLmCcJ4LGIuCQiFki6C1hI1hy+NCLq03UuA+YCNcCsiFjQ2r2bDX4RcWuTQvZtEoXNrKAqNbY3Is7fS/LNLeS/Frh2L+n3A/fnuXervb2S3i1pIfB8Oj5G0nfy3MTMuphKfevSjsr51OVbwJnAGoCI+AMwsZqFMrOOrLzPXDr6+N+yensjYklqezeqr05xzKxT6OC1unKUE/yWSDoZCEk9gCuA56pbLDPrsAKiir29+0s5zd5LgEvJvph+DTg2HZtZYanMreNqteYXEauBC/ZDWcyss+gCzd5yensPlfQ/kl5PU8/cJ+nQ/VE4M+ugCtLb+0PgLmA42VfVdwO3V7NQZtaBNX7kXM7WgZUT/PpGxPcioi5t3wd6V7tgZtZxVWoy0/bU0tjeIWn3Z2lm1DvIYv6HyPkltZl1MV2gt7elDo+nyIJd41NeXHIuSLOrmlnxqIPX6srR0tjesfuzIGbWSXSCzoxylDXCQ9LRZHPj73zXFxG3VatQZtaRdfzOjHK0GvwkTSebcmY82bu+s4BfAQ5+ZkXVBWp+5fT2ngOcDqyIiAuBY4CBVS2VmXVsDWVuHVg5zd43I6JBUp2kAcAqdp8y2syKpIKTmbancoLf/LRo8H+R9QC/Afy2qqUysw6tK/T2ttrsjYhPRMT6iPgu8H5gamr+mllRVWh4m6RZadjssyVpQyTNk/Ri+nNwSpekGyXVSnpG0vElv5ma8r8oaWo5j9Bs8JN0fNMNGAJ0L72pmdk+uAWY3CTtauDBiBgHPJiOIetsHZe2acAM2DkgYzpwItkC5tMbA2ZLWmr2frOFcwGc1trF81qweQhH/dYTyHQmPS9131dnUnfnYxW5TqWavRHxiKQxTZKnkH1hAnAr8DDwzyn9trSU5WOSBkkanvLOi4i1AJLmkQXUFucgaOkj51NzPoeZFUFQ7eFtwyJiedpfAQxL+yOBJSX5lqa05tJb5EXLzSy/8mt+QyWVLoM7MyJmln2biJCq073i4GdmueUIR6sjYkLOy6+UNDwilqdm7aqUvozdP7MbldKWsauZ3Jj+cGs3KecjZzOz3VV3MtPZQGOP7VTgvpL0j6Ze35OADal5PBc4Q9Lg1NFxRkprUTnD20Q2jf2hEXGNpLcAh0TEE7kfycy6hgo1RCXdTlZrGyppKVmv7VeAuyRdBLwKnJuy3w+cDdQCW4ALASJiraQvAk+mfNc0dn60pJxm73fIBqqcBlwDbAJ+BLyrnIczs65FUdHe3vObOXX6XvIGzSyeFhGzgFl57l1O8DsxIo6X9HS6yTpJPfPcxMy6mC4+mWmjHZJqSBVdSQfR4Ycsm1k1FWJ4G3Aj8GPgYEnXkk1n9eWqlsrMOrYusHpbOev2/kDSU2RtcAEfjIjnql4yM+uYKvjOrz2V09v7FrKelf8pTYuIxdUsmJl1YEUIfsBP2bWQUW9gLLAIOKqK5TKzDkxd4K1/Oc3et5cepxldPlG1EpmZ7Qe5h7dFxO8knViNwphZJ1GEZq+kq0oOuwHHA69VrURm1rEVpcMD6F+yX0f2DvBH1SmOmXUKXT34pY+b+0fEP+yn8phZZ9CVg5+k7hFRJ+mU/VkgM+vYRNfv7X2C7P3e7yXNBu4GNjeejIh7q1w2M+uICvTOrzewhmxWl8bv/QJw8DMrqi4e/A5OPb3PsivoNeoCj25mbdYFIkBLwa8G6MfuQa9RF3h0M2urrt7sXR4R1+y3kphZ59HFg1/nn63QzCovukZvb0vz+e0xjbSZGVCx+fwkfUrSAknPSrpdUm9JYyU9LqlW0p2NM8dL6pWOa9P5MfvyCM0Gv3IWADGzYmpcx6O1rcVrSCOBTwITIuJosn6G84CvAtdHxGHAOuCi9JOLgHUp/fqUr828dKWZ5Ve5mZy7A30kdQf6AsvJPqu7J52/Ffhg2p+SjknnT0+rS7aJg5+Z5VNu4MuC31BJ80u2aTsvE7EM+AawmCzobQCeAtZHRF3KthQYmfZHAkvSb+tS/gPb+hi5p7Qys2ITuT51WR0RE/Z6nWyB8SlkEySvJxtFNrkCRSyLa35mllsl3vkB7wNejojXI2IH2aixU4BBqRkMMApYlvaXAaMhm3sAGEg2+qxNHPzMLL/KvPNbDJwkqW96d3c6sBB4CDgn5ZkK3Jf2Z6dj0vlfpIXM28TNXjPLrwIfOUfE45LuAX5HNlfo08BMsjlD75D0pZR2c/rJzcD3JNUCa8l6htvMwc/M8qngrC4RMR2Y3iT5JeCEveTdCvxVZe7s4GdmbdHFh7eZme1VVxje5uBnZrl19VldzMz2VP7ojQ7Nwc/M8nPwM7OiyTnCo8Ny8DOz3NTQ+aOfg5+Z5eN3fmZWVG72mlkxOfiZWRG55mdmxeTgZ2aF00VWb3PwM7Nc/J2fmRVX2+cQ7TAc/MwsN9f8Cmrwt5fRe/4mGgZ2Z+W3DgNg4K0r6D1/E9Fd1B/Sk7WXjSQOqKHbpjqGfH0JPf+4lS2TBrH+74YDoDfrOehfXtl5zZo1O9gycSAbPja8PR6pUN46ZB1f/ct5O49HDtrIjF++i3eMWsmYA9cD0L/XdjZt68l5N53LUSNW8q9n/xLImnzffXQCDy06tD2K3jH4I+eWSZoFfABYlRYk7jI2TxrEG2cNYciNy3ambT2mHxs+MgxqxMDvrWDAva+z4a8PIXp0Y+P5B9Nj8TZ6LN62M3/0qWHVN9+28/jgf/wjb544YL8+R1G9unYw5910LgDd1MDcT97GQ4sO5YdPHrMzz1Wn/4Y3tvUE4I+rhnDBzedQH90Y2m8zd/7tXTzywhjqo7hL4HSFDo9q/u3dwn5chm5/2n7UATT0q9ktbdux/aAmWz952+F9qVmTLTsavbux/U8OIHo0v7Zy99e20W1DHdvH961eoW2vThizjKXrBrJ8Y/+S1OD942uZsyCr1W+t67Ez0PWsqSeizetkdxlqKG9r9TrSIEn3SHpe0nOS3i1piKR5kl5Mfw5OeSXpRkm1kp6RdPy+PEPVgl9EPEK2yEjhHPDgOrYe16/s/H1+tYE3TxkIbV983trozKNqmbPwsN3Sjh+9nLWb+7J43aCdaUePWMk90+7g7ml3cu2ciYWu9WXN3ihva90NwJyIOBI4BngOuBp4MCLGAQ+mY4CzgHFpmwbM2JfHaPe/QUnTGldzr9u4pb2Ls8/63/M61IgtEweW/Zu+v97IlveUn98qo3u3ev503CvMe+5tu6VPPurFnbW+Rs++NoxzZp7HR2adw8dOfpqeNXX7s6gdTiXW7ZU0EJhIWp0tIrZHxHqyhcxvTdluBT6Y9qcAt0XmMbL1fdv8krzdg19EzIyICRExofuAzt3s6/uLdfR+ahNrrxxVdi2uxytboT7Y8bY+VS6dNfWewxbz/IqhrN286/93NWrgtCNeZm6T2mCjl9cMZsv27hx2cCEbNbuUv27v0MbKTdqmlVxlLPA68P8lPS3pJkkHAMMiYnnKswIYlvZHAktKfr80pbWJe3srpNfTm+h/3xpev2YM0av8f1P6PLqBN13raxeTx9cyZ8G43dJOHLuUV9YMYtWmXa8tRgzcyMqN/aiPbgwfsImxB67ntfX9m16uMHJ+5Lw6IiY0c647cDxweVrD9wZ2NXEBiIiQqvNhjYNfGwy5bgm9Fmyh26Y6Dvm7RWz80MEM+PFq2NHA0GteBWD74X1Yf/EIAA655AW6vdkAdUHvJzay+nNvpW50bwD6/mYDqz/71nZ7lqLq3WMHJ45dwpd+NnG39DPH1zJn4e4B8bjRy7nw5Kepa+hGQ4gvz5nI+jcLXFOPqNRkpkuBpRHxeDq+hyz4rZQ0PCKWp2btqnR+GTC65PejUlqbVPNTl9uBSWTV3qXA9Ii4ueVfdQ5rrxq9R9qW9w1uNv+K7x7e/LkZzZ+z6tm6owenXv+xPdKn/+S0PdJ++uwR/PTZI/ZHsTqPCsS+iFghaYmkIyJiEXA6sDBtU4GvpD/vSz+ZDVwm6Q7gRGBDSfM4t6oFv4g4v1rXNrP2VcGG6OXADyT1BF4CLiTri7hL0kXAq8C5Ke/9wNlALbAl5W0zN3vNLJ8AKrSGR0T8HtjbO8HT95I3gEsrcmMc/MysLTy8zcyKyBMbmFkheelKMysez+piZkWUfeTc+aOfg5+Z5dcFprRy8DOz3FzzM7Pi8Ts/Myumio3tbVcOfmaWn5u9ZlY4XrTczArLNT8zK6TOH/sc/MwsPzV0/navg5+Z5RP4I2czKx4R/sjZzAqqCwS/dl+60sw6ocotWo6kmrR05U/S8VhJj0uqlXRnmuIeSb3ScW06P2ZfHsHBz8zyaXznV85WniuA50qOvwpcHxGHAeuAi1L6RcC6lH59ytdmDn5mlpsaGsraWr2ONAr4M+CmdCzgNLJlLAFuBT6Y9qekY9L501P+NnHwM7OcymzyZs3eoZLml2zTmlzsW8A/saueeCCwPiLq0vFSYGTaHwksAUjnN6T8beIODzPLJ8jT4bE6Iva2OhuSPgCsioinJE2qUOnK5uBnZvlV5ju/U4D/J+lsoDcwALgBGCSpe6rdjQKWpfzLgNHAUkndgYHAmrbe3M1eM8tNEWVtLYmIT0fEqIgYA5wH/CIiLgAeAs5J2aYC96X92emYdP4XaS3fNnHwM7P8Kvipy178M3CVpFqyd3o3p/SbgQNT+lXA1fvyCG72mlk+EVBf2fFtEfEw8HDafwk4YS95tgJ/Val7OviZWX5dYISHg5+Z5efgZ2aFE4DX8DCz4gmIzj+nlYOfmeUTVLzDoz04+JlZfn7nZ2aF5OBnZsWzTx8wdxgOfmaWTwBewMjMCsk1PzMrnsoPb2sPDn5mlk9A+Ds/Myskj/Aws0LyOz8zK5wI9/aaWUG55mdmxRNEfX17F2KfOfiZWT5dZEorr+FhZvlFQ3lbCySNlvSQpIWSFki6IqUPkTRP0ovpz8EpXZJulFQr6RlJx+/LIzj4mVkuAURDlLW1og74+4gYD5wEXCppPNnCRA9GxDjgQXYtVHQWMC5t04AZ+/IcDn5mlk9ERWp+EbE8In6X9jcBzwEjgSnArSnbrcAH0/4U4LbIPEa2vu/wtj6G3/mZWW45OjyGSppfcjwzImY2zSRpDHAc8DgwLCKWp1MrgGFpfySwpORnS1PactqgQwW/rX9cvvr5v/zCq+1djioYCqxu70JYLl317+yt+3qBTayb+0DcM7TM7KsjYnJLGST1A34EXBkRGyXtPBcRIakqvSsdKvhFxEHtXYZqkDQ/Iia0dzmsfP47a15rwSwPST3IAt8PIuLelLxS0vCIWJ6atatS+jJgdMnPR6W0NvE7PzNrF8qqeDcDz0XEdSWnZgNT0/5U4L6S9I+mXt+TgA0lzePcOlTNz8wK5RTgr4H/lfT7lPYZ4CvAXZIuAl4Fzk3n7gfOBmqBLcCF+3JzRRcYptLRSZq2t5e81nH576zrc/Azs0LyOz8zKyQHPzMrJAe/KpI0WdKiNBbx6tZ/Ye1N0ixJqyQ9295lsepy8KsSSTXAt8nGI44Hzk/jFq1juwWo2Hds1nE5+FXPCUBtRLwUEduBO8jGJloHFhGPAGvbuxxWfQ5+1dPcOEQz6wAc/MyskBz8qqei4xDNrLIc/KrnSWCcpLGSegLnkY1NNLMOwMGvSiKiDrgMmEs2SeNdEbGgfUtlrZF0O/Bb4AhJS9P4UuuCPLzNzArJNT8zKyQHPzMrJAc/MyskBz8zKyQHPzMrJAe/TkRSvaTfS3pW0t2S+u7DtW6RdE7av6mlSRckTZJ0chvu8YqkPVb5ai69SZ43ct7r85L+IW8Zrbgc/DqXNyPi2Ig4GtgOXFJ6UlKb1mSJiL+NiIUtZJkE5A5+Zh2Zg1/n9ShwWKqVPSppNrBQUo2kr0t6UtIzki6GbKUsSf+R5hd8ADi48UKSHpY0Ie1PlvQ7SX+Q9GBaTPoS4FOp1vleSQdJ+lG6x5OSTkm/PVDSzyUtkHQTIFoh6b8lPZV+M63JuetT+oOSDkppb5M0J/3mUUlHVuI/phWPV2/rhFIN7yxgTko6Hjg6Il5OAWRDRLxLUi/g15J+DhwHHEE2t+AwYCEwq8l1DwL+C5iYrjUkItZK+i7wRkR8I+X7IXB9RPxK0lvIRrH8CTAd+FVEXCPpz4ByRkd8LN2jD/CkpB9FxBrgAGB+RHxK0ufStS8DZgKXRMSLkk4EvgOc1ob/jFZwDn6dS5+SJf4eJVvz9GTgiYh4OaWfAbyj8X0eMBAYB0wEbo+IeuA1Sb/Yy/VPAh5pvFZENDev3fuA8dmyqwAMkNQv3eMv029/KmldGc/0SUl/kfZHp7KuARqAO1P694F70z1OBu4uuXevMu5htgcHv87lzYg4tjQhBYHNpUnA5RExt0m+sytYjm7ASRGxdS9lKZukSWSB9N0RsUXSw0DvZrJHuu/6pv8NzNrC7/y6nrnAxyX1AJB0uKQDgEeAD6V3gsOBU/fy28eAiZLGpt8OSembgP4l+X4OXN54IKkxGD0CfDilnQUMbqWsA4F1KfAdSVbzbNQNaKy9fpisOb0ReFnSX6V7SNIxrdzDbK8c/Lqem8je5/0uLcLzn2Q1/B8DL6Zzt5HNXLKbiHgdmEbWxPwDu5qd/wP8RWOHB/BJYELqUD91e3AAAABgSURBVFnIrl7nL5AFzwVkzd/FrZR1DtBd0nPAV8iCb6PNwAnpGU4DrknpFwAXpfItwEsDWBt5VhczKyTX/MyskBz8zKyQHPzMrJAc/MyskBz8zKyQHPzMrJAc/MyskP4PW78PDTxnhLEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(Naive_Bayes, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfEO6MRyFsKe",
        "outputId": "c36b37a5-5195-4ab6-c5c3-648d8c5dd8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.64281901 0.66684463 0.65776829 0.64102564 0.64583333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KNN"
      ],
      "metadata": {
        "id": "A7zYptQGAkL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn=knn.fit(X_train,y_train)\n",
        "knn_y_pred, knn_accuracy, knn_precision, knn_recall, knn_f1 = evaluate(X_test,y_test, knn)\n",
        "print(\"KNN Accuracy: \", knn_accuracy*100, \"%\")\n",
        "print(\"KNN Precision: \", knn_precision*100, \"%\")\n",
        "print(\"KNN Recall: \", knn_recall*100, \"%\")\n",
        "print(\"KNN F1 score: \", knn_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, knn_y_pred))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "U8Ni40_4-ck5",
        "outputId": "753d8cdc-a379-484a-b3cd-5b817b36f1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy:  76.07774732120608 %\n",
            "KNN Precision:  76.44565707756152 %\n",
            "KNN Recall:  76.07774732120608 %\n",
            "KNN F1 score:  75.97686675549787 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(knn, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMDLZlC5E_r5",
        "outputId": "1668b138-f7a8-4a15-de55-b24f8b47458a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.76187934 0.77148959 0.76294714 0.76816239 0.76762821]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Grid SearchCV to find the best estimators"
      ],
      "metadata": {
        "id": "4VfjhAjrBbEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'n_neighbors':[1,3,5,7,9,11,13,15], 'p':[1, 2,3]}\n",
        "gsv = GridSearchCV(knn,param_grid = parameters)\n",
        "gsv = gsv.fit(X_train, y_train)\n",
        "gsv.cv_results_\n",
        "gsv.best_params_\n",
        "knn_gsv =gsv.best_estimator_\n"
      ],
      "metadata": {
        "id": "a3iVDiFTBjjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsv.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R42s9wQ8D12r",
        "outputId": "9abd1874-4fe9-4a02-f709-d4a7bb00d34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_neighbors': 11, 'p': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Highest Accuracy reached when number of neighbours is 11"
      ],
      "metadata": {
        "id": "iyIyBnCbEvm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_gsv_y_pred, knn_gsv_accuracy, knn_gsv_precision, knn_gsv_recall, knn_gsv_f1 = evaluate(X_test,y_test, knn_gsv)\n",
        "print(\"KNN GSV Accuracy: \", knn_gsv_accuracy*100, \"%\")\n",
        "print(\"KNN GSV Precision: \", knn_gsv_precision*100, \"%\")\n",
        "print(\"KNN GSV Recall: \", knn_gsv_recall*100, \"%\")\n",
        "print(\"KNN GSV F1 score: \", knn_gsv_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, knn_gsv_y_pred))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "2rjv4g1YBjQP",
        "outputId": "08fe0237-8a1b-4f62-9c70-2d2e237e29ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN GSV Accuracy:  78.51981061549964 %\n",
            "KNN GSV Precision:  79.60757433698954 %\n",
            "KNN GSV Recall:  78.51981061549964 %\n",
            "KNN GSV F1 score:  78.29729674175199 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(knn_gsv, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsKpKiQJFyYe",
        "outputId": "54e4a34b-d093-425c-c5da-b4ccdc4c2233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.77736252 0.77736252 0.77949813 0.78151709 0.77457265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AdaBoost"
      ],
      "metadata": {
        "id": "mdRqS_uE995M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "adaboost=adaboost.fit(X_train,y_train)\n",
        "adaboost_y_pred, adaboost_accuracy, adaboost_precision, adaboost_recall, adaboost_f1 = evaluate(X_test,y_test, adaboost)\n",
        "print(\"AdaBoost Accuracy: \", adaboost_accuracy*100, \"%\")\n",
        "print(\"AdaBoost Precision: \", adaboost_precision*100, \"%\")\n",
        "print(\"AdaBoost Recall: \", adaboost_recall*100, \"%\")\n",
        "print(\"AdaBoost F1 score: \", adaboost_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, adaboost_y_pred))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "m-FQ_ISK5wlj",
        "outputId": "127b80ec-de25-4456-c5d2-a8821582d65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy:  82.50685272863194 %\n",
            "AdaBoost Precision:  82.52818632412254 %\n",
            "AdaBoost Recall:  82.50685272863194 %\n",
            "AdaBoost F1 score:  82.5014361964065 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(adaboost, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEHerbAYJxjs",
        "outputId": "08d685b3-bb9f-466f-ebaa-d7433e0994ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.82274426 0.82754939 0.82327816 0.82318376 0.82211538]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'n_estimators':[ 5,10,20,50, 100 , 200,500]}\n",
        "gsv2 = GridSearchCV(adaboost,param_grid = parameters)\n",
        "gsv2 = gsv2.fit(X_train, y_train)\n",
        "gsv2.cv_results_\n",
        "gsv2.best_params_\n",
        "adaboost_gsv =gsv2.best_estimator_"
      ],
      "metadata": {
        "id": "4pwEpJOHGvxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsv2.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghr5R7V6HHuO",
        "outputId": "1ee9369d-0f24-4e0b-ca2f-d52f7f72f429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_estimators': 500}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost_gsv_y_pred, adaboost_gsv_accuracy, adaboost_gsv_precision, adaboost_gsv_recall, adaboost_gsv_f1 = evaluate(X_test,y_test, adaboost_gsv)\n",
        "print(\"AdaBoost GSV Accuracy: \", adaboost_gsv_accuracy*100, \"%\")\n",
        "print(\"AdaBoost GSV Precision: \", adaboost_gsv_precision*100, \"%\")\n",
        "print(\"AdaBoost GSV Recall: \", adaboost_gsv_recall*100, \"%\")\n",
        "print(\"AdaBoost GSV F1 score: \", adaboost_gsv_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, adaboost_gsv_y_pred))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "RJk3GF24HLN4",
        "outputId": "7e41dfc7-3bce-46c0-aed1-2b2f7926bd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost GSV Accuracy:  83.50361325691502 %\n",
            "AdaBoost GSV Precision:  83.52955272366889 %\n",
            "AdaBoost GSV Recall:  83.50361325691502 %\n",
            "AdaBoost GSV F1 score:  83.49789428430483 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(adaboost_gsv, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_LuEMhTIiLQ",
        "outputId": "d3d74d06-999f-48ff-c0b5-69c1693cf18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.83502403 0.82754939 0.81580352 0.8215812  0.82799145]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest Classifier"
      ],
      "metadata": {
        "id": "miyUyXO1KGfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "random_forest=random_forest.fit(X_train,y_train)\n",
        "random_forest_y_pred, random_forest_accuracy, random_forest_precision, random_forest_recall, random_forest_f1 = evaluate(X_test,y_test, random_forest)\n",
        "print(\"Random Forest Accuracy: \", random_forest_accuracy*100, \"%\")\n",
        "print(\"Random Forest Precision: \", random_forest_precision*100, \"%\")\n",
        "print(\"Random Forest Recall: \", random_forest_recall*100, \"%\")\n",
        "print(\"Random Forest F1 score: \", random_forest_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, random_forest_y_pred))\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "cbnFgRQNKGM6",
        "outputId": "f117ebcf-0a0d-484e-94d3-608c1f54a059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy:  86.79292300024919 %\n",
            "Random Forest Precision:  86.91432926482943 %\n",
            "Random Forest Recall:  86.79292300024919 %\n",
            "Random Forest F1 score:  86.77828968684067 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(random_forest, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjnXfPhfMnSa",
        "outputId": "04262204-4fe8-41aa-8a69-b341c57eb347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.85691404 0.86492258 0.85691404 0.85470085 0.84882479]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'n_estimators':[ 5,10,20,50, 100 , 200,500]}\n",
        "gsv3 = GridSearchCV(random_forest,param_grid = parameters)\n",
        "gsv3 = gsv3.fit(X_train, y_train)\n",
        "gsv3.best_params_\n",
        "random_forest_gsv =gsv3.best_estimator_"
      ],
      "metadata": {
        "id": "0TOD-E7RJ12X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsv3.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq8Ayk0kMOtR",
        "outputId": "463d97f3-4022-409c-9e71-12e3bda11c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_estimators': 200}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_gsv_y_pred, random_forest_gsv_accuracy, random_forest_gsv_precision, random_forest_gsv_recall, random_forest_gsv_f1 = evaluate(X_test,y_test, random_forest_gsv)\n",
        "print(\"Random Forest GSV Accuracy: \", random_forest_gsv_accuracy*100, \"%\")\n",
        "print(\"Random Forest GSV Precision: \", random_forest_gsv_precision*100, \"%\")\n",
        "print(\"Random Forest GSV Recall: \", random_forest_gsv_recall*100, \"%\")\n",
        "print(\"Random Forest GSV F1 score: \", random_forest_gsv_f1*100, \"%\")\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, random_forest_gsv_y_pred))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "in9hE9zAMQrR",
        "outputId": "e31718ad-e5ed-4c38-ba26-1afae9216078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest GSV Accuracy:  86.86768003987042 %\n",
            "Random Forest GSV Precision:  86.98309169971525 %\n",
            "Random Forest GSV Recall:  86.86768003987042 %\n",
            "Random Forest GSV F1 score:  86.8537819921936 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(random_forest_gsv, X_train, y_train, cv=5, scoring = 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJxjY9LMNWzX",
        "outputId": "6da5a7fb-47a8-4aa4-dfae-0dad862c9b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.85424453 0.8670582  0.85744794 0.8616453  0.85149573]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network"
      ],
      "metadata": {
        "id": "ws2XuWDhR7k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "2a5CCyEHR865"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ts = torch.from_numpy(X_train.values).float()\n",
        "X_test_ts = torch.from_numpy(X_test.values).float()"
      ],
      "metadata": {
        "id": "hfQSBK_VPCUI"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "y_train_encoded_ts = le.fit_transform(y_train)\n",
        "y_test_encoded_ts = le.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "1y3ReXXyPCRq"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train_encoded_ts = torch.from_numpy(y_train_encoded_ts).float()\n",
        "y_test_encoded_ts = torch.from_numpy(y_test_encoded_ts).float()"
      ],
      "metadata": {
        "id": "UQrK-818Q2YW"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_ts.shape)\n",
        "print(X_test_ts.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQuv95eYQ2U1",
        "outputId": "a144bf2d-623d-4987-d6de-15d005daf302"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9363, 10])\n",
            "torch.Size([4013, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.values).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "        self.len = self.X.shape[0]\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "   \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "   \n",
        "batch_size = 64\n",
        "\n",
        "# Instantiate training and test data\n",
        "train_data = Data(X_train, y_train_encoded_ts)\n",
        "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = Data(X_test, y_test_encoded_ts)\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Check it's working\n",
        "for batch, (X, y) in enumerate(train_dataloader):\n",
        "    print(f\"Batch: {batch+1}\")\n",
        "    print(f\"X shape: {X.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQUcg9Yxay7W",
        "outputId": "3f4ca9ca-b5d8-4534-e1a2-e9958657c8fd"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 1\n",
            "X shape: torch.Size([64, 10])\n",
            "y shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, hidden_layer1, hidden_layer2):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.l1 = nn.Linear(10, hidden_layer1)\n",
        "        self.l2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
        "        self.out = nn.Linear(hidden_layer2, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, data):\n",
        "        x = self.l1(data)\n",
        "        x = torch.relu(x)\n",
        "        x = self.l2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.out(x)\n",
        "        return torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "L4XBPNx-Sue9"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NeuralNetwork_Model1 = NeuralNetwork(32,64)\n",
        "print(NeuralNetwork_Model1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks7fvr35PCXx",
        "outputId": "b0fa2b95-dee5-4eae-8773-c48099d288bf"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (l1): Linear(in_features=10, out_features=32, bias=True)\n",
            "  (l2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCELoss()\n",
        "optimizer=torch.optim.SGD(NeuralNetwork_Model1.parameters(), lr=0.1)\n",
        "EPOCHS= 20\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "u6Y_1mo2Q2OL"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[INFO] training using {}...\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9-SZrOAXDkr",
        "outputId": "35be8a7e-5ac8-47ab-e59b-8a993afc7a6a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training using cpu...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(500):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = NeuralNetwork_Model1(inputs)\n",
        "        loss = criterion(outputs.reshape(labels.shape), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "        running_loss = 0.0\n",
        "       \n",
        "            \n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P0yHHbzWAuQ",
        "outputId": "d7c4a711-6967-4a28-9f87-5dfd11d02a6c"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[466,   147] loss: 0.024\n",
            "[467,     1] loss: 0.022\n",
            "[467,     2] loss: 0.026\n",
            "[467,     3] loss: 0.030\n",
            "[467,     4] loss: 0.021\n",
            "[467,     5] loss: 0.022\n",
            "[467,     6] loss: 0.028\n",
            "[467,     7] loss: 0.024\n",
            "[467,     8] loss: 0.024\n",
            "[467,     9] loss: 0.023\n",
            "[467,    10] loss: 0.027\n",
            "[467,    11] loss: 0.025\n",
            "[467,    12] loss: 0.025\n",
            "[467,    13] loss: 0.029\n",
            "[467,    14] loss: 0.019\n",
            "[467,    15] loss: 0.023\n",
            "[467,    16] loss: 0.025\n",
            "[467,    17] loss: 0.030\n",
            "[467,    18] loss: 0.023\n",
            "[467,    19] loss: 0.021\n",
            "[467,    20] loss: 0.028\n",
            "[467,    21] loss: 0.029\n",
            "[467,    22] loss: 0.024\n",
            "[467,    23] loss: 0.027\n",
            "[467,    24] loss: 0.024\n",
            "[467,    25] loss: 0.027\n",
            "[467,    26] loss: 0.026\n",
            "[467,    27] loss: 0.024\n",
            "[467,    28] loss: 0.025\n",
            "[467,    29] loss: 0.024\n",
            "[467,    30] loss: 0.027\n",
            "[467,    31] loss: 0.028\n",
            "[467,    32] loss: 0.027\n",
            "[467,    33] loss: 0.025\n",
            "[467,    34] loss: 0.027\n",
            "[467,    35] loss: 0.027\n",
            "[467,    36] loss: 0.025\n",
            "[467,    37] loss: 0.027\n",
            "[467,    38] loss: 0.024\n",
            "[467,    39] loss: 0.026\n",
            "[467,    40] loss: 0.023\n",
            "[467,    41] loss: 0.028\n",
            "[467,    42] loss: 0.027\n",
            "[467,    43] loss: 0.030\n",
            "[467,    44] loss: 0.030\n",
            "[467,    45] loss: 0.022\n",
            "[467,    46] loss: 0.025\n",
            "[467,    47] loss: 0.024\n",
            "[467,    48] loss: 0.020\n",
            "[467,    49] loss: 0.030\n",
            "[467,    50] loss: 0.027\n",
            "[467,    51] loss: 0.025\n",
            "[467,    52] loss: 0.024\n",
            "[467,    53] loss: 0.026\n",
            "[467,    54] loss: 0.027\n",
            "[467,    55] loss: 0.030\n",
            "[467,    56] loss: 0.028\n",
            "[467,    57] loss: 0.027\n",
            "[467,    58] loss: 0.026\n",
            "[467,    59] loss: 0.026\n",
            "[467,    60] loss: 0.021\n",
            "[467,    61] loss: 0.022\n",
            "[467,    62] loss: 0.016\n",
            "[467,    63] loss: 0.027\n",
            "[467,    64] loss: 0.026\n",
            "[467,    65] loss: 0.027\n",
            "[467,    66] loss: 0.024\n",
            "[467,    67] loss: 0.025\n",
            "[467,    68] loss: 0.029\n",
            "[467,    69] loss: 0.023\n",
            "[467,    70] loss: 0.020\n",
            "[467,    71] loss: 0.030\n",
            "[467,    72] loss: 0.023\n",
            "[467,    73] loss: 0.024\n",
            "[467,    74] loss: 0.027\n",
            "[467,    75] loss: 0.027\n",
            "[467,    76] loss: 0.029\n",
            "[467,    77] loss: 0.024\n",
            "[467,    78] loss: 0.023\n",
            "[467,    79] loss: 0.025\n",
            "[467,    80] loss: 0.028\n",
            "[467,    81] loss: 0.020\n",
            "[467,    82] loss: 0.023\n",
            "[467,    83] loss: 0.025\n",
            "[467,    84] loss: 0.027\n",
            "[467,    85] loss: 0.025\n",
            "[467,    86] loss: 0.025\n",
            "[467,    87] loss: 0.026\n",
            "[467,    88] loss: 0.024\n",
            "[467,    89] loss: 0.027\n",
            "[467,    90] loss: 0.024\n",
            "[467,    91] loss: 0.027\n",
            "[467,    92] loss: 0.027\n",
            "[467,    93] loss: 0.026\n",
            "[467,    94] loss: 0.023\n",
            "[467,    95] loss: 0.019\n",
            "[467,    96] loss: 0.019\n",
            "[467,    97] loss: 0.026\n",
            "[467,    98] loss: 0.027\n",
            "[467,    99] loss: 0.031\n",
            "[467,   100] loss: 0.025\n",
            "[467,   101] loss: 0.024\n",
            "[467,   102] loss: 0.020\n",
            "[467,   103] loss: 0.026\n",
            "[467,   104] loss: 0.025\n",
            "[467,   105] loss: 0.024\n",
            "[467,   106] loss: 0.026\n",
            "[467,   107] loss: 0.026\n",
            "[467,   108] loss: 0.023\n",
            "[467,   109] loss: 0.021\n",
            "[467,   110] loss: 0.027\n",
            "[467,   111] loss: 0.029\n",
            "[467,   112] loss: 0.021\n",
            "[467,   113] loss: 0.022\n",
            "[467,   114] loss: 0.020\n",
            "[467,   115] loss: 0.024\n",
            "[467,   116] loss: 0.027\n",
            "[467,   117] loss: 0.022\n",
            "[467,   118] loss: 0.027\n",
            "[467,   119] loss: 0.027\n",
            "[467,   120] loss: 0.024\n",
            "[467,   121] loss: 0.023\n",
            "[467,   122] loss: 0.021\n",
            "[467,   123] loss: 0.030\n",
            "[467,   124] loss: 0.027\n",
            "[467,   125] loss: 0.025\n",
            "[467,   126] loss: 0.020\n",
            "[467,   127] loss: 0.021\n",
            "[467,   128] loss: 0.027\n",
            "[467,   129] loss: 0.025\n",
            "[467,   130] loss: 0.022\n",
            "[467,   131] loss: 0.027\n",
            "[467,   132] loss: 0.028\n",
            "[467,   133] loss: 0.029\n",
            "[467,   134] loss: 0.027\n",
            "[467,   135] loss: 0.026\n",
            "[467,   136] loss: 0.027\n",
            "[467,   137] loss: 0.023\n",
            "[467,   138] loss: 0.023\n",
            "[467,   139] loss: 0.022\n",
            "[467,   140] loss: 0.028\n",
            "[467,   141] loss: 0.028\n",
            "[467,   142] loss: 0.028\n",
            "[467,   143] loss: 0.028\n",
            "[467,   144] loss: 0.029\n",
            "[467,   145] loss: 0.022\n",
            "[467,   146] loss: 0.023\n",
            "[467,   147] loss: 0.024\n",
            "[468,     1] loss: 0.022\n",
            "[468,     2] loss: 0.030\n",
            "[468,     3] loss: 0.034\n",
            "[468,     4] loss: 0.028\n",
            "[468,     5] loss: 0.022\n",
            "[468,     6] loss: 0.027\n",
            "[468,     7] loss: 0.026\n",
            "[468,     8] loss: 0.021\n",
            "[468,     9] loss: 0.028\n",
            "[468,    10] loss: 0.027\n",
            "[468,    11] loss: 0.020\n",
            "[468,    12] loss: 0.026\n",
            "[468,    13] loss: 0.029\n",
            "[468,    14] loss: 0.024\n",
            "[468,    15] loss: 0.023\n",
            "[468,    16] loss: 0.024\n",
            "[468,    17] loss: 0.023\n",
            "[468,    18] loss: 0.025\n",
            "[468,    19] loss: 0.026\n",
            "[468,    20] loss: 0.027\n",
            "[468,    21] loss: 0.027\n",
            "[468,    22] loss: 0.027\n",
            "[468,    23] loss: 0.021\n",
            "[468,    24] loss: 0.028\n",
            "[468,    25] loss: 0.026\n",
            "[468,    26] loss: 0.026\n",
            "[468,    27] loss: 0.029\n",
            "[468,    28] loss: 0.025\n",
            "[468,    29] loss: 0.026\n",
            "[468,    30] loss: 0.030\n",
            "[468,    31] loss: 0.029\n",
            "[468,    32] loss: 0.025\n",
            "[468,    33] loss: 0.028\n",
            "[468,    34] loss: 0.025\n",
            "[468,    35] loss: 0.024\n",
            "[468,    36] loss: 0.023\n",
            "[468,    37] loss: 0.022\n",
            "[468,    38] loss: 0.025\n",
            "[468,    39] loss: 0.032\n",
            "[468,    40] loss: 0.022\n",
            "[468,    41] loss: 0.027\n",
            "[468,    42] loss: 0.020\n",
            "[468,    43] loss: 0.025\n",
            "[468,    44] loss: 0.019\n",
            "[468,    45] loss: 0.027\n",
            "[468,    46] loss: 0.026\n",
            "[468,    47] loss: 0.024\n",
            "[468,    48] loss: 0.023\n",
            "[468,    49] loss: 0.028\n",
            "[468,    50] loss: 0.025\n",
            "[468,    51] loss: 0.024\n",
            "[468,    52] loss: 0.025\n",
            "[468,    53] loss: 0.027\n",
            "[468,    54] loss: 0.025\n",
            "[468,    55] loss: 0.023\n",
            "[468,    56] loss: 0.027\n",
            "[468,    57] loss: 0.022\n",
            "[468,    58] loss: 0.027\n",
            "[468,    59] loss: 0.023\n",
            "[468,    60] loss: 0.027\n",
            "[468,    61] loss: 0.024\n",
            "[468,    62] loss: 0.025\n",
            "[468,    63] loss: 0.022\n",
            "[468,    64] loss: 0.021\n",
            "[468,    65] loss: 0.021\n",
            "[468,    66] loss: 0.025\n",
            "[468,    67] loss: 0.027\n",
            "[468,    68] loss: 0.027\n",
            "[468,    69] loss: 0.026\n",
            "[468,    70] loss: 0.027\n",
            "[468,    71] loss: 0.025\n",
            "[468,    72] loss: 0.026\n",
            "[468,    73] loss: 0.027\n",
            "[468,    74] loss: 0.020\n",
            "[468,    75] loss: 0.027\n",
            "[468,    76] loss: 0.023\n",
            "[468,    77] loss: 0.024\n",
            "[468,    78] loss: 0.027\n",
            "[468,    79] loss: 0.023\n",
            "[468,    80] loss: 0.023\n",
            "[468,    81] loss: 0.022\n",
            "[468,    82] loss: 0.030\n",
            "[468,    83] loss: 0.032\n",
            "[468,    84] loss: 0.025\n",
            "[468,    85] loss: 0.027\n",
            "[468,    86] loss: 0.026\n",
            "[468,    87] loss: 0.026\n",
            "[468,    88] loss: 0.022\n",
            "[468,    89] loss: 0.026\n",
            "[468,    90] loss: 0.027\n",
            "[468,    91] loss: 0.026\n",
            "[468,    92] loss: 0.027\n",
            "[468,    93] loss: 0.018\n",
            "[468,    94] loss: 0.027\n",
            "[468,    95] loss: 0.026\n",
            "[468,    96] loss: 0.024\n",
            "[468,    97] loss: 0.027\n",
            "[468,    98] loss: 0.026\n",
            "[468,    99] loss: 0.028\n",
            "[468,   100] loss: 0.020\n",
            "[468,   101] loss: 0.023\n",
            "[468,   102] loss: 0.024\n",
            "[468,   103] loss: 0.027\n",
            "[468,   104] loss: 0.029\n",
            "[468,   105] loss: 0.027\n",
            "[468,   106] loss: 0.023\n",
            "[468,   107] loss: 0.023\n",
            "[468,   108] loss: 0.024\n",
            "[468,   109] loss: 0.023\n",
            "[468,   110] loss: 0.025\n",
            "[468,   111] loss: 0.024\n",
            "[468,   112] loss: 0.029\n",
            "[468,   113] loss: 0.029\n",
            "[468,   114] loss: 0.030\n",
            "[468,   115] loss: 0.028\n",
            "[468,   116] loss: 0.024\n",
            "[468,   117] loss: 0.027\n",
            "[468,   118] loss: 0.024\n",
            "[468,   119] loss: 0.028\n",
            "[468,   120] loss: 0.022\n",
            "[468,   121] loss: 0.028\n",
            "[468,   122] loss: 0.022\n",
            "[468,   123] loss: 0.022\n",
            "[468,   124] loss: 0.032\n",
            "[468,   125] loss: 0.025\n",
            "[468,   126] loss: 0.020\n",
            "[468,   127] loss: 0.020\n",
            "[468,   128] loss: 0.027\n",
            "[468,   129] loss: 0.022\n",
            "[468,   130] loss: 0.023\n",
            "[468,   131] loss: 0.026\n",
            "[468,   132] loss: 0.022\n",
            "[468,   133] loss: 0.023\n",
            "[468,   134] loss: 0.019\n",
            "[468,   135] loss: 0.023\n",
            "[468,   136] loss: 0.031\n",
            "[468,   137] loss: 0.025\n",
            "[468,   138] loss: 0.025\n",
            "[468,   139] loss: 0.027\n",
            "[468,   140] loss: 0.021\n",
            "[468,   141] loss: 0.026\n",
            "[468,   142] loss: 0.021\n",
            "[468,   143] loss: 0.030\n",
            "[468,   144] loss: 0.027\n",
            "[468,   145] loss: 0.023\n",
            "[468,   146] loss: 0.026\n",
            "[468,   147] loss: 0.021\n",
            "[469,     1] loss: 0.031\n",
            "[469,     2] loss: 0.026\n",
            "[469,     3] loss: 0.020\n",
            "[469,     4] loss: 0.021\n",
            "[469,     5] loss: 0.024\n",
            "[469,     6] loss: 0.020\n",
            "[469,     7] loss: 0.026\n",
            "[469,     8] loss: 0.023\n",
            "[469,     9] loss: 0.027\n",
            "[469,    10] loss: 0.027\n",
            "[469,    11] loss: 0.025\n",
            "[469,    12] loss: 0.026\n",
            "[469,    13] loss: 0.026\n",
            "[469,    14] loss: 0.027\n",
            "[469,    15] loss: 0.019\n",
            "[469,    16] loss: 0.027\n",
            "[469,    17] loss: 0.023\n",
            "[469,    18] loss: 0.027\n",
            "[469,    19] loss: 0.020\n",
            "[469,    20] loss: 0.026\n",
            "[469,    21] loss: 0.024\n",
            "[469,    22] loss: 0.020\n",
            "[469,    23] loss: 0.020\n",
            "[469,    24] loss: 0.027\n",
            "[469,    25] loss: 0.027\n",
            "[469,    26] loss: 0.024\n",
            "[469,    27] loss: 0.025\n",
            "[469,    28] loss: 0.029\n",
            "[469,    29] loss: 0.023\n",
            "[469,    30] loss: 0.023\n",
            "[469,    31] loss: 0.027\n",
            "[469,    32] loss: 0.034\n",
            "[469,    33] loss: 0.025\n",
            "[469,    34] loss: 0.028\n",
            "[469,    35] loss: 0.023\n",
            "[469,    36] loss: 0.023\n",
            "[469,    37] loss: 0.026\n",
            "[469,    38] loss: 0.028\n",
            "[469,    39] loss: 0.030\n",
            "[469,    40] loss: 0.026\n",
            "[469,    41] loss: 0.027\n",
            "[469,    42] loss: 0.023\n",
            "[469,    43] loss: 0.030\n",
            "[469,    44] loss: 0.024\n",
            "[469,    45] loss: 0.023\n",
            "[469,    46] loss: 0.026\n",
            "[469,    47] loss: 0.023\n",
            "[469,    48] loss: 0.029\n",
            "[469,    49] loss: 0.023\n",
            "[469,    50] loss: 0.024\n",
            "[469,    51] loss: 0.027\n",
            "[469,    52] loss: 0.027\n",
            "[469,    53] loss: 0.031\n",
            "[469,    54] loss: 0.020\n",
            "[469,    55] loss: 0.020\n",
            "[469,    56] loss: 0.028\n",
            "[469,    57] loss: 0.020\n",
            "[469,    58] loss: 0.024\n",
            "[469,    59] loss: 0.024\n",
            "[469,    60] loss: 0.026\n",
            "[469,    61] loss: 0.029\n",
            "[469,    62] loss: 0.027\n",
            "[469,    63] loss: 0.016\n",
            "[469,    64] loss: 0.028\n",
            "[469,    65] loss: 0.026\n",
            "[469,    66] loss: 0.023\n",
            "[469,    67] loss: 0.027\n",
            "[469,    68] loss: 0.023\n",
            "[469,    69] loss: 0.023\n",
            "[469,    70] loss: 0.023\n",
            "[469,    71] loss: 0.026\n",
            "[469,    72] loss: 0.023\n",
            "[469,    73] loss: 0.027\n",
            "[469,    74] loss: 0.024\n",
            "[469,    75] loss: 0.026\n",
            "[469,    76] loss: 0.028\n",
            "[469,    77] loss: 0.026\n",
            "[469,    78] loss: 0.021\n",
            "[469,    79] loss: 0.021\n",
            "[469,    80] loss: 0.028\n",
            "[469,    81] loss: 0.020\n",
            "[469,    82] loss: 0.025\n",
            "[469,    83] loss: 0.027\n",
            "[469,    84] loss: 0.025\n",
            "[469,    85] loss: 0.025\n",
            "[469,    86] loss: 0.028\n",
            "[469,    87] loss: 0.027\n",
            "[469,    88] loss: 0.028\n",
            "[469,    89] loss: 0.027\n",
            "[469,    90] loss: 0.017\n",
            "[469,    91] loss: 0.024\n",
            "[469,    92] loss: 0.023\n",
            "[469,    93] loss: 0.028\n",
            "[469,    94] loss: 0.026\n",
            "[469,    95] loss: 0.024\n",
            "[469,    96] loss: 0.030\n",
            "[469,    97] loss: 0.034\n",
            "[469,    98] loss: 0.026\n",
            "[469,    99] loss: 0.024\n",
            "[469,   100] loss: 0.026\n",
            "[469,   101] loss: 0.026\n",
            "[469,   102] loss: 0.027\n",
            "[469,   103] loss: 0.027\n",
            "[469,   104] loss: 0.028\n",
            "[469,   105] loss: 0.024\n",
            "[469,   106] loss: 0.025\n",
            "[469,   107] loss: 0.027\n",
            "[469,   108] loss: 0.023\n",
            "[469,   109] loss: 0.026\n",
            "[469,   110] loss: 0.024\n",
            "[469,   111] loss: 0.028\n",
            "[469,   112] loss: 0.024\n",
            "[469,   113] loss: 0.027\n",
            "[469,   114] loss: 0.029\n",
            "[469,   115] loss: 0.023\n",
            "[469,   116] loss: 0.023\n",
            "[469,   117] loss: 0.028\n",
            "[469,   118] loss: 0.029\n",
            "[469,   119] loss: 0.023\n",
            "[469,   120] loss: 0.025\n",
            "[469,   121] loss: 0.020\n",
            "[469,   122] loss: 0.026\n",
            "[469,   123] loss: 0.027\n",
            "[469,   124] loss: 0.023\n",
            "[469,   125] loss: 0.023\n",
            "[469,   126] loss: 0.023\n",
            "[469,   127] loss: 0.027\n",
            "[469,   128] loss: 0.029\n",
            "[469,   129] loss: 0.021\n",
            "[469,   130] loss: 0.023\n",
            "[469,   131] loss: 0.025\n",
            "[469,   132] loss: 0.023\n",
            "[469,   133] loss: 0.027\n",
            "[469,   134] loss: 0.026\n",
            "[469,   135] loss: 0.030\n",
            "[469,   136] loss: 0.027\n",
            "[469,   137] loss: 0.023\n",
            "[469,   138] loss: 0.027\n",
            "[469,   139] loss: 0.028\n",
            "[469,   140] loss: 0.028\n",
            "[469,   141] loss: 0.025\n",
            "[469,   142] loss: 0.029\n",
            "[469,   143] loss: 0.020\n",
            "[469,   144] loss: 0.025\n",
            "[469,   145] loss: 0.020\n",
            "[469,   146] loss: 0.030\n",
            "[469,   147] loss: 0.013\n",
            "[470,     1] loss: 0.016\n",
            "[470,     2] loss: 0.022\n",
            "[470,     3] loss: 0.027\n",
            "[470,     4] loss: 0.024\n",
            "[470,     5] loss: 0.032\n",
            "[470,     6] loss: 0.027\n",
            "[470,     7] loss: 0.032\n",
            "[470,     8] loss: 0.030\n",
            "[470,     9] loss: 0.027\n",
            "[470,    10] loss: 0.030\n",
            "[470,    11] loss: 0.024\n",
            "[470,    12] loss: 0.026\n",
            "[470,    13] loss: 0.018\n",
            "[470,    14] loss: 0.021\n",
            "[470,    15] loss: 0.030\n",
            "[470,    16] loss: 0.019\n",
            "[470,    17] loss: 0.030\n",
            "[470,    18] loss: 0.021\n",
            "[470,    19] loss: 0.027\n",
            "[470,    20] loss: 0.029\n",
            "[470,    21] loss: 0.029\n",
            "[470,    22] loss: 0.023\n",
            "[470,    23] loss: 0.025\n",
            "[470,    24] loss: 0.022\n",
            "[470,    25] loss: 0.029\n",
            "[470,    26] loss: 0.024\n",
            "[470,    27] loss: 0.025\n",
            "[470,    28] loss: 0.025\n",
            "[470,    29] loss: 0.027\n",
            "[470,    30] loss: 0.023\n",
            "[470,    31] loss: 0.023\n",
            "[470,    32] loss: 0.022\n",
            "[470,    33] loss: 0.026\n",
            "[470,    34] loss: 0.021\n",
            "[470,    35] loss: 0.027\n",
            "[470,    36] loss: 0.029\n",
            "[470,    37] loss: 0.026\n",
            "[470,    38] loss: 0.027\n",
            "[470,    39] loss: 0.025\n",
            "[470,    40] loss: 0.026\n",
            "[470,    41] loss: 0.024\n",
            "[470,    42] loss: 0.023\n",
            "[470,    43] loss: 0.030\n",
            "[470,    44] loss: 0.027\n",
            "[470,    45] loss: 0.026\n",
            "[470,    46] loss: 0.020\n",
            "[470,    47] loss: 0.026\n",
            "[470,    48] loss: 0.027\n",
            "[470,    49] loss: 0.021\n",
            "[470,    50] loss: 0.027\n",
            "[470,    51] loss: 0.025\n",
            "[470,    52] loss: 0.023\n",
            "[470,    53] loss: 0.022\n",
            "[470,    54] loss: 0.030\n",
            "[470,    55] loss: 0.023\n",
            "[470,    56] loss: 0.027\n",
            "[470,    57] loss: 0.020\n",
            "[470,    58] loss: 0.023\n",
            "[470,    59] loss: 0.025\n",
            "[470,    60] loss: 0.021\n",
            "[470,    61] loss: 0.017\n",
            "[470,    62] loss: 0.027\n",
            "[470,    63] loss: 0.027\n",
            "[470,    64] loss: 0.033\n",
            "[470,    65] loss: 0.030\n",
            "[470,    66] loss: 0.018\n",
            "[470,    67] loss: 0.022\n",
            "[470,    68] loss: 0.026\n",
            "[470,    69] loss: 0.026\n",
            "[470,    70] loss: 0.024\n",
            "[470,    71] loss: 0.027\n",
            "[470,    72] loss: 0.029\n",
            "[470,    73] loss: 0.017\n",
            "[470,    74] loss: 0.023\n",
            "[470,    75] loss: 0.021\n",
            "[470,    76] loss: 0.025\n",
            "[470,    77] loss: 0.026\n",
            "[470,    78] loss: 0.023\n",
            "[470,    79] loss: 0.029\n",
            "[470,    80] loss: 0.023\n",
            "[470,    81] loss: 0.027\n",
            "[470,    82] loss: 0.030\n",
            "[470,    83] loss: 0.023\n",
            "[470,    84] loss: 0.023\n",
            "[470,    85] loss: 0.019\n",
            "[470,    86] loss: 0.023\n",
            "[470,    87] loss: 0.023\n",
            "[470,    88] loss: 0.021\n",
            "[470,    89] loss: 0.029\n",
            "[470,    90] loss: 0.027\n",
            "[470,    91] loss: 0.027\n",
            "[470,    92] loss: 0.026\n",
            "[470,    93] loss: 0.020\n",
            "[470,    94] loss: 0.027\n",
            "[470,    95] loss: 0.025\n",
            "[470,    96] loss: 0.030\n",
            "[470,    97] loss: 0.020\n",
            "[470,    98] loss: 0.030\n",
            "[470,    99] loss: 0.027\n",
            "[470,   100] loss: 0.026\n",
            "[470,   101] loss: 0.026\n",
            "[470,   102] loss: 0.028\n",
            "[470,   103] loss: 0.027\n",
            "[470,   104] loss: 0.030\n",
            "[470,   105] loss: 0.028\n",
            "[470,   106] loss: 0.028\n",
            "[470,   107] loss: 0.024\n",
            "[470,   108] loss: 0.025\n",
            "[470,   109] loss: 0.031\n",
            "[470,   110] loss: 0.031\n",
            "[470,   111] loss: 0.027\n",
            "[470,   112] loss: 0.027\n",
            "[470,   113] loss: 0.027\n",
            "[470,   114] loss: 0.024\n",
            "[470,   115] loss: 0.023\n",
            "[470,   116] loss: 0.027\n",
            "[470,   117] loss: 0.021\n",
            "[470,   118] loss: 0.025\n",
            "[470,   119] loss: 0.027\n",
            "[470,   120] loss: 0.017\n",
            "[470,   121] loss: 0.025\n",
            "[470,   122] loss: 0.024\n",
            "[470,   123] loss: 0.029\n",
            "[470,   124] loss: 0.021\n",
            "[470,   125] loss: 0.019\n",
            "[470,   126] loss: 0.025\n",
            "[470,   127] loss: 0.029\n",
            "[470,   128] loss: 0.027\n",
            "[470,   129] loss: 0.028\n",
            "[470,   130] loss: 0.027\n",
            "[470,   131] loss: 0.018\n",
            "[470,   132] loss: 0.024\n",
            "[470,   133] loss: 0.023\n",
            "[470,   134] loss: 0.026\n",
            "[470,   135] loss: 0.027\n",
            "[470,   136] loss: 0.023\n",
            "[470,   137] loss: 0.017\n",
            "[470,   138] loss: 0.025\n",
            "[470,   139] loss: 0.025\n",
            "[470,   140] loss: 0.024\n",
            "[470,   141] loss: 0.024\n",
            "[470,   142] loss: 0.028\n",
            "[470,   143] loss: 0.026\n",
            "[470,   144] loss: 0.032\n",
            "[470,   145] loss: 0.034\n",
            "[470,   146] loss: 0.020\n",
            "[470,   147] loss: 0.029\n",
            "[471,     1] loss: 0.024\n",
            "[471,     2] loss: 0.023\n",
            "[471,     3] loss: 0.027\n",
            "[471,     4] loss: 0.023\n",
            "[471,     5] loss: 0.024\n",
            "[471,     6] loss: 0.022\n",
            "[471,     7] loss: 0.027\n",
            "[471,     8] loss: 0.024\n",
            "[471,     9] loss: 0.025\n",
            "[471,    10] loss: 0.028\n",
            "[471,    11] loss: 0.024\n",
            "[471,    12] loss: 0.030\n",
            "[471,    13] loss: 0.028\n",
            "[471,    14] loss: 0.023\n",
            "[471,    15] loss: 0.028\n",
            "[471,    16] loss: 0.033\n",
            "[471,    17] loss: 0.023\n",
            "[471,    18] loss: 0.024\n",
            "[471,    19] loss: 0.027\n",
            "[471,    20] loss: 0.024\n",
            "[471,    21] loss: 0.029\n",
            "[471,    22] loss: 0.020\n",
            "[471,    23] loss: 0.023\n",
            "[471,    24] loss: 0.030\n",
            "[471,    25] loss: 0.026\n",
            "[471,    26] loss: 0.023\n",
            "[471,    27] loss: 0.028\n",
            "[471,    28] loss: 0.025\n",
            "[471,    29] loss: 0.019\n",
            "[471,    30] loss: 0.027\n",
            "[471,    31] loss: 0.025\n",
            "[471,    32] loss: 0.030\n",
            "[471,    33] loss: 0.024\n",
            "[471,    34] loss: 0.026\n",
            "[471,    35] loss: 0.027\n",
            "[471,    36] loss: 0.028\n",
            "[471,    37] loss: 0.026\n",
            "[471,    38] loss: 0.026\n",
            "[471,    39] loss: 0.020\n",
            "[471,    40] loss: 0.027\n",
            "[471,    41] loss: 0.030\n",
            "[471,    42] loss: 0.030\n",
            "[471,    43] loss: 0.029\n",
            "[471,    44] loss: 0.023\n",
            "[471,    45] loss: 0.024\n",
            "[471,    46] loss: 0.030\n",
            "[471,    47] loss: 0.030\n",
            "[471,    48] loss: 0.025\n",
            "[471,    49] loss: 0.022\n",
            "[471,    50] loss: 0.023\n",
            "[471,    51] loss: 0.026\n",
            "[471,    52] loss: 0.028\n",
            "[471,    53] loss: 0.023\n",
            "[471,    54] loss: 0.023\n",
            "[471,    55] loss: 0.028\n",
            "[471,    56] loss: 0.028\n",
            "[471,    57] loss: 0.022\n",
            "[471,    58] loss: 0.021\n",
            "[471,    59] loss: 0.024\n",
            "[471,    60] loss: 0.020\n",
            "[471,    61] loss: 0.025\n",
            "[471,    62] loss: 0.021\n",
            "[471,    63] loss: 0.020\n",
            "[471,    64] loss: 0.023\n",
            "[471,    65] loss: 0.029\n",
            "[471,    66] loss: 0.031\n",
            "[471,    67] loss: 0.023\n",
            "[471,    68] loss: 0.027\n",
            "[471,    69] loss: 0.027\n",
            "[471,    70] loss: 0.019\n",
            "[471,    71] loss: 0.024\n",
            "[471,    72] loss: 0.025\n",
            "[471,    73] loss: 0.026\n",
            "[471,    74] loss: 0.027\n",
            "[471,    75] loss: 0.024\n",
            "[471,    76] loss: 0.025\n",
            "[471,    77] loss: 0.030\n",
            "[471,    78] loss: 0.023\n",
            "[471,    79] loss: 0.025\n",
            "[471,    80] loss: 0.020\n",
            "[471,    81] loss: 0.030\n",
            "[471,    82] loss: 0.027\n",
            "[471,    83] loss: 0.022\n",
            "[471,    84] loss: 0.022\n",
            "[471,    85] loss: 0.023\n",
            "[471,    86] loss: 0.027\n",
            "[471,    87] loss: 0.022\n",
            "[471,    88] loss: 0.026\n",
            "[471,    89] loss: 0.027\n",
            "[471,    90] loss: 0.021\n",
            "[471,    91] loss: 0.029\n",
            "[471,    92] loss: 0.026\n",
            "[471,    93] loss: 0.025\n",
            "[471,    94] loss: 0.025\n",
            "[471,    95] loss: 0.027\n",
            "[471,    96] loss: 0.022\n",
            "[471,    97] loss: 0.021\n",
            "[471,    98] loss: 0.025\n",
            "[471,    99] loss: 0.025\n",
            "[471,   100] loss: 0.029\n",
            "[471,   101] loss: 0.022\n",
            "[471,   102] loss: 0.028\n",
            "[471,   103] loss: 0.030\n",
            "[471,   104] loss: 0.027\n",
            "[471,   105] loss: 0.023\n",
            "[471,   106] loss: 0.029\n",
            "[471,   107] loss: 0.026\n",
            "[471,   108] loss: 0.028\n",
            "[471,   109] loss: 0.021\n",
            "[471,   110] loss: 0.030\n",
            "[471,   111] loss: 0.023\n",
            "[471,   112] loss: 0.025\n",
            "[471,   113] loss: 0.027\n",
            "[471,   114] loss: 0.027\n",
            "[471,   115] loss: 0.027\n",
            "[471,   116] loss: 0.027\n",
            "[471,   117] loss: 0.029\n",
            "[471,   118] loss: 0.023\n",
            "[471,   119] loss: 0.025\n",
            "[471,   120] loss: 0.024\n",
            "[471,   121] loss: 0.027\n",
            "[471,   122] loss: 0.027\n",
            "[471,   123] loss: 0.026\n",
            "[471,   124] loss: 0.022\n",
            "[471,   125] loss: 0.027\n",
            "[471,   126] loss: 0.021\n",
            "[471,   127] loss: 0.020\n",
            "[471,   128] loss: 0.022\n",
            "[471,   129] loss: 0.023\n",
            "[471,   130] loss: 0.023\n",
            "[471,   131] loss: 0.030\n",
            "[471,   132] loss: 0.026\n",
            "[471,   133] loss: 0.023\n",
            "[471,   134] loss: 0.022\n",
            "[471,   135] loss: 0.020\n",
            "[471,   136] loss: 0.024\n",
            "[471,   137] loss: 0.027\n",
            "[471,   138] loss: 0.027\n",
            "[471,   139] loss: 0.022\n",
            "[471,   140] loss: 0.024\n",
            "[471,   141] loss: 0.024\n",
            "[471,   142] loss: 0.025\n",
            "[471,   143] loss: 0.026\n",
            "[471,   144] loss: 0.025\n",
            "[471,   145] loss: 0.023\n",
            "[471,   146] loss: 0.024\n",
            "[471,   147] loss: 0.026\n",
            "[472,     1] loss: 0.027\n",
            "[472,     2] loss: 0.020\n",
            "[472,     3] loss: 0.023\n",
            "[472,     4] loss: 0.021\n",
            "[472,     5] loss: 0.026\n",
            "[472,     6] loss: 0.025\n",
            "[472,     7] loss: 0.023\n",
            "[472,     8] loss: 0.027\n",
            "[472,     9] loss: 0.023\n",
            "[472,    10] loss: 0.024\n",
            "[472,    11] loss: 0.030\n",
            "[472,    12] loss: 0.031\n",
            "[472,    13] loss: 0.020\n",
            "[472,    14] loss: 0.025\n",
            "[472,    15] loss: 0.021\n",
            "[472,    16] loss: 0.027\n",
            "[472,    17] loss: 0.030\n",
            "[472,    18] loss: 0.027\n",
            "[472,    19] loss: 0.025\n",
            "[472,    20] loss: 0.029\n",
            "[472,    21] loss: 0.025\n",
            "[472,    22] loss: 0.023\n",
            "[472,    23] loss: 0.030\n",
            "[472,    24] loss: 0.028\n",
            "[472,    25] loss: 0.027\n",
            "[472,    26] loss: 0.024\n",
            "[472,    27] loss: 0.020\n",
            "[472,    28] loss: 0.026\n",
            "[472,    29] loss: 0.023\n",
            "[472,    30] loss: 0.028\n",
            "[472,    31] loss: 0.020\n",
            "[472,    32] loss: 0.025\n",
            "[472,    33] loss: 0.024\n",
            "[472,    34] loss: 0.020\n",
            "[472,    35] loss: 0.024\n",
            "[472,    36] loss: 0.028\n",
            "[472,    37] loss: 0.023\n",
            "[472,    38] loss: 0.023\n",
            "[472,    39] loss: 0.029\n",
            "[472,    40] loss: 0.030\n",
            "[472,    41] loss: 0.023\n",
            "[472,    42] loss: 0.028\n",
            "[472,    43] loss: 0.020\n",
            "[472,    44] loss: 0.029\n",
            "[472,    45] loss: 0.021\n",
            "[472,    46] loss: 0.024\n",
            "[472,    47] loss: 0.024\n",
            "[472,    48] loss: 0.022\n",
            "[472,    49] loss: 0.029\n",
            "[472,    50] loss: 0.030\n",
            "[472,    51] loss: 0.023\n",
            "[472,    52] loss: 0.026\n",
            "[472,    53] loss: 0.027\n",
            "[472,    54] loss: 0.023\n",
            "[472,    55] loss: 0.025\n",
            "[472,    56] loss: 0.023\n",
            "[472,    57] loss: 0.022\n",
            "[472,    58] loss: 0.027\n",
            "[472,    59] loss: 0.019\n",
            "[472,    60] loss: 0.025\n",
            "[472,    61] loss: 0.030\n",
            "[472,    62] loss: 0.024\n",
            "[472,    63] loss: 0.023\n",
            "[472,    64] loss: 0.027\n",
            "[472,    65] loss: 0.027\n",
            "[472,    66] loss: 0.023\n",
            "[472,    67] loss: 0.022\n",
            "[472,    68] loss: 0.026\n",
            "[472,    69] loss: 0.027\n",
            "[472,    70] loss: 0.023\n",
            "[472,    71] loss: 0.027\n",
            "[472,    72] loss: 0.027\n",
            "[472,    73] loss: 0.026\n",
            "[472,    74] loss: 0.020\n",
            "[472,    75] loss: 0.027\n",
            "[472,    76] loss: 0.028\n",
            "[472,    77] loss: 0.021\n",
            "[472,    78] loss: 0.025\n",
            "[472,    79] loss: 0.026\n",
            "[472,    80] loss: 0.026\n",
            "[472,    81] loss: 0.024\n",
            "[472,    82] loss: 0.018\n",
            "[472,    83] loss: 0.024\n",
            "[472,    84] loss: 0.026\n",
            "[472,    85] loss: 0.026\n",
            "[472,    86] loss: 0.026\n",
            "[472,    87] loss: 0.024\n",
            "[472,    88] loss: 0.020\n",
            "[472,    89] loss: 0.023\n",
            "[472,    90] loss: 0.020\n",
            "[472,    91] loss: 0.030\n",
            "[472,    92] loss: 0.030\n",
            "[472,    93] loss: 0.024\n",
            "[472,    94] loss: 0.029\n",
            "[472,    95] loss: 0.032\n",
            "[472,    96] loss: 0.027\n",
            "[472,    97] loss: 0.024\n",
            "[472,    98] loss: 0.027\n",
            "[472,    99] loss: 0.023\n",
            "[472,   100] loss: 0.020\n",
            "[472,   101] loss: 0.023\n",
            "[472,   102] loss: 0.020\n",
            "[472,   103] loss: 0.021\n",
            "[472,   104] loss: 0.023\n",
            "[472,   105] loss: 0.024\n",
            "[472,   106] loss: 0.024\n",
            "[472,   107] loss: 0.028\n",
            "[472,   108] loss: 0.023\n",
            "[472,   109] loss: 0.023\n",
            "[472,   110] loss: 0.022\n",
            "[472,   111] loss: 0.028\n",
            "[472,   112] loss: 0.028\n",
            "[472,   113] loss: 0.025\n",
            "[472,   114] loss: 0.030\n",
            "[472,   115] loss: 0.034\n",
            "[472,   116] loss: 0.025\n",
            "[472,   117] loss: 0.024\n",
            "[472,   118] loss: 0.032\n",
            "[472,   119] loss: 0.028\n",
            "[472,   120] loss: 0.023\n",
            "[472,   121] loss: 0.024\n",
            "[472,   122] loss: 0.027\n",
            "[472,   123] loss: 0.027\n",
            "[472,   124] loss: 0.023\n",
            "[472,   125] loss: 0.027\n",
            "[472,   126] loss: 0.032\n",
            "[472,   127] loss: 0.027\n",
            "[472,   128] loss: 0.025\n",
            "[472,   129] loss: 0.026\n",
            "[472,   130] loss: 0.026\n",
            "[472,   131] loss: 0.023\n",
            "[472,   132] loss: 0.028\n",
            "[472,   133] loss: 0.027\n",
            "[472,   134] loss: 0.029\n",
            "[472,   135] loss: 0.025\n",
            "[472,   136] loss: 0.031\n",
            "[472,   137] loss: 0.023\n",
            "[472,   138] loss: 0.027\n",
            "[472,   139] loss: 0.025\n",
            "[472,   140] loss: 0.026\n",
            "[472,   141] loss: 0.025\n",
            "[472,   142] loss: 0.019\n",
            "[472,   143] loss: 0.027\n",
            "[472,   144] loss: 0.026\n",
            "[472,   145] loss: 0.027\n",
            "[472,   146] loss: 0.024\n",
            "[472,   147] loss: 0.024\n",
            "[473,     1] loss: 0.022\n",
            "[473,     2] loss: 0.029\n",
            "[473,     3] loss: 0.028\n",
            "[473,     4] loss: 0.027\n",
            "[473,     5] loss: 0.025\n",
            "[473,     6] loss: 0.024\n",
            "[473,     7] loss: 0.027\n",
            "[473,     8] loss: 0.027\n",
            "[473,     9] loss: 0.024\n",
            "[473,    10] loss: 0.027\n",
            "[473,    11] loss: 0.027\n",
            "[473,    12] loss: 0.025\n",
            "[473,    13] loss: 0.018\n",
            "[473,    14] loss: 0.025\n",
            "[473,    15] loss: 0.025\n",
            "[473,    16] loss: 0.026\n",
            "[473,    17] loss: 0.025\n",
            "[473,    18] loss: 0.030\n",
            "[473,    19] loss: 0.027\n",
            "[473,    20] loss: 0.023\n",
            "[473,    21] loss: 0.031\n",
            "[473,    22] loss: 0.023\n",
            "[473,    23] loss: 0.024\n",
            "[473,    24] loss: 0.025\n",
            "[473,    25] loss: 0.024\n",
            "[473,    26] loss: 0.024\n",
            "[473,    27] loss: 0.030\n",
            "[473,    28] loss: 0.026\n",
            "[473,    29] loss: 0.027\n",
            "[473,    30] loss: 0.017\n",
            "[473,    31] loss: 0.023\n",
            "[473,    32] loss: 0.026\n",
            "[473,    33] loss: 0.027\n",
            "[473,    34] loss: 0.027\n",
            "[473,    35] loss: 0.026\n",
            "[473,    36] loss: 0.023\n",
            "[473,    37] loss: 0.029\n",
            "[473,    38] loss: 0.027\n",
            "[473,    39] loss: 0.027\n",
            "[473,    40] loss: 0.032\n",
            "[473,    41] loss: 0.024\n",
            "[473,    42] loss: 0.027\n",
            "[473,    43] loss: 0.023\n",
            "[473,    44] loss: 0.027\n",
            "[473,    45] loss: 0.023\n",
            "[473,    46] loss: 0.028\n",
            "[473,    47] loss: 0.024\n",
            "[473,    48] loss: 0.029\n",
            "[473,    49] loss: 0.024\n",
            "[473,    50] loss: 0.029\n",
            "[473,    51] loss: 0.022\n",
            "[473,    52] loss: 0.027\n",
            "[473,    53] loss: 0.024\n",
            "[473,    54] loss: 0.023\n",
            "[473,    55] loss: 0.025\n",
            "[473,    56] loss: 0.029\n",
            "[473,    57] loss: 0.022\n",
            "[473,    58] loss: 0.025\n",
            "[473,    59] loss: 0.021\n",
            "[473,    60] loss: 0.027\n",
            "[473,    61] loss: 0.024\n",
            "[473,    62] loss: 0.025\n",
            "[473,    63] loss: 0.030\n",
            "[473,    64] loss: 0.028\n",
            "[473,    65] loss: 0.027\n",
            "[473,    66] loss: 0.020\n",
            "[473,    67] loss: 0.022\n",
            "[473,    68] loss: 0.024\n",
            "[473,    69] loss: 0.021\n",
            "[473,    70] loss: 0.030\n",
            "[473,    71] loss: 0.024\n",
            "[473,    72] loss: 0.028\n",
            "[473,    73] loss: 0.021\n",
            "[473,    74] loss: 0.020\n",
            "[473,    75] loss: 0.025\n",
            "[473,    76] loss: 0.024\n",
            "[473,    77] loss: 0.025\n",
            "[473,    78] loss: 0.030\n",
            "[473,    79] loss: 0.027\n",
            "[473,    80] loss: 0.020\n",
            "[473,    81] loss: 0.026\n",
            "[473,    82] loss: 0.031\n",
            "[473,    83] loss: 0.026\n",
            "[473,    84] loss: 0.027\n",
            "[473,    85] loss: 0.025\n",
            "[473,    86] loss: 0.023\n",
            "[473,    87] loss: 0.023\n",
            "[473,    88] loss: 0.021\n",
            "[473,    89] loss: 0.018\n",
            "[473,    90] loss: 0.019\n",
            "[473,    91] loss: 0.023\n",
            "[473,    92] loss: 0.019\n",
            "[473,    93] loss: 0.020\n",
            "[473,    94] loss: 0.024\n",
            "[473,    95] loss: 0.025\n",
            "[473,    96] loss: 0.018\n",
            "[473,    97] loss: 0.028\n",
            "[473,    98] loss: 0.023\n",
            "[473,    99] loss: 0.023\n",
            "[473,   100] loss: 0.023\n",
            "[473,   101] loss: 0.025\n",
            "[473,   102] loss: 0.016\n",
            "[473,   103] loss: 0.025\n",
            "[473,   104] loss: 0.028\n",
            "[473,   105] loss: 0.025\n",
            "[473,   106] loss: 0.026\n",
            "[473,   107] loss: 0.024\n",
            "[473,   108] loss: 0.031\n",
            "[473,   109] loss: 0.027\n",
            "[473,   110] loss: 0.020\n",
            "[473,   111] loss: 0.026\n",
            "[473,   112] loss: 0.025\n",
            "[473,   113] loss: 0.026\n",
            "[473,   114] loss: 0.022\n",
            "[473,   115] loss: 0.021\n",
            "[473,   116] loss: 0.029\n",
            "[473,   117] loss: 0.025\n",
            "[473,   118] loss: 0.028\n",
            "[473,   119] loss: 0.028\n",
            "[473,   120] loss: 0.027\n",
            "[473,   121] loss: 0.026\n",
            "[473,   122] loss: 0.023\n",
            "[473,   123] loss: 0.023\n",
            "[473,   124] loss: 0.032\n",
            "[473,   125] loss: 0.030\n",
            "[473,   126] loss: 0.025\n",
            "[473,   127] loss: 0.023\n",
            "[473,   128] loss: 0.027\n",
            "[473,   129] loss: 0.029\n",
            "[473,   130] loss: 0.026\n",
            "[473,   131] loss: 0.026\n",
            "[473,   132] loss: 0.024\n",
            "[473,   133] loss: 0.026\n",
            "[473,   134] loss: 0.025\n",
            "[473,   135] loss: 0.024\n",
            "[473,   136] loss: 0.027\n",
            "[473,   137] loss: 0.027\n",
            "[473,   138] loss: 0.029\n",
            "[473,   139] loss: 0.031\n",
            "[473,   140] loss: 0.025\n",
            "[473,   141] loss: 0.028\n",
            "[473,   142] loss: 0.028\n",
            "[473,   143] loss: 0.023\n",
            "[473,   144] loss: 0.024\n",
            "[473,   145] loss: 0.021\n",
            "[473,   146] loss: 0.027\n",
            "[473,   147] loss: 0.013\n",
            "[474,     1] loss: 0.022\n",
            "[474,     2] loss: 0.025\n",
            "[474,     3] loss: 0.024\n",
            "[474,     4] loss: 0.024\n",
            "[474,     5] loss: 0.026\n",
            "[474,     6] loss: 0.026\n",
            "[474,     7] loss: 0.028\n",
            "[474,     8] loss: 0.026\n",
            "[474,     9] loss: 0.023\n",
            "[474,    10] loss: 0.027\n",
            "[474,    11] loss: 0.025\n",
            "[474,    12] loss: 0.022\n",
            "[474,    13] loss: 0.024\n",
            "[474,    14] loss: 0.023\n",
            "[474,    15] loss: 0.023\n",
            "[474,    16] loss: 0.020\n",
            "[474,    17] loss: 0.022\n",
            "[474,    18] loss: 0.030\n",
            "[474,    19] loss: 0.028\n",
            "[474,    20] loss: 0.027\n",
            "[474,    21] loss: 0.025\n",
            "[474,    22] loss: 0.021\n",
            "[474,    23] loss: 0.026\n",
            "[474,    24] loss: 0.023\n",
            "[474,    25] loss: 0.027\n",
            "[474,    26] loss: 0.024\n",
            "[474,    27] loss: 0.023\n",
            "[474,    28] loss: 0.024\n",
            "[474,    29] loss: 0.025\n",
            "[474,    30] loss: 0.021\n",
            "[474,    31] loss: 0.030\n",
            "[474,    32] loss: 0.023\n",
            "[474,    33] loss: 0.021\n",
            "[474,    34] loss: 0.028\n",
            "[474,    35] loss: 0.027\n",
            "[474,    36] loss: 0.023\n",
            "[474,    37] loss: 0.026\n",
            "[474,    38] loss: 0.029\n",
            "[474,    39] loss: 0.030\n",
            "[474,    40] loss: 0.018\n",
            "[474,    41] loss: 0.026\n",
            "[474,    42] loss: 0.030\n",
            "[474,    43] loss: 0.023\n",
            "[474,    44] loss: 0.027\n",
            "[474,    45] loss: 0.023\n",
            "[474,    46] loss: 0.028\n",
            "[474,    47] loss: 0.023\n",
            "[474,    48] loss: 0.024\n",
            "[474,    49] loss: 0.022\n",
            "[474,    50] loss: 0.028\n",
            "[474,    51] loss: 0.027\n",
            "[474,    52] loss: 0.018\n",
            "[474,    53] loss: 0.032\n",
            "[474,    54] loss: 0.023\n",
            "[474,    55] loss: 0.027\n",
            "[474,    56] loss: 0.027\n",
            "[474,    57] loss: 0.024\n",
            "[474,    58] loss: 0.027\n",
            "[474,    59] loss: 0.025\n",
            "[474,    60] loss: 0.020\n",
            "[474,    61] loss: 0.027\n",
            "[474,    62] loss: 0.030\n",
            "[474,    63] loss: 0.026\n",
            "[474,    64] loss: 0.030\n",
            "[474,    65] loss: 0.023\n",
            "[474,    66] loss: 0.023\n",
            "[474,    67] loss: 0.024\n",
            "[474,    68] loss: 0.030\n",
            "[474,    69] loss: 0.028\n",
            "[474,    70] loss: 0.025\n",
            "[474,    71] loss: 0.025\n",
            "[474,    72] loss: 0.029\n",
            "[474,    73] loss: 0.027\n",
            "[474,    74] loss: 0.024\n",
            "[474,    75] loss: 0.024\n",
            "[474,    76] loss: 0.022\n",
            "[474,    77] loss: 0.028\n",
            "[474,    78] loss: 0.028\n",
            "[474,    79] loss: 0.025\n",
            "[474,    80] loss: 0.027\n",
            "[474,    81] loss: 0.024\n",
            "[474,    82] loss: 0.027\n",
            "[474,    83] loss: 0.027\n",
            "[474,    84] loss: 0.024\n",
            "[474,    85] loss: 0.026\n",
            "[474,    86] loss: 0.027\n",
            "[474,    87] loss: 0.028\n",
            "[474,    88] loss: 0.023\n",
            "[474,    89] loss: 0.028\n",
            "[474,    90] loss: 0.029\n",
            "[474,    91] loss: 0.027\n",
            "[474,    92] loss: 0.027\n",
            "[474,    93] loss: 0.028\n",
            "[474,    94] loss: 0.027\n",
            "[474,    95] loss: 0.024\n",
            "[474,    96] loss: 0.020\n",
            "[474,    97] loss: 0.023\n",
            "[474,    98] loss: 0.023\n",
            "[474,    99] loss: 0.024\n",
            "[474,   100] loss: 0.026\n",
            "[474,   101] loss: 0.032\n",
            "[474,   102] loss: 0.018\n",
            "[474,   103] loss: 0.027\n",
            "[474,   104] loss: 0.029\n",
            "[474,   105] loss: 0.020\n",
            "[474,   106] loss: 0.026\n",
            "[474,   107] loss: 0.024\n",
            "[474,   108] loss: 0.021\n",
            "[474,   109] loss: 0.025\n",
            "[474,   110] loss: 0.029\n",
            "[474,   111] loss: 0.023\n",
            "[474,   112] loss: 0.025\n",
            "[474,   113] loss: 0.027\n",
            "[474,   114] loss: 0.027\n",
            "[474,   115] loss: 0.025\n",
            "[474,   116] loss: 0.023\n",
            "[474,   117] loss: 0.020\n",
            "[474,   118] loss: 0.021\n",
            "[474,   119] loss: 0.022\n",
            "[474,   120] loss: 0.024\n",
            "[474,   121] loss: 0.030\n",
            "[474,   122] loss: 0.022\n",
            "[474,   123] loss: 0.026\n",
            "[474,   124] loss: 0.026\n",
            "[474,   125] loss: 0.023\n",
            "[474,   126] loss: 0.026\n",
            "[474,   127] loss: 0.030\n",
            "[474,   128] loss: 0.025\n",
            "[474,   129] loss: 0.020\n",
            "[474,   130] loss: 0.025\n",
            "[474,   131] loss: 0.030\n",
            "[474,   132] loss: 0.027\n",
            "[474,   133] loss: 0.027\n",
            "[474,   134] loss: 0.026\n",
            "[474,   135] loss: 0.018\n",
            "[474,   136] loss: 0.029\n",
            "[474,   137] loss: 0.023\n",
            "[474,   138] loss: 0.031\n",
            "[474,   139] loss: 0.021\n",
            "[474,   140] loss: 0.023\n",
            "[474,   141] loss: 0.027\n",
            "[474,   142] loss: 0.030\n",
            "[474,   143] loss: 0.021\n",
            "[474,   144] loss: 0.027\n",
            "[474,   145] loss: 0.023\n",
            "[474,   146] loss: 0.023\n",
            "[474,   147] loss: 0.029\n",
            "[475,     1] loss: 0.025\n",
            "[475,     2] loss: 0.025\n",
            "[475,     3] loss: 0.024\n",
            "[475,     4] loss: 0.028\n",
            "[475,     5] loss: 0.023\n",
            "[475,     6] loss: 0.027\n",
            "[475,     7] loss: 0.030\n",
            "[475,     8] loss: 0.023\n",
            "[475,     9] loss: 0.024\n",
            "[475,    10] loss: 0.027\n",
            "[475,    11] loss: 0.025\n",
            "[475,    12] loss: 0.026\n",
            "[475,    13] loss: 0.028\n",
            "[475,    14] loss: 0.019\n",
            "[475,    15] loss: 0.027\n",
            "[475,    16] loss: 0.024\n",
            "[475,    17] loss: 0.027\n",
            "[475,    18] loss: 0.024\n",
            "[475,    19] loss: 0.024\n",
            "[475,    20] loss: 0.021\n",
            "[475,    21] loss: 0.022\n",
            "[475,    22] loss: 0.030\n",
            "[475,    23] loss: 0.025\n",
            "[475,    24] loss: 0.028\n",
            "[475,    25] loss: 0.026\n",
            "[475,    26] loss: 0.023\n",
            "[475,    27] loss: 0.025\n",
            "[475,    28] loss: 0.023\n",
            "[475,    29] loss: 0.022\n",
            "[475,    30] loss: 0.025\n",
            "[475,    31] loss: 0.028\n",
            "[475,    32] loss: 0.022\n",
            "[475,    33] loss: 0.026\n",
            "[475,    34] loss: 0.022\n",
            "[475,    35] loss: 0.024\n",
            "[475,    36] loss: 0.026\n",
            "[475,    37] loss: 0.024\n",
            "[475,    38] loss: 0.027\n",
            "[475,    39] loss: 0.026\n",
            "[475,    40] loss: 0.023\n",
            "[475,    41] loss: 0.026\n",
            "[475,    42] loss: 0.028\n",
            "[475,    43] loss: 0.023\n",
            "[475,    44] loss: 0.028\n",
            "[475,    45] loss: 0.023\n",
            "[475,    46] loss: 0.022\n",
            "[475,    47] loss: 0.027\n",
            "[475,    48] loss: 0.027\n",
            "[475,    49] loss: 0.029\n",
            "[475,    50] loss: 0.025\n",
            "[475,    51] loss: 0.026\n",
            "[475,    52] loss: 0.030\n",
            "[475,    53] loss: 0.026\n",
            "[475,    54] loss: 0.035\n",
            "[475,    55] loss: 0.027\n",
            "[475,    56] loss: 0.030\n",
            "[475,    57] loss: 0.019\n",
            "[475,    58] loss: 0.021\n",
            "[475,    59] loss: 0.026\n",
            "[475,    60] loss: 0.031\n",
            "[475,    61] loss: 0.021\n",
            "[475,    62] loss: 0.027\n",
            "[475,    63] loss: 0.020\n",
            "[475,    64] loss: 0.029\n",
            "[475,    65] loss: 0.021\n",
            "[475,    66] loss: 0.027\n",
            "[475,    67] loss: 0.025\n",
            "[475,    68] loss: 0.027\n",
            "[475,    69] loss: 0.030\n",
            "[475,    70] loss: 0.026\n",
            "[475,    71] loss: 0.030\n",
            "[475,    72] loss: 0.031\n",
            "[475,    73] loss: 0.030\n",
            "[475,    74] loss: 0.021\n",
            "[475,    75] loss: 0.024\n",
            "[475,    76] loss: 0.024\n",
            "[475,    77] loss: 0.024\n",
            "[475,    78] loss: 0.023\n",
            "[475,    79] loss: 0.033\n",
            "[475,    80] loss: 0.027\n",
            "[475,    81] loss: 0.028\n",
            "[475,    82] loss: 0.023\n",
            "[475,    83] loss: 0.021\n",
            "[475,    84] loss: 0.023\n",
            "[475,    85] loss: 0.026\n",
            "[475,    86] loss: 0.023\n",
            "[475,    87] loss: 0.026\n",
            "[475,    88] loss: 0.028\n",
            "[475,    89] loss: 0.021\n",
            "[475,    90] loss: 0.027\n",
            "[475,    91] loss: 0.027\n",
            "[475,    92] loss: 0.023\n",
            "[475,    93] loss: 0.030\n",
            "[475,    94] loss: 0.024\n",
            "[475,    95] loss: 0.019\n",
            "[475,    96] loss: 0.027\n",
            "[475,    97] loss: 0.028\n",
            "[475,    98] loss: 0.021\n",
            "[475,    99] loss: 0.017\n",
            "[475,   100] loss: 0.029\n",
            "[475,   101] loss: 0.031\n",
            "[475,   102] loss: 0.027\n",
            "[475,   103] loss: 0.025\n",
            "[475,   104] loss: 0.016\n",
            "[475,   105] loss: 0.033\n",
            "[475,   106] loss: 0.021\n",
            "[475,   107] loss: 0.023\n",
            "[475,   108] loss: 0.027\n",
            "[475,   109] loss: 0.022\n",
            "[475,   110] loss: 0.021\n",
            "[475,   111] loss: 0.026\n",
            "[475,   112] loss: 0.027\n",
            "[475,   113] loss: 0.027\n",
            "[475,   114] loss: 0.024\n",
            "[475,   115] loss: 0.023\n",
            "[475,   116] loss: 0.028\n",
            "[475,   117] loss: 0.027\n",
            "[475,   118] loss: 0.027\n",
            "[475,   119] loss: 0.020\n",
            "[475,   120] loss: 0.023\n",
            "[475,   121] loss: 0.020\n",
            "[475,   122] loss: 0.030\n",
            "[475,   123] loss: 0.027\n",
            "[475,   124] loss: 0.026\n",
            "[475,   125] loss: 0.029\n",
            "[475,   126] loss: 0.022\n",
            "[475,   127] loss: 0.024\n",
            "[475,   128] loss: 0.032\n",
            "[475,   129] loss: 0.026\n",
            "[475,   130] loss: 0.025\n",
            "[475,   131] loss: 0.023\n",
            "[475,   132] loss: 0.019\n",
            "[475,   133] loss: 0.024\n",
            "[475,   134] loss: 0.020\n",
            "[475,   135] loss: 0.027\n",
            "[475,   136] loss: 0.018\n",
            "[475,   137] loss: 0.026\n",
            "[475,   138] loss: 0.027\n",
            "[475,   139] loss: 0.026\n",
            "[475,   140] loss: 0.023\n",
            "[475,   141] loss: 0.021\n",
            "[475,   142] loss: 0.027\n",
            "[475,   143] loss: 0.022\n",
            "[475,   144] loss: 0.023\n",
            "[475,   145] loss: 0.030\n",
            "[475,   146] loss: 0.029\n",
            "[475,   147] loss: 0.024\n",
            "[476,     1] loss: 0.027\n",
            "[476,     2] loss: 0.031\n",
            "[476,     3] loss: 0.028\n",
            "[476,     4] loss: 0.027\n",
            "[476,     5] loss: 0.023\n",
            "[476,     6] loss: 0.027\n",
            "[476,     7] loss: 0.027\n",
            "[476,     8] loss: 0.025\n",
            "[476,     9] loss: 0.025\n",
            "[476,    10] loss: 0.025\n",
            "[476,    11] loss: 0.027\n",
            "[476,    12] loss: 0.029\n",
            "[476,    13] loss: 0.020\n",
            "[476,    14] loss: 0.021\n",
            "[476,    15] loss: 0.021\n",
            "[476,    16] loss: 0.020\n",
            "[476,    17] loss: 0.030\n",
            "[476,    18] loss: 0.029\n",
            "[476,    19] loss: 0.027\n",
            "[476,    20] loss: 0.027\n",
            "[476,    21] loss: 0.027\n",
            "[476,    22] loss: 0.027\n",
            "[476,    23] loss: 0.021\n",
            "[476,    24] loss: 0.023\n",
            "[476,    25] loss: 0.021\n",
            "[476,    26] loss: 0.022\n",
            "[476,    27] loss: 0.025\n",
            "[476,    28] loss: 0.029\n",
            "[476,    29] loss: 0.023\n",
            "[476,    30] loss: 0.023\n",
            "[476,    31] loss: 0.026\n",
            "[476,    32] loss: 0.032\n",
            "[476,    33] loss: 0.022\n",
            "[476,    34] loss: 0.023\n",
            "[476,    35] loss: 0.027\n",
            "[476,    36] loss: 0.019\n",
            "[476,    37] loss: 0.024\n",
            "[476,    38] loss: 0.024\n",
            "[476,    39] loss: 0.023\n",
            "[476,    40] loss: 0.030\n",
            "[476,    41] loss: 0.029\n",
            "[476,    42] loss: 0.025\n",
            "[476,    43] loss: 0.029\n",
            "[476,    44] loss: 0.025\n",
            "[476,    45] loss: 0.020\n",
            "[476,    46] loss: 0.026\n",
            "[476,    47] loss: 0.027\n",
            "[476,    48] loss: 0.029\n",
            "[476,    49] loss: 0.027\n",
            "[476,    50] loss: 0.027\n",
            "[476,    51] loss: 0.024\n",
            "[476,    52] loss: 0.029\n",
            "[476,    53] loss: 0.023\n",
            "[476,    54] loss: 0.026\n",
            "[476,    55] loss: 0.020\n",
            "[476,    56] loss: 0.024\n",
            "[476,    57] loss: 0.021\n",
            "[476,    58] loss: 0.023\n",
            "[476,    59] loss: 0.030\n",
            "[476,    60] loss: 0.030\n",
            "[476,    61] loss: 0.019\n",
            "[476,    62] loss: 0.022\n",
            "[476,    63] loss: 0.026\n",
            "[476,    64] loss: 0.026\n",
            "[476,    65] loss: 0.025\n",
            "[476,    66] loss: 0.026\n",
            "[476,    67] loss: 0.024\n",
            "[476,    68] loss: 0.020\n",
            "[476,    69] loss: 0.023\n",
            "[476,    70] loss: 0.027\n",
            "[476,    71] loss: 0.027\n",
            "[476,    72] loss: 0.023\n",
            "[476,    73] loss: 0.024\n",
            "[476,    74] loss: 0.022\n",
            "[476,    75] loss: 0.023\n",
            "[476,    76] loss: 0.025\n",
            "[476,    77] loss: 0.024\n",
            "[476,    78] loss: 0.024\n",
            "[476,    79] loss: 0.024\n",
            "[476,    80] loss: 0.029\n",
            "[476,    81] loss: 0.026\n",
            "[476,    82] loss: 0.023\n",
            "[476,    83] loss: 0.030\n",
            "[476,    84] loss: 0.028\n",
            "[476,    85] loss: 0.027\n",
            "[476,    86] loss: 0.027\n",
            "[476,    87] loss: 0.024\n",
            "[476,    88] loss: 0.030\n",
            "[476,    89] loss: 0.023\n",
            "[476,    90] loss: 0.027\n",
            "[476,    91] loss: 0.027\n",
            "[476,    92] loss: 0.023\n",
            "[476,    93] loss: 0.024\n",
            "[476,    94] loss: 0.025\n",
            "[476,    95] loss: 0.027\n",
            "[476,    96] loss: 0.027\n",
            "[476,    97] loss: 0.026\n",
            "[476,    98] loss: 0.023\n",
            "[476,    99] loss: 0.029\n",
            "[476,   100] loss: 0.025\n",
            "[476,   101] loss: 0.023\n",
            "[476,   102] loss: 0.027\n",
            "[476,   103] loss: 0.022\n",
            "[476,   104] loss: 0.023\n",
            "[476,   105] loss: 0.025\n",
            "[476,   106] loss: 0.028\n",
            "[476,   107] loss: 0.024\n",
            "[476,   108] loss: 0.025\n",
            "[476,   109] loss: 0.023\n",
            "[476,   110] loss: 0.031\n",
            "[476,   111] loss: 0.027\n",
            "[476,   112] loss: 0.021\n",
            "[476,   113] loss: 0.024\n",
            "[476,   114] loss: 0.021\n",
            "[476,   115] loss: 0.027\n",
            "[476,   116] loss: 0.030\n",
            "[476,   117] loss: 0.025\n",
            "[476,   118] loss: 0.023\n",
            "[476,   119] loss: 0.022\n",
            "[476,   120] loss: 0.023\n",
            "[476,   121] loss: 0.023\n",
            "[476,   122] loss: 0.026\n",
            "[476,   123] loss: 0.022\n",
            "[476,   124] loss: 0.027\n",
            "[476,   125] loss: 0.024\n",
            "[476,   126] loss: 0.023\n",
            "[476,   127] loss: 0.026\n",
            "[476,   128] loss: 0.024\n",
            "[476,   129] loss: 0.032\n",
            "[476,   130] loss: 0.022\n",
            "[476,   131] loss: 0.030\n",
            "[476,   132] loss: 0.023\n",
            "[476,   133] loss: 0.023\n",
            "[476,   134] loss: 0.024\n",
            "[476,   135] loss: 0.027\n",
            "[476,   136] loss: 0.022\n",
            "[476,   137] loss: 0.027\n",
            "[476,   138] loss: 0.024\n",
            "[476,   139] loss: 0.031\n",
            "[476,   140] loss: 0.027\n",
            "[476,   141] loss: 0.023\n",
            "[476,   142] loss: 0.025\n",
            "[476,   143] loss: 0.025\n",
            "[476,   144] loss: 0.027\n",
            "[476,   145] loss: 0.022\n",
            "[476,   146] loss: 0.026\n",
            "[476,   147] loss: 0.026\n",
            "[477,     1] loss: 0.027\n",
            "[477,     2] loss: 0.024\n",
            "[477,     3] loss: 0.022\n",
            "[477,     4] loss: 0.030\n",
            "[477,     5] loss: 0.027\n",
            "[477,     6] loss: 0.016\n",
            "[477,     7] loss: 0.027\n",
            "[477,     8] loss: 0.031\n",
            "[477,     9] loss: 0.022\n",
            "[477,    10] loss: 0.025\n",
            "[477,    11] loss: 0.026\n",
            "[477,    12] loss: 0.029\n",
            "[477,    13] loss: 0.023\n",
            "[477,    14] loss: 0.023\n",
            "[477,    15] loss: 0.017\n",
            "[477,    16] loss: 0.026\n",
            "[477,    17] loss: 0.019\n",
            "[477,    18] loss: 0.024\n",
            "[477,    19] loss: 0.030\n",
            "[477,    20] loss: 0.024\n",
            "[477,    21] loss: 0.021\n",
            "[477,    22] loss: 0.023\n",
            "[477,    23] loss: 0.030\n",
            "[477,    24] loss: 0.023\n",
            "[477,    25] loss: 0.028\n",
            "[477,    26] loss: 0.022\n",
            "[477,    27] loss: 0.020\n",
            "[477,    28] loss: 0.027\n",
            "[477,    29] loss: 0.025\n",
            "[477,    30] loss: 0.028\n",
            "[477,    31] loss: 0.027\n",
            "[477,    32] loss: 0.025\n",
            "[477,    33] loss: 0.028\n",
            "[477,    34] loss: 0.025\n",
            "[477,    35] loss: 0.023\n",
            "[477,    36] loss: 0.029\n",
            "[477,    37] loss: 0.024\n",
            "[477,    38] loss: 0.027\n",
            "[477,    39] loss: 0.029\n",
            "[477,    40] loss: 0.028\n",
            "[477,    41] loss: 0.028\n",
            "[477,    42] loss: 0.028\n",
            "[477,    43] loss: 0.026\n",
            "[477,    44] loss: 0.026\n",
            "[477,    45] loss: 0.029\n",
            "[477,    46] loss: 0.024\n",
            "[477,    47] loss: 0.029\n",
            "[477,    48] loss: 0.026\n",
            "[477,    49] loss: 0.024\n",
            "[477,    50] loss: 0.025\n",
            "[477,    51] loss: 0.022\n",
            "[477,    52] loss: 0.027\n",
            "[477,    53] loss: 0.023\n",
            "[477,    54] loss: 0.025\n",
            "[477,    55] loss: 0.025\n",
            "[477,    56] loss: 0.029\n",
            "[477,    57] loss: 0.027\n",
            "[477,    58] loss: 0.023\n",
            "[477,    59] loss: 0.028\n",
            "[477,    60] loss: 0.031\n",
            "[477,    61] loss: 0.023\n",
            "[477,    62] loss: 0.023\n",
            "[477,    63] loss: 0.020\n",
            "[477,    64] loss: 0.025\n",
            "[477,    65] loss: 0.030\n",
            "[477,    66] loss: 0.029\n",
            "[477,    67] loss: 0.027\n",
            "[477,    68] loss: 0.021\n",
            "[477,    69] loss: 0.023\n",
            "[477,    70] loss: 0.027\n",
            "[477,    71] loss: 0.022\n",
            "[477,    72] loss: 0.024\n",
            "[477,    73] loss: 0.023\n",
            "[477,    74] loss: 0.027\n",
            "[477,    75] loss: 0.023\n",
            "[477,    76] loss: 0.030\n",
            "[477,    77] loss: 0.026\n",
            "[477,    78] loss: 0.023\n",
            "[477,    79] loss: 0.027\n",
            "[477,    80] loss: 0.023\n",
            "[477,    81] loss: 0.023\n",
            "[477,    82] loss: 0.029\n",
            "[477,    83] loss: 0.030\n",
            "[477,    84] loss: 0.024\n",
            "[477,    85] loss: 0.023\n",
            "[477,    86] loss: 0.023\n",
            "[477,    87] loss: 0.027\n",
            "[477,    88] loss: 0.023\n",
            "[477,    89] loss: 0.023\n",
            "[477,    90] loss: 0.026\n",
            "[477,    91] loss: 0.032\n",
            "[477,    92] loss: 0.030\n",
            "[477,    93] loss: 0.020\n",
            "[477,    94] loss: 0.030\n",
            "[477,    95] loss: 0.023\n",
            "[477,    96] loss: 0.023\n",
            "[477,    97] loss: 0.028\n",
            "[477,    98] loss: 0.022\n",
            "[477,    99] loss: 0.020\n",
            "[477,   100] loss: 0.027\n",
            "[477,   101] loss: 0.028\n",
            "[477,   102] loss: 0.026\n",
            "[477,   103] loss: 0.027\n",
            "[477,   104] loss: 0.026\n",
            "[477,   105] loss: 0.020\n",
            "[477,   106] loss: 0.025\n",
            "[477,   107] loss: 0.022\n",
            "[477,   108] loss: 0.028\n",
            "[477,   109] loss: 0.026\n",
            "[477,   110] loss: 0.019\n",
            "[477,   111] loss: 0.027\n",
            "[477,   112] loss: 0.021\n",
            "[477,   113] loss: 0.020\n",
            "[477,   114] loss: 0.022\n",
            "[477,   115] loss: 0.024\n",
            "[477,   116] loss: 0.028\n",
            "[477,   117] loss: 0.024\n",
            "[477,   118] loss: 0.019\n",
            "[477,   119] loss: 0.026\n",
            "[477,   120] loss: 0.018\n",
            "[477,   121] loss: 0.020\n",
            "[477,   122] loss: 0.026\n",
            "[477,   123] loss: 0.027\n",
            "[477,   124] loss: 0.026\n",
            "[477,   125] loss: 0.023\n",
            "[477,   126] loss: 0.023\n",
            "[477,   127] loss: 0.028\n",
            "[477,   128] loss: 0.028\n",
            "[477,   129] loss: 0.030\n",
            "[477,   130] loss: 0.029\n",
            "[477,   131] loss: 0.026\n",
            "[477,   132] loss: 0.022\n",
            "[477,   133] loss: 0.029\n",
            "[477,   134] loss: 0.027\n",
            "[477,   135] loss: 0.028\n",
            "[477,   136] loss: 0.026\n",
            "[477,   137] loss: 0.023\n",
            "[477,   138] loss: 0.026\n",
            "[477,   139] loss: 0.032\n",
            "[477,   140] loss: 0.028\n",
            "[477,   141] loss: 0.023\n",
            "[477,   142] loss: 0.019\n",
            "[477,   143] loss: 0.023\n",
            "[477,   144] loss: 0.026\n",
            "[477,   145] loss: 0.026\n",
            "[477,   146] loss: 0.023\n",
            "[477,   147] loss: 0.029\n",
            "[478,     1] loss: 0.027\n",
            "[478,     2] loss: 0.025\n",
            "[478,     3] loss: 0.030\n",
            "[478,     4] loss: 0.023\n",
            "[478,     5] loss: 0.023\n",
            "[478,     6] loss: 0.021\n",
            "[478,     7] loss: 0.027\n",
            "[478,     8] loss: 0.029\n",
            "[478,     9] loss: 0.029\n",
            "[478,    10] loss: 0.026\n",
            "[478,    11] loss: 0.027\n",
            "[478,    12] loss: 0.030\n",
            "[478,    13] loss: 0.023\n",
            "[478,    14] loss: 0.029\n",
            "[478,    15] loss: 0.029\n",
            "[478,    16] loss: 0.027\n",
            "[478,    17] loss: 0.028\n",
            "[478,    18] loss: 0.019\n",
            "[478,    19] loss: 0.027\n",
            "[478,    20] loss: 0.024\n",
            "[478,    21] loss: 0.024\n",
            "[478,    22] loss: 0.023\n",
            "[478,    23] loss: 0.025\n",
            "[478,    24] loss: 0.026\n",
            "[478,    25] loss: 0.028\n",
            "[478,    26] loss: 0.026\n",
            "[478,    27] loss: 0.023\n",
            "[478,    28] loss: 0.019\n",
            "[478,    29] loss: 0.024\n",
            "[478,    30] loss: 0.030\n",
            "[478,    31] loss: 0.025\n",
            "[478,    32] loss: 0.030\n",
            "[478,    33] loss: 0.020\n",
            "[478,    34] loss: 0.023\n",
            "[478,    35] loss: 0.027\n",
            "[478,    36] loss: 0.023\n",
            "[478,    37] loss: 0.023\n",
            "[478,    38] loss: 0.029\n",
            "[478,    39] loss: 0.029\n",
            "[478,    40] loss: 0.023\n",
            "[478,    41] loss: 0.027\n",
            "[478,    42] loss: 0.024\n",
            "[478,    43] loss: 0.023\n",
            "[478,    44] loss: 0.025\n",
            "[478,    45] loss: 0.024\n",
            "[478,    46] loss: 0.020\n",
            "[478,    47] loss: 0.025\n",
            "[478,    48] loss: 0.027\n",
            "[478,    49] loss: 0.025\n",
            "[478,    50] loss: 0.027\n",
            "[478,    51] loss: 0.030\n",
            "[478,    52] loss: 0.026\n",
            "[478,    53] loss: 0.023\n",
            "[478,    54] loss: 0.024\n",
            "[478,    55] loss: 0.025\n",
            "[478,    56] loss: 0.027\n",
            "[478,    57] loss: 0.029\n",
            "[478,    58] loss: 0.026\n",
            "[478,    59] loss: 0.026\n",
            "[478,    60] loss: 0.020\n",
            "[478,    61] loss: 0.030\n",
            "[478,    62] loss: 0.028\n",
            "[478,    63] loss: 0.024\n",
            "[478,    64] loss: 0.025\n",
            "[478,    65] loss: 0.022\n",
            "[478,    66] loss: 0.023\n",
            "[478,    67] loss: 0.023\n",
            "[478,    68] loss: 0.023\n",
            "[478,    69] loss: 0.024\n",
            "[478,    70] loss: 0.017\n",
            "[478,    71] loss: 0.022\n",
            "[478,    72] loss: 0.023\n",
            "[478,    73] loss: 0.020\n",
            "[478,    74] loss: 0.029\n",
            "[478,    75] loss: 0.026\n",
            "[478,    76] loss: 0.020\n",
            "[478,    77] loss: 0.030\n",
            "[478,    78] loss: 0.020\n",
            "[478,    79] loss: 0.026\n",
            "[478,    80] loss: 0.021\n",
            "[478,    81] loss: 0.024\n",
            "[478,    82] loss: 0.023\n",
            "[478,    83] loss: 0.027\n",
            "[478,    84] loss: 0.028\n",
            "[478,    85] loss: 0.027\n",
            "[478,    86] loss: 0.023\n",
            "[478,    87] loss: 0.024\n",
            "[478,    88] loss: 0.021\n",
            "[478,    89] loss: 0.027\n",
            "[478,    90] loss: 0.025\n",
            "[478,    91] loss: 0.027\n",
            "[478,    92] loss: 0.031\n",
            "[478,    93] loss: 0.027\n",
            "[478,    94] loss: 0.027\n",
            "[478,    95] loss: 0.027\n",
            "[478,    96] loss: 0.019\n",
            "[478,    97] loss: 0.023\n",
            "[478,    98] loss: 0.027\n",
            "[478,    99] loss: 0.025\n",
            "[478,   100] loss: 0.023\n",
            "[478,   101] loss: 0.027\n",
            "[478,   102] loss: 0.025\n",
            "[478,   103] loss: 0.023\n",
            "[478,   104] loss: 0.027\n",
            "[478,   105] loss: 0.023\n",
            "[478,   106] loss: 0.027\n",
            "[478,   107] loss: 0.023\n",
            "[478,   108] loss: 0.024\n",
            "[478,   109] loss: 0.022\n",
            "[478,   110] loss: 0.030\n",
            "[478,   111] loss: 0.028\n",
            "[478,   112] loss: 0.027\n",
            "[478,   113] loss: 0.033\n",
            "[478,   114] loss: 0.023\n",
            "[478,   115] loss: 0.028\n",
            "[478,   116] loss: 0.025\n",
            "[478,   117] loss: 0.019\n",
            "[478,   118] loss: 0.021\n",
            "[478,   119] loss: 0.023\n",
            "[478,   120] loss: 0.025\n",
            "[478,   121] loss: 0.028\n",
            "[478,   122] loss: 0.024\n",
            "[478,   123] loss: 0.024\n",
            "[478,   124] loss: 0.025\n",
            "[478,   125] loss: 0.024\n",
            "[478,   126] loss: 0.022\n",
            "[478,   127] loss: 0.028\n",
            "[478,   128] loss: 0.022\n",
            "[478,   129] loss: 0.023\n",
            "[478,   130] loss: 0.027\n",
            "[478,   131] loss: 0.030\n",
            "[478,   132] loss: 0.026\n",
            "[478,   133] loss: 0.024\n",
            "[478,   134] loss: 0.025\n",
            "[478,   135] loss: 0.028\n",
            "[478,   136] loss: 0.027\n",
            "[478,   137] loss: 0.025\n",
            "[478,   138] loss: 0.026\n",
            "[478,   139] loss: 0.021\n",
            "[478,   140] loss: 0.028\n",
            "[478,   141] loss: 0.029\n",
            "[478,   142] loss: 0.024\n",
            "[478,   143] loss: 0.027\n",
            "[478,   144] loss: 0.029\n",
            "[478,   145] loss: 0.021\n",
            "[478,   146] loss: 0.025\n",
            "[478,   147] loss: 0.029\n",
            "[479,     1] loss: 0.026\n",
            "[479,     2] loss: 0.027\n",
            "[479,     3] loss: 0.024\n",
            "[479,     4] loss: 0.029\n",
            "[479,     5] loss: 0.023\n",
            "[479,     6] loss: 0.022\n",
            "[479,     7] loss: 0.023\n",
            "[479,     8] loss: 0.025\n",
            "[479,     9] loss: 0.023\n",
            "[479,    10] loss: 0.027\n",
            "[479,    11] loss: 0.032\n",
            "[479,    12] loss: 0.029\n",
            "[479,    13] loss: 0.026\n",
            "[479,    14] loss: 0.028\n",
            "[479,    15] loss: 0.030\n",
            "[479,    16] loss: 0.020\n",
            "[479,    17] loss: 0.024\n",
            "[479,    18] loss: 0.028\n",
            "[479,    19] loss: 0.027\n",
            "[479,    20] loss: 0.024\n",
            "[479,    21] loss: 0.026\n",
            "[479,    22] loss: 0.026\n",
            "[479,    23] loss: 0.023\n",
            "[479,    24] loss: 0.027\n",
            "[479,    25] loss: 0.023\n",
            "[479,    26] loss: 0.027\n",
            "[479,    27] loss: 0.024\n",
            "[479,    28] loss: 0.025\n",
            "[479,    29] loss: 0.025\n",
            "[479,    30] loss: 0.019\n",
            "[479,    31] loss: 0.027\n",
            "[479,    32] loss: 0.021\n",
            "[479,    33] loss: 0.026\n",
            "[479,    34] loss: 0.022\n",
            "[479,    35] loss: 0.021\n",
            "[479,    36] loss: 0.022\n",
            "[479,    37] loss: 0.021\n",
            "[479,    38] loss: 0.030\n",
            "[479,    39] loss: 0.028\n",
            "[479,    40] loss: 0.023\n",
            "[479,    41] loss: 0.023\n",
            "[479,    42] loss: 0.026\n",
            "[479,    43] loss: 0.026\n",
            "[479,    44] loss: 0.031\n",
            "[479,    45] loss: 0.023\n",
            "[479,    46] loss: 0.025\n",
            "[479,    47] loss: 0.027\n",
            "[479,    48] loss: 0.026\n",
            "[479,    49] loss: 0.029\n",
            "[479,    50] loss: 0.027\n",
            "[479,    51] loss: 0.022\n",
            "[479,    52] loss: 0.029\n",
            "[479,    53] loss: 0.028\n",
            "[479,    54] loss: 0.029\n",
            "[479,    55] loss: 0.027\n",
            "[479,    56] loss: 0.027\n",
            "[479,    57] loss: 0.028\n",
            "[479,    58] loss: 0.030\n",
            "[479,    59] loss: 0.027\n",
            "[479,    60] loss: 0.027\n",
            "[479,    61] loss: 0.029\n",
            "[479,    62] loss: 0.029\n",
            "[479,    63] loss: 0.027\n",
            "[479,    64] loss: 0.027\n",
            "[479,    65] loss: 0.023\n",
            "[479,    66] loss: 0.030\n",
            "[479,    67] loss: 0.027\n",
            "[479,    68] loss: 0.028\n",
            "[479,    69] loss: 0.023\n",
            "[479,    70] loss: 0.025\n",
            "[479,    71] loss: 0.020\n",
            "[479,    72] loss: 0.020\n",
            "[479,    73] loss: 0.026\n",
            "[479,    74] loss: 0.027\n",
            "[479,    75] loss: 0.024\n",
            "[479,    76] loss: 0.028\n",
            "[479,    77] loss: 0.026\n",
            "[479,    78] loss: 0.024\n",
            "[479,    79] loss: 0.023\n",
            "[479,    80] loss: 0.023\n",
            "[479,    81] loss: 0.020\n",
            "[479,    82] loss: 0.023\n",
            "[479,    83] loss: 0.023\n",
            "[479,    84] loss: 0.027\n",
            "[479,    85] loss: 0.025\n",
            "[479,    86] loss: 0.030\n",
            "[479,    87] loss: 0.017\n",
            "[479,    88] loss: 0.025\n",
            "[479,    89] loss: 0.026\n",
            "[479,    90] loss: 0.019\n",
            "[479,    91] loss: 0.027\n",
            "[479,    92] loss: 0.025\n",
            "[479,    93] loss: 0.026\n",
            "[479,    94] loss: 0.026\n",
            "[479,    95] loss: 0.025\n",
            "[479,    96] loss: 0.025\n",
            "[479,    97] loss: 0.022\n",
            "[479,    98] loss: 0.022\n",
            "[479,    99] loss: 0.023\n",
            "[479,   100] loss: 0.030\n",
            "[479,   101] loss: 0.023\n",
            "[479,   102] loss: 0.024\n",
            "[479,   103] loss: 0.021\n",
            "[479,   104] loss: 0.026\n",
            "[479,   105] loss: 0.028\n",
            "[479,   106] loss: 0.024\n",
            "[479,   107] loss: 0.027\n",
            "[479,   108] loss: 0.028\n",
            "[479,   109] loss: 0.030\n",
            "[479,   110] loss: 0.021\n",
            "[479,   111] loss: 0.026\n",
            "[479,   112] loss: 0.028\n",
            "[479,   113] loss: 0.024\n",
            "[479,   114] loss: 0.023\n",
            "[479,   115] loss: 0.031\n",
            "[479,   116] loss: 0.022\n",
            "[479,   117] loss: 0.025\n",
            "[479,   118] loss: 0.024\n",
            "[479,   119] loss: 0.027\n",
            "[479,   120] loss: 0.027\n",
            "[479,   121] loss: 0.019\n",
            "[479,   122] loss: 0.027\n",
            "[479,   123] loss: 0.029\n",
            "[479,   124] loss: 0.023\n",
            "[479,   125] loss: 0.025\n",
            "[479,   126] loss: 0.020\n",
            "[479,   127] loss: 0.027\n",
            "[479,   128] loss: 0.026\n",
            "[479,   129] loss: 0.029\n",
            "[479,   130] loss: 0.025\n",
            "[479,   131] loss: 0.020\n",
            "[479,   132] loss: 0.024\n",
            "[479,   133] loss: 0.029\n",
            "[479,   134] loss: 0.022\n",
            "[479,   135] loss: 0.021\n",
            "[479,   136] loss: 0.016\n",
            "[479,   137] loss: 0.032\n",
            "[479,   138] loss: 0.027\n",
            "[479,   139] loss: 0.026\n",
            "[479,   140] loss: 0.024\n",
            "[479,   141] loss: 0.025\n",
            "[479,   142] loss: 0.023\n",
            "[479,   143] loss: 0.027\n",
            "[479,   144] loss: 0.027\n",
            "[479,   145] loss: 0.023\n",
            "[479,   146] loss: 0.021\n",
            "[479,   147] loss: 0.018\n",
            "[480,     1] loss: 0.027\n",
            "[480,     2] loss: 0.032\n",
            "[480,     3] loss: 0.025\n",
            "[480,     4] loss: 0.024\n",
            "[480,     5] loss: 0.030\n",
            "[480,     6] loss: 0.023\n",
            "[480,     7] loss: 0.026\n",
            "[480,     8] loss: 0.023\n",
            "[480,     9] loss: 0.027\n",
            "[480,    10] loss: 0.026\n",
            "[480,    11] loss: 0.022\n",
            "[480,    12] loss: 0.026\n",
            "[480,    13] loss: 0.030\n",
            "[480,    14] loss: 0.025\n",
            "[480,    15] loss: 0.027\n",
            "[480,    16] loss: 0.023\n",
            "[480,    17] loss: 0.021\n",
            "[480,    18] loss: 0.026\n",
            "[480,    19] loss: 0.021\n",
            "[480,    20] loss: 0.024\n",
            "[480,    21] loss: 0.025\n",
            "[480,    22] loss: 0.023\n",
            "[480,    23] loss: 0.028\n",
            "[480,    24] loss: 0.029\n",
            "[480,    25] loss: 0.027\n",
            "[480,    26] loss: 0.027\n",
            "[480,    27] loss: 0.030\n",
            "[480,    28] loss: 0.025\n",
            "[480,    29] loss: 0.027\n",
            "[480,    30] loss: 0.017\n",
            "[480,    31] loss: 0.022\n",
            "[480,    32] loss: 0.023\n",
            "[480,    33] loss: 0.025\n",
            "[480,    34] loss: 0.016\n",
            "[480,    35] loss: 0.024\n",
            "[480,    36] loss: 0.024\n",
            "[480,    37] loss: 0.028\n",
            "[480,    38] loss: 0.032\n",
            "[480,    39] loss: 0.025\n",
            "[480,    40] loss: 0.026\n",
            "[480,    41] loss: 0.027\n",
            "[480,    42] loss: 0.023\n",
            "[480,    43] loss: 0.023\n",
            "[480,    44] loss: 0.026\n",
            "[480,    45] loss: 0.023\n",
            "[480,    46] loss: 0.032\n",
            "[480,    47] loss: 0.025\n",
            "[480,    48] loss: 0.025\n",
            "[480,    49] loss: 0.026\n",
            "[480,    50] loss: 0.019\n",
            "[480,    51] loss: 0.024\n",
            "[480,    52] loss: 0.021\n",
            "[480,    53] loss: 0.031\n",
            "[480,    54] loss: 0.023\n",
            "[480,    55] loss: 0.020\n",
            "[480,    56] loss: 0.026\n",
            "[480,    57] loss: 0.026\n",
            "[480,    58] loss: 0.027\n",
            "[480,    59] loss: 0.025\n",
            "[480,    60] loss: 0.029\n",
            "[480,    61] loss: 0.021\n",
            "[480,    62] loss: 0.023\n",
            "[480,    63] loss: 0.023\n",
            "[480,    64] loss: 0.026\n",
            "[480,    65] loss: 0.027\n",
            "[480,    66] loss: 0.019\n",
            "[480,    67] loss: 0.027\n",
            "[480,    68] loss: 0.025\n",
            "[480,    69] loss: 0.029\n",
            "[480,    70] loss: 0.028\n",
            "[480,    71] loss: 0.027\n",
            "[480,    72] loss: 0.027\n",
            "[480,    73] loss: 0.022\n",
            "[480,    74] loss: 0.026\n",
            "[480,    75] loss: 0.027\n",
            "[480,    76] loss: 0.033\n",
            "[480,    77] loss: 0.032\n",
            "[480,    78] loss: 0.023\n",
            "[480,    79] loss: 0.025\n",
            "[480,    80] loss: 0.030\n",
            "[480,    81] loss: 0.022\n",
            "[480,    82] loss: 0.027\n",
            "[480,    83] loss: 0.030\n",
            "[480,    84] loss: 0.025\n",
            "[480,    85] loss: 0.027\n",
            "[480,    86] loss: 0.026\n",
            "[480,    87] loss: 0.026\n",
            "[480,    88] loss: 0.023\n",
            "[480,    89] loss: 0.027\n",
            "[480,    90] loss: 0.023\n",
            "[480,    91] loss: 0.024\n",
            "[480,    92] loss: 0.027\n",
            "[480,    93] loss: 0.022\n",
            "[480,    94] loss: 0.024\n",
            "[480,    95] loss: 0.020\n",
            "[480,    96] loss: 0.023\n",
            "[480,    97] loss: 0.026\n",
            "[480,    98] loss: 0.025\n",
            "[480,    99] loss: 0.030\n",
            "[480,   100] loss: 0.025\n",
            "[480,   101] loss: 0.026\n",
            "[480,   102] loss: 0.023\n",
            "[480,   103] loss: 0.023\n",
            "[480,   104] loss: 0.026\n",
            "[480,   105] loss: 0.025\n",
            "[480,   106] loss: 0.027\n",
            "[480,   107] loss: 0.025\n",
            "[480,   108] loss: 0.027\n",
            "[480,   109] loss: 0.020\n",
            "[480,   110] loss: 0.024\n",
            "[480,   111] loss: 0.029\n",
            "[480,   112] loss: 0.028\n",
            "[480,   113] loss: 0.030\n",
            "[480,   114] loss: 0.026\n",
            "[480,   115] loss: 0.020\n",
            "[480,   116] loss: 0.025\n",
            "[480,   117] loss: 0.026\n",
            "[480,   118] loss: 0.024\n",
            "[480,   119] loss: 0.020\n",
            "[480,   120] loss: 0.020\n",
            "[480,   121] loss: 0.024\n",
            "[480,   122] loss: 0.026\n",
            "[480,   123] loss: 0.025\n",
            "[480,   124] loss: 0.023\n",
            "[480,   125] loss: 0.022\n",
            "[480,   126] loss: 0.027\n",
            "[480,   127] loss: 0.030\n",
            "[480,   128] loss: 0.027\n",
            "[480,   129] loss: 0.029\n",
            "[480,   130] loss: 0.020\n",
            "[480,   131] loss: 0.024\n",
            "[480,   132] loss: 0.027\n",
            "[480,   133] loss: 0.026\n",
            "[480,   134] loss: 0.019\n",
            "[480,   135] loss: 0.026\n",
            "[480,   136] loss: 0.020\n",
            "[480,   137] loss: 0.024\n",
            "[480,   138] loss: 0.027\n",
            "[480,   139] loss: 0.027\n",
            "[480,   140] loss: 0.031\n",
            "[480,   141] loss: 0.026\n",
            "[480,   142] loss: 0.027\n",
            "[480,   143] loss: 0.023\n",
            "[480,   144] loss: 0.023\n",
            "[480,   145] loss: 0.024\n",
            "[480,   146] loss: 0.024\n",
            "[480,   147] loss: 0.032\n",
            "[481,     1] loss: 0.025\n",
            "[481,     2] loss: 0.023\n",
            "[481,     3] loss: 0.023\n",
            "[481,     4] loss: 0.024\n",
            "[481,     5] loss: 0.027\n",
            "[481,     6] loss: 0.025\n",
            "[481,     7] loss: 0.029\n",
            "[481,     8] loss: 0.026\n",
            "[481,     9] loss: 0.020\n",
            "[481,    10] loss: 0.025\n",
            "[481,    11] loss: 0.022\n",
            "[481,    12] loss: 0.025\n",
            "[481,    13] loss: 0.021\n",
            "[481,    14] loss: 0.027\n",
            "[481,    15] loss: 0.028\n",
            "[481,    16] loss: 0.028\n",
            "[481,    17] loss: 0.021\n",
            "[481,    18] loss: 0.027\n",
            "[481,    19] loss: 0.028\n",
            "[481,    20] loss: 0.027\n",
            "[481,    21] loss: 0.022\n",
            "[481,    22] loss: 0.022\n",
            "[481,    23] loss: 0.023\n",
            "[481,    24] loss: 0.025\n",
            "[481,    25] loss: 0.020\n",
            "[481,    26] loss: 0.027\n",
            "[481,    27] loss: 0.025\n",
            "[481,    28] loss: 0.026\n",
            "[481,    29] loss: 0.025\n",
            "[481,    30] loss: 0.026\n",
            "[481,    31] loss: 0.028\n",
            "[481,    32] loss: 0.030\n",
            "[481,    33] loss: 0.030\n",
            "[481,    34] loss: 0.025\n",
            "[481,    35] loss: 0.027\n",
            "[481,    36] loss: 0.027\n",
            "[481,    37] loss: 0.025\n",
            "[481,    38] loss: 0.023\n",
            "[481,    39] loss: 0.027\n",
            "[481,    40] loss: 0.022\n",
            "[481,    41] loss: 0.020\n",
            "[481,    42] loss: 0.018\n",
            "[481,    43] loss: 0.029\n",
            "[481,    44] loss: 0.025\n",
            "[481,    45] loss: 0.026\n",
            "[481,    46] loss: 0.027\n",
            "[481,    47] loss: 0.029\n",
            "[481,    48] loss: 0.024\n",
            "[481,    49] loss: 0.024\n",
            "[481,    50] loss: 0.030\n",
            "[481,    51] loss: 0.024\n",
            "[481,    52] loss: 0.020\n",
            "[481,    53] loss: 0.022\n",
            "[481,    54] loss: 0.028\n",
            "[481,    55] loss: 0.026\n",
            "[481,    56] loss: 0.028\n",
            "[481,    57] loss: 0.025\n",
            "[481,    58] loss: 0.025\n",
            "[481,    59] loss: 0.022\n",
            "[481,    60] loss: 0.022\n",
            "[481,    61] loss: 0.025\n",
            "[481,    62] loss: 0.029\n",
            "[481,    63] loss: 0.025\n",
            "[481,    64] loss: 0.023\n",
            "[481,    65] loss: 0.027\n",
            "[481,    66] loss: 0.025\n",
            "[481,    67] loss: 0.020\n",
            "[481,    68] loss: 0.024\n",
            "[481,    69] loss: 0.027\n",
            "[481,    70] loss: 0.027\n",
            "[481,    71] loss: 0.027\n",
            "[481,    72] loss: 0.025\n",
            "[481,    73] loss: 0.024\n",
            "[481,    74] loss: 0.027\n",
            "[481,    75] loss: 0.021\n",
            "[481,    76] loss: 0.025\n",
            "[481,    77] loss: 0.028\n",
            "[481,    78] loss: 0.021\n",
            "[481,    79] loss: 0.021\n",
            "[481,    80] loss: 0.023\n",
            "[481,    81] loss: 0.027\n",
            "[481,    82] loss: 0.023\n",
            "[481,    83] loss: 0.025\n",
            "[481,    84] loss: 0.023\n",
            "[481,    85] loss: 0.031\n",
            "[481,    86] loss: 0.031\n",
            "[481,    87] loss: 0.027\n",
            "[481,    88] loss: 0.025\n",
            "[481,    89] loss: 0.024\n",
            "[481,    90] loss: 0.024\n",
            "[481,    91] loss: 0.022\n",
            "[481,    92] loss: 0.028\n",
            "[481,    93] loss: 0.022\n",
            "[481,    94] loss: 0.029\n",
            "[481,    95] loss: 0.027\n",
            "[481,    96] loss: 0.022\n",
            "[481,    97] loss: 0.024\n",
            "[481,    98] loss: 0.029\n",
            "[481,    99] loss: 0.026\n",
            "[481,   100] loss: 0.023\n",
            "[481,   101] loss: 0.023\n",
            "[481,   102] loss: 0.027\n",
            "[481,   103] loss: 0.027\n",
            "[481,   104] loss: 0.022\n",
            "[481,   105] loss: 0.026\n",
            "[481,   106] loss: 0.027\n",
            "[481,   107] loss: 0.024\n",
            "[481,   108] loss: 0.027\n",
            "[481,   109] loss: 0.023\n",
            "[481,   110] loss: 0.022\n",
            "[481,   111] loss: 0.030\n",
            "[481,   112] loss: 0.019\n",
            "[481,   113] loss: 0.025\n",
            "[481,   114] loss: 0.021\n",
            "[481,   115] loss: 0.025\n",
            "[481,   116] loss: 0.028\n",
            "[481,   117] loss: 0.023\n",
            "[481,   118] loss: 0.020\n",
            "[481,   119] loss: 0.024\n",
            "[481,   120] loss: 0.023\n",
            "[481,   121] loss: 0.023\n",
            "[481,   122] loss: 0.021\n",
            "[481,   123] loss: 0.027\n",
            "[481,   124] loss: 0.027\n",
            "[481,   125] loss: 0.023\n",
            "[481,   126] loss: 0.024\n",
            "[481,   127] loss: 0.024\n",
            "[481,   128] loss: 0.027\n",
            "[481,   129] loss: 0.028\n",
            "[481,   130] loss: 0.029\n",
            "[481,   131] loss: 0.030\n",
            "[481,   132] loss: 0.024\n",
            "[481,   133] loss: 0.025\n",
            "[481,   134] loss: 0.026\n",
            "[481,   135] loss: 0.028\n",
            "[481,   136] loss: 0.025\n",
            "[481,   137] loss: 0.024\n",
            "[481,   138] loss: 0.028\n",
            "[481,   139] loss: 0.031\n",
            "[481,   140] loss: 0.027\n",
            "[481,   141] loss: 0.027\n",
            "[481,   142] loss: 0.028\n",
            "[481,   143] loss: 0.025\n",
            "[481,   144] loss: 0.027\n",
            "[481,   145] loss: 0.030\n",
            "[481,   146] loss: 0.031\n",
            "[481,   147] loss: 0.026\n",
            "[482,     1] loss: 0.025\n",
            "[482,     2] loss: 0.023\n",
            "[482,     3] loss: 0.020\n",
            "[482,     4] loss: 0.027\n",
            "[482,     5] loss: 0.027\n",
            "[482,     6] loss: 0.025\n",
            "[482,     7] loss: 0.031\n",
            "[482,     8] loss: 0.027\n",
            "[482,     9] loss: 0.026\n",
            "[482,    10] loss: 0.023\n",
            "[482,    11] loss: 0.028\n",
            "[482,    12] loss: 0.022\n",
            "[482,    13] loss: 0.023\n",
            "[482,    14] loss: 0.021\n",
            "[482,    15] loss: 0.023\n",
            "[482,    16] loss: 0.022\n",
            "[482,    17] loss: 0.030\n",
            "[482,    18] loss: 0.025\n",
            "[482,    19] loss: 0.028\n",
            "[482,    20] loss: 0.022\n",
            "[482,    21] loss: 0.027\n",
            "[482,    22] loss: 0.023\n",
            "[482,    23] loss: 0.023\n",
            "[482,    24] loss: 0.025\n",
            "[482,    25] loss: 0.025\n",
            "[482,    26] loss: 0.023\n",
            "[482,    27] loss: 0.023\n",
            "[482,    28] loss: 0.020\n",
            "[482,    29] loss: 0.032\n",
            "[482,    30] loss: 0.021\n",
            "[482,    31] loss: 0.030\n",
            "[482,    32] loss: 0.023\n",
            "[482,    33] loss: 0.023\n",
            "[482,    34] loss: 0.027\n",
            "[482,    35] loss: 0.031\n",
            "[482,    36] loss: 0.023\n",
            "[482,    37] loss: 0.024\n",
            "[482,    38] loss: 0.027\n",
            "[482,    39] loss: 0.027\n",
            "[482,    40] loss: 0.028\n",
            "[482,    41] loss: 0.027\n",
            "[482,    42] loss: 0.023\n",
            "[482,    43] loss: 0.023\n",
            "[482,    44] loss: 0.026\n",
            "[482,    45] loss: 0.023\n",
            "[482,    46] loss: 0.025\n",
            "[482,    47] loss: 0.030\n",
            "[482,    48] loss: 0.026\n",
            "[482,    49] loss: 0.022\n",
            "[482,    50] loss: 0.027\n",
            "[482,    51] loss: 0.027\n",
            "[482,    52] loss: 0.023\n",
            "[482,    53] loss: 0.027\n",
            "[482,    54] loss: 0.024\n",
            "[482,    55] loss: 0.021\n",
            "[482,    56] loss: 0.023\n",
            "[482,    57] loss: 0.023\n",
            "[482,    58] loss: 0.022\n",
            "[482,    59] loss: 0.026\n",
            "[482,    60] loss: 0.034\n",
            "[482,    61] loss: 0.028\n",
            "[482,    62] loss: 0.027\n",
            "[482,    63] loss: 0.023\n",
            "[482,    64] loss: 0.026\n",
            "[482,    65] loss: 0.020\n",
            "[482,    66] loss: 0.020\n",
            "[482,    67] loss: 0.024\n",
            "[482,    68] loss: 0.027\n",
            "[482,    69] loss: 0.028\n",
            "[482,    70] loss: 0.023\n",
            "[482,    71] loss: 0.021\n",
            "[482,    72] loss: 0.022\n",
            "[482,    73] loss: 0.023\n",
            "[482,    74] loss: 0.023\n",
            "[482,    75] loss: 0.023\n",
            "[482,    76] loss: 0.025\n",
            "[482,    77] loss: 0.028\n",
            "[482,    78] loss: 0.022\n",
            "[482,    79] loss: 0.029\n",
            "[482,    80] loss: 0.026\n",
            "[482,    81] loss: 0.022\n",
            "[482,    82] loss: 0.020\n",
            "[482,    83] loss: 0.025\n",
            "[482,    84] loss: 0.024\n",
            "[482,    85] loss: 0.027\n",
            "[482,    86] loss: 0.024\n",
            "[482,    87] loss: 0.026\n",
            "[482,    88] loss: 0.023\n",
            "[482,    89] loss: 0.035\n",
            "[482,    90] loss: 0.026\n",
            "[482,    91] loss: 0.032\n",
            "[482,    92] loss: 0.021\n",
            "[482,    93] loss: 0.024\n",
            "[482,    94] loss: 0.033\n",
            "[482,    95] loss: 0.028\n",
            "[482,    96] loss: 0.022\n",
            "[482,    97] loss: 0.030\n",
            "[482,    98] loss: 0.026\n",
            "[482,    99] loss: 0.024\n",
            "[482,   100] loss: 0.027\n",
            "[482,   101] loss: 0.028\n",
            "[482,   102] loss: 0.027\n",
            "[482,   103] loss: 0.032\n",
            "[482,   104] loss: 0.020\n",
            "[482,   105] loss: 0.028\n",
            "[482,   106] loss: 0.020\n",
            "[482,   107] loss: 0.025\n",
            "[482,   108] loss: 0.024\n",
            "[482,   109] loss: 0.026\n",
            "[482,   110] loss: 0.030\n",
            "[482,   111] loss: 0.027\n",
            "[482,   112] loss: 0.023\n",
            "[482,   113] loss: 0.027\n",
            "[482,   114] loss: 0.029\n",
            "[482,   115] loss: 0.022\n",
            "[482,   116] loss: 0.029\n",
            "[482,   117] loss: 0.029\n",
            "[482,   118] loss: 0.024\n",
            "[482,   119] loss: 0.016\n",
            "[482,   120] loss: 0.023\n",
            "[482,   121] loss: 0.025\n",
            "[482,   122] loss: 0.023\n",
            "[482,   123] loss: 0.026\n",
            "[482,   124] loss: 0.020\n",
            "[482,   125] loss: 0.021\n",
            "[482,   126] loss: 0.022\n",
            "[482,   127] loss: 0.024\n",
            "[482,   128] loss: 0.029\n",
            "[482,   129] loss: 0.023\n",
            "[482,   130] loss: 0.027\n",
            "[482,   131] loss: 0.026\n",
            "[482,   132] loss: 0.026\n",
            "[482,   133] loss: 0.028\n",
            "[482,   134] loss: 0.031\n",
            "[482,   135] loss: 0.025\n",
            "[482,   136] loss: 0.026\n",
            "[482,   137] loss: 0.023\n",
            "[482,   138] loss: 0.023\n",
            "[482,   139] loss: 0.026\n",
            "[482,   140] loss: 0.027\n",
            "[482,   141] loss: 0.023\n",
            "[482,   142] loss: 0.027\n",
            "[482,   143] loss: 0.029\n",
            "[482,   144] loss: 0.025\n",
            "[482,   145] loss: 0.022\n",
            "[482,   146] loss: 0.027\n",
            "[482,   147] loss: 0.029\n",
            "[483,     1] loss: 0.025\n",
            "[483,     2] loss: 0.026\n",
            "[483,     3] loss: 0.027\n",
            "[483,     4] loss: 0.028\n",
            "[483,     5] loss: 0.025\n",
            "[483,     6] loss: 0.020\n",
            "[483,     7] loss: 0.026\n",
            "[483,     8] loss: 0.025\n",
            "[483,     9] loss: 0.030\n",
            "[483,    10] loss: 0.030\n",
            "[483,    11] loss: 0.021\n",
            "[483,    12] loss: 0.027\n",
            "[483,    13] loss: 0.027\n",
            "[483,    14] loss: 0.023\n",
            "[483,    15] loss: 0.027\n",
            "[483,    16] loss: 0.025\n",
            "[483,    17] loss: 0.022\n",
            "[483,    18] loss: 0.023\n",
            "[483,    19] loss: 0.025\n",
            "[483,    20] loss: 0.023\n",
            "[483,    21] loss: 0.027\n",
            "[483,    22] loss: 0.024\n",
            "[483,    23] loss: 0.028\n",
            "[483,    24] loss: 0.023\n",
            "[483,    25] loss: 0.025\n",
            "[483,    26] loss: 0.025\n",
            "[483,    27] loss: 0.029\n",
            "[483,    28] loss: 0.022\n",
            "[483,    29] loss: 0.023\n",
            "[483,    30] loss: 0.023\n",
            "[483,    31] loss: 0.021\n",
            "[483,    32] loss: 0.024\n",
            "[483,    33] loss: 0.020\n",
            "[483,    34] loss: 0.029\n",
            "[483,    35] loss: 0.027\n",
            "[483,    36] loss: 0.030\n",
            "[483,    37] loss: 0.019\n",
            "[483,    38] loss: 0.027\n",
            "[483,    39] loss: 0.029\n",
            "[483,    40] loss: 0.021\n",
            "[483,    41] loss: 0.025\n",
            "[483,    42] loss: 0.027\n",
            "[483,    43] loss: 0.027\n",
            "[483,    44] loss: 0.024\n",
            "[483,    45] loss: 0.023\n",
            "[483,    46] loss: 0.028\n",
            "[483,    47] loss: 0.030\n",
            "[483,    48] loss: 0.023\n",
            "[483,    49] loss: 0.020\n",
            "[483,    50] loss: 0.023\n",
            "[483,    51] loss: 0.023\n",
            "[483,    52] loss: 0.027\n",
            "[483,    53] loss: 0.025\n",
            "[483,    54] loss: 0.022\n",
            "[483,    55] loss: 0.022\n",
            "[483,    56] loss: 0.020\n",
            "[483,    57] loss: 0.023\n",
            "[483,    58] loss: 0.027\n",
            "[483,    59] loss: 0.027\n",
            "[483,    60] loss: 0.027\n",
            "[483,    61] loss: 0.030\n",
            "[483,    62] loss: 0.020\n",
            "[483,    63] loss: 0.030\n",
            "[483,    64] loss: 0.025\n",
            "[483,    65] loss: 0.027\n",
            "[483,    66] loss: 0.029\n",
            "[483,    67] loss: 0.026\n",
            "[483,    68] loss: 0.026\n",
            "[483,    69] loss: 0.024\n",
            "[483,    70] loss: 0.022\n",
            "[483,    71] loss: 0.027\n",
            "[483,    72] loss: 0.030\n",
            "[483,    73] loss: 0.031\n",
            "[483,    74] loss: 0.030\n",
            "[483,    75] loss: 0.026\n",
            "[483,    76] loss: 0.027\n",
            "[483,    77] loss: 0.030\n",
            "[483,    78] loss: 0.022\n",
            "[483,    79] loss: 0.019\n",
            "[483,    80] loss: 0.025\n",
            "[483,    81] loss: 0.027\n",
            "[483,    82] loss: 0.024\n",
            "[483,    83] loss: 0.023\n",
            "[483,    84] loss: 0.021\n",
            "[483,    85] loss: 0.025\n",
            "[483,    86] loss: 0.026\n",
            "[483,    87] loss: 0.030\n",
            "[483,    88] loss: 0.025\n",
            "[483,    89] loss: 0.029\n",
            "[483,    90] loss: 0.022\n",
            "[483,    91] loss: 0.026\n",
            "[483,    92] loss: 0.025\n",
            "[483,    93] loss: 0.023\n",
            "[483,    94] loss: 0.024\n",
            "[483,    95] loss: 0.025\n",
            "[483,    96] loss: 0.029\n",
            "[483,    97] loss: 0.020\n",
            "[483,    98] loss: 0.026\n",
            "[483,    99] loss: 0.026\n",
            "[483,   100] loss: 0.021\n",
            "[483,   101] loss: 0.024\n",
            "[483,   102] loss: 0.027\n",
            "[483,   103] loss: 0.020\n",
            "[483,   104] loss: 0.027\n",
            "[483,   105] loss: 0.020\n",
            "[483,   106] loss: 0.027\n",
            "[483,   107] loss: 0.026\n",
            "[483,   108] loss: 0.021\n",
            "[483,   109] loss: 0.027\n",
            "[483,   110] loss: 0.024\n",
            "[483,   111] loss: 0.023\n",
            "[483,   112] loss: 0.022\n",
            "[483,   113] loss: 0.022\n",
            "[483,   114] loss: 0.028\n",
            "[483,   115] loss: 0.025\n",
            "[483,   116] loss: 0.023\n",
            "[483,   117] loss: 0.022\n",
            "[483,   118] loss: 0.025\n",
            "[483,   119] loss: 0.023\n",
            "[483,   120] loss: 0.026\n",
            "[483,   121] loss: 0.027\n",
            "[483,   122] loss: 0.024\n",
            "[483,   123] loss: 0.023\n",
            "[483,   124] loss: 0.024\n",
            "[483,   125] loss: 0.023\n",
            "[483,   126] loss: 0.025\n",
            "[483,   127] loss: 0.025\n",
            "[483,   128] loss: 0.027\n",
            "[483,   129] loss: 0.020\n",
            "[483,   130] loss: 0.026\n",
            "[483,   131] loss: 0.027\n",
            "[483,   132] loss: 0.028\n",
            "[483,   133] loss: 0.028\n",
            "[483,   134] loss: 0.025\n",
            "[483,   135] loss: 0.028\n",
            "[483,   136] loss: 0.023\n",
            "[483,   137] loss: 0.028\n",
            "[483,   138] loss: 0.029\n",
            "[483,   139] loss: 0.030\n",
            "[483,   140] loss: 0.027\n",
            "[483,   141] loss: 0.028\n",
            "[483,   142] loss: 0.030\n",
            "[483,   143] loss: 0.025\n",
            "[483,   144] loss: 0.025\n",
            "[483,   145] loss: 0.023\n",
            "[483,   146] loss: 0.029\n",
            "[483,   147] loss: 0.024\n",
            "[484,     1] loss: 0.027\n",
            "[484,     2] loss: 0.023\n",
            "[484,     3] loss: 0.024\n",
            "[484,     4] loss: 0.029\n",
            "[484,     5] loss: 0.029\n",
            "[484,     6] loss: 0.030\n",
            "[484,     7] loss: 0.023\n",
            "[484,     8] loss: 0.027\n",
            "[484,     9] loss: 0.030\n",
            "[484,    10] loss: 0.023\n",
            "[484,    11] loss: 0.027\n",
            "[484,    12] loss: 0.025\n",
            "[484,    13] loss: 0.030\n",
            "[484,    14] loss: 0.021\n",
            "[484,    15] loss: 0.027\n",
            "[484,    16] loss: 0.023\n",
            "[484,    17] loss: 0.027\n",
            "[484,    18] loss: 0.021\n",
            "[484,    19] loss: 0.027\n",
            "[484,    20] loss: 0.023\n",
            "[484,    21] loss: 0.029\n",
            "[484,    22] loss: 0.023\n",
            "[484,    23] loss: 0.022\n",
            "[484,    24] loss: 0.023\n",
            "[484,    25] loss: 0.023\n",
            "[484,    26] loss: 0.025\n",
            "[484,    27] loss: 0.024\n",
            "[484,    28] loss: 0.032\n",
            "[484,    29] loss: 0.024\n",
            "[484,    30] loss: 0.020\n",
            "[484,    31] loss: 0.034\n",
            "[484,    32] loss: 0.030\n",
            "[484,    33] loss: 0.023\n",
            "[484,    34] loss: 0.028\n",
            "[484,    35] loss: 0.027\n",
            "[484,    36] loss: 0.027\n",
            "[484,    37] loss: 0.025\n",
            "[484,    38] loss: 0.030\n",
            "[484,    39] loss: 0.019\n",
            "[484,    40] loss: 0.023\n",
            "[484,    41] loss: 0.023\n",
            "[484,    42] loss: 0.026\n",
            "[484,    43] loss: 0.019\n",
            "[484,    44] loss: 0.020\n",
            "[484,    45] loss: 0.028\n",
            "[484,    46] loss: 0.028\n",
            "[484,    47] loss: 0.024\n",
            "[484,    48] loss: 0.020\n",
            "[484,    49] loss: 0.027\n",
            "[484,    50] loss: 0.027\n",
            "[484,    51] loss: 0.023\n",
            "[484,    52] loss: 0.026\n",
            "[484,    53] loss: 0.026\n",
            "[484,    54] loss: 0.027\n",
            "[484,    55] loss: 0.026\n",
            "[484,    56] loss: 0.023\n",
            "[484,    57] loss: 0.026\n",
            "[484,    58] loss: 0.022\n",
            "[484,    59] loss: 0.027\n",
            "[484,    60] loss: 0.025\n",
            "[484,    61] loss: 0.020\n",
            "[484,    62] loss: 0.030\n",
            "[484,    63] loss: 0.030\n",
            "[484,    64] loss: 0.024\n",
            "[484,    65] loss: 0.030\n",
            "[484,    66] loss: 0.024\n",
            "[484,    67] loss: 0.026\n",
            "[484,    68] loss: 0.023\n",
            "[484,    69] loss: 0.023\n",
            "[484,    70] loss: 0.027\n",
            "[484,    71] loss: 0.028\n",
            "[484,    72] loss: 0.027\n",
            "[484,    73] loss: 0.024\n",
            "[484,    74] loss: 0.022\n",
            "[484,    75] loss: 0.028\n",
            "[484,    76] loss: 0.028\n",
            "[484,    77] loss: 0.027\n",
            "[484,    78] loss: 0.020\n",
            "[484,    79] loss: 0.028\n",
            "[484,    80] loss: 0.027\n",
            "[484,    81] loss: 0.027\n",
            "[484,    82] loss: 0.026\n",
            "[484,    83] loss: 0.030\n",
            "[484,    84] loss: 0.027\n",
            "[484,    85] loss: 0.025\n",
            "[484,    86] loss: 0.022\n",
            "[484,    87] loss: 0.021\n",
            "[484,    88] loss: 0.020\n",
            "[484,    89] loss: 0.030\n",
            "[484,    90] loss: 0.023\n",
            "[484,    91] loss: 0.024\n",
            "[484,    92] loss: 0.025\n",
            "[484,    93] loss: 0.027\n",
            "[484,    94] loss: 0.025\n",
            "[484,    95] loss: 0.025\n",
            "[484,    96] loss: 0.026\n",
            "[484,    97] loss: 0.025\n",
            "[484,    98] loss: 0.027\n",
            "[484,    99] loss: 0.027\n",
            "[484,   100] loss: 0.027\n",
            "[484,   101] loss: 0.023\n",
            "[484,   102] loss: 0.027\n",
            "[484,   103] loss: 0.018\n",
            "[484,   104] loss: 0.030\n",
            "[484,   105] loss: 0.020\n",
            "[484,   106] loss: 0.025\n",
            "[484,   107] loss: 0.023\n",
            "[484,   108] loss: 0.027\n",
            "[484,   109] loss: 0.028\n",
            "[484,   110] loss: 0.030\n",
            "[484,   111] loss: 0.027\n",
            "[484,   112] loss: 0.030\n",
            "[484,   113] loss: 0.020\n",
            "[484,   114] loss: 0.021\n",
            "[484,   115] loss: 0.023\n",
            "[484,   116] loss: 0.023\n",
            "[484,   117] loss: 0.018\n",
            "[484,   118] loss: 0.027\n",
            "[484,   119] loss: 0.029\n",
            "[484,   120] loss: 0.021\n",
            "[484,   121] loss: 0.024\n",
            "[484,   122] loss: 0.027\n",
            "[484,   123] loss: 0.020\n",
            "[484,   124] loss: 0.025\n",
            "[484,   125] loss: 0.025\n",
            "[484,   126] loss: 0.020\n",
            "[484,   127] loss: 0.019\n",
            "[484,   128] loss: 0.024\n",
            "[484,   129] loss: 0.030\n",
            "[484,   130] loss: 0.028\n",
            "[484,   131] loss: 0.031\n",
            "[484,   132] loss: 0.026\n",
            "[484,   133] loss: 0.027\n",
            "[484,   134] loss: 0.032\n",
            "[484,   135] loss: 0.025\n",
            "[484,   136] loss: 0.020\n",
            "[484,   137] loss: 0.026\n",
            "[484,   138] loss: 0.025\n",
            "[484,   139] loss: 0.023\n",
            "[484,   140] loss: 0.026\n",
            "[484,   141] loss: 0.027\n",
            "[484,   142] loss: 0.017\n",
            "[484,   143] loss: 0.029\n",
            "[484,   144] loss: 0.022\n",
            "[484,   145] loss: 0.026\n",
            "[484,   146] loss: 0.024\n",
            "[484,   147] loss: 0.011\n",
            "[485,     1] loss: 0.030\n",
            "[485,     2] loss: 0.021\n",
            "[485,     3] loss: 0.024\n",
            "[485,     4] loss: 0.025\n",
            "[485,     5] loss: 0.025\n",
            "[485,     6] loss: 0.027\n",
            "[485,     7] loss: 0.022\n",
            "[485,     8] loss: 0.027\n",
            "[485,     9] loss: 0.023\n",
            "[485,    10] loss: 0.025\n",
            "[485,    11] loss: 0.029\n",
            "[485,    12] loss: 0.023\n",
            "[485,    13] loss: 0.020\n",
            "[485,    14] loss: 0.022\n",
            "[485,    15] loss: 0.020\n",
            "[485,    16] loss: 0.023\n",
            "[485,    17] loss: 0.030\n",
            "[485,    18] loss: 0.023\n",
            "[485,    19] loss: 0.023\n",
            "[485,    20] loss: 0.017\n",
            "[485,    21] loss: 0.025\n",
            "[485,    22] loss: 0.033\n",
            "[485,    23] loss: 0.026\n",
            "[485,    24] loss: 0.029\n",
            "[485,    25] loss: 0.026\n",
            "[485,    26] loss: 0.030\n",
            "[485,    27] loss: 0.023\n",
            "[485,    28] loss: 0.024\n",
            "[485,    29] loss: 0.027\n",
            "[485,    30] loss: 0.025\n",
            "[485,    31] loss: 0.021\n",
            "[485,    32] loss: 0.028\n",
            "[485,    33] loss: 0.025\n",
            "[485,    34] loss: 0.025\n",
            "[485,    35] loss: 0.028\n",
            "[485,    36] loss: 0.030\n",
            "[485,    37] loss: 0.021\n",
            "[485,    38] loss: 0.028\n",
            "[485,    39] loss: 0.025\n",
            "[485,    40] loss: 0.023\n",
            "[485,    41] loss: 0.028\n",
            "[485,    42] loss: 0.028\n",
            "[485,    43] loss: 0.028\n",
            "[485,    44] loss: 0.026\n",
            "[485,    45] loss: 0.027\n",
            "[485,    46] loss: 0.027\n",
            "[485,    47] loss: 0.027\n",
            "[485,    48] loss: 0.021\n",
            "[485,    49] loss: 0.027\n",
            "[485,    50] loss: 0.028\n",
            "[485,    51] loss: 0.023\n",
            "[485,    52] loss: 0.026\n",
            "[485,    53] loss: 0.025\n",
            "[485,    54] loss: 0.019\n",
            "[485,    55] loss: 0.030\n",
            "[485,    56] loss: 0.021\n",
            "[485,    57] loss: 0.023\n",
            "[485,    58] loss: 0.023\n",
            "[485,    59] loss: 0.025\n",
            "[485,    60] loss: 0.025\n",
            "[485,    61] loss: 0.030\n",
            "[485,    62] loss: 0.023\n",
            "[485,    63] loss: 0.023\n",
            "[485,    64] loss: 0.025\n",
            "[485,    65] loss: 0.026\n",
            "[485,    66] loss: 0.024\n",
            "[485,    67] loss: 0.024\n",
            "[485,    68] loss: 0.026\n",
            "[485,    69] loss: 0.026\n",
            "[485,    70] loss: 0.027\n",
            "[485,    71] loss: 0.025\n",
            "[485,    72] loss: 0.027\n",
            "[485,    73] loss: 0.025\n",
            "[485,    74] loss: 0.024\n",
            "[485,    75] loss: 0.023\n",
            "[485,    76] loss: 0.020\n",
            "[485,    77] loss: 0.027\n",
            "[485,    78] loss: 0.025\n",
            "[485,    79] loss: 0.020\n",
            "[485,    80] loss: 0.023\n",
            "[485,    81] loss: 0.021\n",
            "[485,    82] loss: 0.027\n",
            "[485,    83] loss: 0.030\n",
            "[485,    84] loss: 0.026\n",
            "[485,    85] loss: 0.025\n",
            "[485,    86] loss: 0.023\n",
            "[485,    87] loss: 0.027\n",
            "[485,    88] loss: 0.023\n",
            "[485,    89] loss: 0.025\n",
            "[485,    90] loss: 0.022\n",
            "[485,    91] loss: 0.029\n",
            "[485,    92] loss: 0.024\n",
            "[485,    93] loss: 0.025\n",
            "[485,    94] loss: 0.027\n",
            "[485,    95] loss: 0.023\n",
            "[485,    96] loss: 0.028\n",
            "[485,    97] loss: 0.021\n",
            "[485,    98] loss: 0.027\n",
            "[485,    99] loss: 0.033\n",
            "[485,   100] loss: 0.030\n",
            "[485,   101] loss: 0.029\n",
            "[485,   102] loss: 0.024\n",
            "[485,   103] loss: 0.026\n",
            "[485,   104] loss: 0.024\n",
            "[485,   105] loss: 0.034\n",
            "[485,   106] loss: 0.023\n",
            "[485,   107] loss: 0.026\n",
            "[485,   108] loss: 0.028\n",
            "[485,   109] loss: 0.023\n",
            "[485,   110] loss: 0.025\n",
            "[485,   111] loss: 0.027\n",
            "[485,   112] loss: 0.025\n",
            "[485,   113] loss: 0.020\n",
            "[485,   114] loss: 0.022\n",
            "[485,   115] loss: 0.025\n",
            "[485,   116] loss: 0.023\n",
            "[485,   117] loss: 0.029\n",
            "[485,   118] loss: 0.019\n",
            "[485,   119] loss: 0.024\n",
            "[485,   120] loss: 0.025\n",
            "[485,   121] loss: 0.024\n",
            "[485,   122] loss: 0.031\n",
            "[485,   123] loss: 0.023\n",
            "[485,   124] loss: 0.023\n",
            "[485,   125] loss: 0.020\n",
            "[485,   126] loss: 0.027\n",
            "[485,   127] loss: 0.021\n",
            "[485,   128] loss: 0.023\n",
            "[485,   129] loss: 0.027\n",
            "[485,   130] loss: 0.027\n",
            "[485,   131] loss: 0.022\n",
            "[485,   132] loss: 0.027\n",
            "[485,   133] loss: 0.026\n",
            "[485,   134] loss: 0.027\n",
            "[485,   135] loss: 0.029\n",
            "[485,   136] loss: 0.027\n",
            "[485,   137] loss: 0.024\n",
            "[485,   138] loss: 0.024\n",
            "[485,   139] loss: 0.029\n",
            "[485,   140] loss: 0.030\n",
            "[485,   141] loss: 0.020\n",
            "[485,   142] loss: 0.026\n",
            "[485,   143] loss: 0.028\n",
            "[485,   144] loss: 0.027\n",
            "[485,   145] loss: 0.024\n",
            "[485,   146] loss: 0.025\n",
            "[485,   147] loss: 0.021\n",
            "[486,     1] loss: 0.023\n",
            "[486,     2] loss: 0.025\n",
            "[486,     3] loss: 0.025\n",
            "[486,     4] loss: 0.027\n",
            "[486,     5] loss: 0.024\n",
            "[486,     6] loss: 0.023\n",
            "[486,     7] loss: 0.019\n",
            "[486,     8] loss: 0.022\n",
            "[486,     9] loss: 0.024\n",
            "[486,    10] loss: 0.028\n",
            "[486,    11] loss: 0.025\n",
            "[486,    12] loss: 0.025\n",
            "[486,    13] loss: 0.027\n",
            "[486,    14] loss: 0.023\n",
            "[486,    15] loss: 0.022\n",
            "[486,    16] loss: 0.026\n",
            "[486,    17] loss: 0.024\n",
            "[486,    18] loss: 0.022\n",
            "[486,    19] loss: 0.026\n",
            "[486,    20] loss: 0.030\n",
            "[486,    21] loss: 0.027\n",
            "[486,    22] loss: 0.021\n",
            "[486,    23] loss: 0.026\n",
            "[486,    24] loss: 0.023\n",
            "[486,    25] loss: 0.025\n",
            "[486,    26] loss: 0.027\n",
            "[486,    27] loss: 0.021\n",
            "[486,    28] loss: 0.028\n",
            "[486,    29] loss: 0.027\n",
            "[486,    30] loss: 0.024\n",
            "[486,    31] loss: 0.028\n",
            "[486,    32] loss: 0.020\n",
            "[486,    33] loss: 0.024\n",
            "[486,    34] loss: 0.026\n",
            "[486,    35] loss: 0.024\n",
            "[486,    36] loss: 0.025\n",
            "[486,    37] loss: 0.027\n",
            "[486,    38] loss: 0.030\n",
            "[486,    39] loss: 0.023\n",
            "[486,    40] loss: 0.027\n",
            "[486,    41] loss: 0.023\n",
            "[486,    42] loss: 0.019\n",
            "[486,    43] loss: 0.020\n",
            "[486,    44] loss: 0.022\n",
            "[486,    45] loss: 0.027\n",
            "[486,    46] loss: 0.030\n",
            "[486,    47] loss: 0.022\n",
            "[486,    48] loss: 0.034\n",
            "[486,    49] loss: 0.023\n",
            "[486,    50] loss: 0.023\n",
            "[486,    51] loss: 0.024\n",
            "[486,    52] loss: 0.021\n",
            "[486,    53] loss: 0.020\n",
            "[486,    54] loss: 0.024\n",
            "[486,    55] loss: 0.024\n",
            "[486,    56] loss: 0.027\n",
            "[486,    57] loss: 0.025\n",
            "[486,    58] loss: 0.025\n",
            "[486,    59] loss: 0.027\n",
            "[486,    60] loss: 0.027\n",
            "[486,    61] loss: 0.026\n",
            "[486,    62] loss: 0.022\n",
            "[486,    63] loss: 0.019\n",
            "[486,    64] loss: 0.027\n",
            "[486,    65] loss: 0.023\n",
            "[486,    66] loss: 0.026\n",
            "[486,    67] loss: 0.029\n",
            "[486,    68] loss: 0.026\n",
            "[486,    69] loss: 0.025\n",
            "[486,    70] loss: 0.025\n",
            "[486,    71] loss: 0.024\n",
            "[486,    72] loss: 0.024\n",
            "[486,    73] loss: 0.028\n",
            "[486,    74] loss: 0.020\n",
            "[486,    75] loss: 0.023\n",
            "[486,    76] loss: 0.025\n",
            "[486,    77] loss: 0.023\n",
            "[486,    78] loss: 0.027\n",
            "[486,    79] loss: 0.030\n",
            "[486,    80] loss: 0.024\n",
            "[486,    81] loss: 0.027\n",
            "[486,    82] loss: 0.030\n",
            "[486,    83] loss: 0.023\n",
            "[486,    84] loss: 0.019\n",
            "[486,    85] loss: 0.029\n",
            "[486,    86] loss: 0.030\n",
            "[486,    87] loss: 0.026\n",
            "[486,    88] loss: 0.029\n",
            "[486,    89] loss: 0.021\n",
            "[486,    90] loss: 0.034\n",
            "[486,    91] loss: 0.027\n",
            "[486,    92] loss: 0.026\n",
            "[486,    93] loss: 0.027\n",
            "[486,    94] loss: 0.026\n",
            "[486,    95] loss: 0.025\n",
            "[486,    96] loss: 0.027\n",
            "[486,    97] loss: 0.031\n",
            "[486,    98] loss: 0.026\n",
            "[486,    99] loss: 0.027\n",
            "[486,   100] loss: 0.023\n",
            "[486,   101] loss: 0.027\n",
            "[486,   102] loss: 0.024\n",
            "[486,   103] loss: 0.034\n",
            "[486,   104] loss: 0.028\n",
            "[486,   105] loss: 0.023\n",
            "[486,   106] loss: 0.026\n",
            "[486,   107] loss: 0.023\n",
            "[486,   108] loss: 0.030\n",
            "[486,   109] loss: 0.028\n",
            "[486,   110] loss: 0.022\n",
            "[486,   111] loss: 0.023\n",
            "[486,   112] loss: 0.021\n",
            "[486,   113] loss: 0.023\n",
            "[486,   114] loss: 0.024\n",
            "[486,   115] loss: 0.027\n",
            "[486,   116] loss: 0.025\n",
            "[486,   117] loss: 0.027\n",
            "[486,   118] loss: 0.030\n",
            "[486,   119] loss: 0.029\n",
            "[486,   120] loss: 0.027\n",
            "[486,   121] loss: 0.029\n",
            "[486,   122] loss: 0.027\n",
            "[486,   123] loss: 0.027\n",
            "[486,   124] loss: 0.023\n",
            "[486,   125] loss: 0.028\n",
            "[486,   126] loss: 0.030\n",
            "[486,   127] loss: 0.026\n",
            "[486,   128] loss: 0.030\n",
            "[486,   129] loss: 0.020\n",
            "[486,   130] loss: 0.026\n",
            "[486,   131] loss: 0.020\n",
            "[486,   132] loss: 0.022\n",
            "[486,   133] loss: 0.029\n",
            "[486,   134] loss: 0.021\n",
            "[486,   135] loss: 0.023\n",
            "[486,   136] loss: 0.020\n",
            "[486,   137] loss: 0.024\n",
            "[486,   138] loss: 0.019\n",
            "[486,   139] loss: 0.020\n",
            "[486,   140] loss: 0.027\n",
            "[486,   141] loss: 0.023\n",
            "[486,   142] loss: 0.027\n",
            "[486,   143] loss: 0.029\n",
            "[486,   144] loss: 0.032\n",
            "[486,   145] loss: 0.022\n",
            "[486,   146] loss: 0.023\n",
            "[486,   147] loss: 0.029\n",
            "[487,     1] loss: 0.027\n",
            "[487,     2] loss: 0.020\n",
            "[487,     3] loss: 0.022\n",
            "[487,     4] loss: 0.028\n",
            "[487,     5] loss: 0.027\n",
            "[487,     6] loss: 0.024\n",
            "[487,     7] loss: 0.023\n",
            "[487,     8] loss: 0.024\n",
            "[487,     9] loss: 0.023\n",
            "[487,    10] loss: 0.025\n",
            "[487,    11] loss: 0.022\n",
            "[487,    12] loss: 0.026\n",
            "[487,    13] loss: 0.023\n",
            "[487,    14] loss: 0.021\n",
            "[487,    15] loss: 0.028\n",
            "[487,    16] loss: 0.020\n",
            "[487,    17] loss: 0.024\n",
            "[487,    18] loss: 0.023\n",
            "[487,    19] loss: 0.021\n",
            "[487,    20] loss: 0.025\n",
            "[487,    21] loss: 0.027\n",
            "[487,    22] loss: 0.023\n",
            "[487,    23] loss: 0.027\n",
            "[487,    24] loss: 0.028\n",
            "[487,    25] loss: 0.023\n",
            "[487,    26] loss: 0.023\n",
            "[487,    27] loss: 0.027\n",
            "[487,    28] loss: 0.025\n",
            "[487,    29] loss: 0.023\n",
            "[487,    30] loss: 0.023\n",
            "[487,    31] loss: 0.022\n",
            "[487,    32] loss: 0.029\n",
            "[487,    33] loss: 0.024\n",
            "[487,    34] loss: 0.018\n",
            "[487,    35] loss: 0.022\n",
            "[487,    36] loss: 0.023\n",
            "[487,    37] loss: 0.030\n",
            "[487,    38] loss: 0.027\n",
            "[487,    39] loss: 0.028\n",
            "[487,    40] loss: 0.027\n",
            "[487,    41] loss: 0.025\n",
            "[487,    42] loss: 0.027\n",
            "[487,    43] loss: 0.030\n",
            "[487,    44] loss: 0.023\n",
            "[487,    45] loss: 0.031\n",
            "[487,    46] loss: 0.022\n",
            "[487,    47] loss: 0.032\n",
            "[487,    48] loss: 0.027\n",
            "[487,    49] loss: 0.022\n",
            "[487,    50] loss: 0.027\n",
            "[487,    51] loss: 0.023\n",
            "[487,    52] loss: 0.032\n",
            "[487,    53] loss: 0.024\n",
            "[487,    54] loss: 0.021\n",
            "[487,    55] loss: 0.030\n",
            "[487,    56] loss: 0.028\n",
            "[487,    57] loss: 0.025\n",
            "[487,    58] loss: 0.026\n",
            "[487,    59] loss: 0.020\n",
            "[487,    60] loss: 0.031\n",
            "[487,    61] loss: 0.030\n",
            "[487,    62] loss: 0.023\n",
            "[487,    63] loss: 0.023\n",
            "[487,    64] loss: 0.026\n",
            "[487,    65] loss: 0.021\n",
            "[487,    66] loss: 0.026\n",
            "[487,    67] loss: 0.023\n",
            "[487,    68] loss: 0.026\n",
            "[487,    69] loss: 0.029\n",
            "[487,    70] loss: 0.024\n",
            "[487,    71] loss: 0.022\n",
            "[487,    72] loss: 0.030\n",
            "[487,    73] loss: 0.027\n",
            "[487,    74] loss: 0.020\n",
            "[487,    75] loss: 0.024\n",
            "[487,    76] loss: 0.027\n",
            "[487,    77] loss: 0.025\n",
            "[487,    78] loss: 0.023\n",
            "[487,    79] loss: 0.021\n",
            "[487,    80] loss: 0.023\n",
            "[487,    81] loss: 0.026\n",
            "[487,    82] loss: 0.028\n",
            "[487,    83] loss: 0.027\n",
            "[487,    84] loss: 0.022\n",
            "[487,    85] loss: 0.026\n",
            "[487,    86] loss: 0.026\n",
            "[487,    87] loss: 0.021\n",
            "[487,    88] loss: 0.026\n",
            "[487,    89] loss: 0.028\n",
            "[487,    90] loss: 0.021\n",
            "[487,    91] loss: 0.022\n",
            "[487,    92] loss: 0.030\n",
            "[487,    93] loss: 0.024\n",
            "[487,    94] loss: 0.031\n",
            "[487,    95] loss: 0.027\n",
            "[487,    96] loss: 0.019\n",
            "[487,    97] loss: 0.030\n",
            "[487,    98] loss: 0.020\n",
            "[487,    99] loss: 0.029\n",
            "[487,   100] loss: 0.022\n",
            "[487,   101] loss: 0.027\n",
            "[487,   102] loss: 0.025\n",
            "[487,   103] loss: 0.024\n",
            "[487,   104] loss: 0.023\n",
            "[487,   105] loss: 0.023\n",
            "[487,   106] loss: 0.024\n",
            "[487,   107] loss: 0.023\n",
            "[487,   108] loss: 0.026\n",
            "[487,   109] loss: 0.023\n",
            "[487,   110] loss: 0.023\n",
            "[487,   111] loss: 0.023\n",
            "[487,   112] loss: 0.029\n",
            "[487,   113] loss: 0.021\n",
            "[487,   114] loss: 0.024\n",
            "[487,   115] loss: 0.024\n",
            "[487,   116] loss: 0.021\n",
            "[487,   117] loss: 0.022\n",
            "[487,   118] loss: 0.027\n",
            "[487,   119] loss: 0.023\n",
            "[487,   120] loss: 0.028\n",
            "[487,   121] loss: 0.023\n",
            "[487,   122] loss: 0.025\n",
            "[487,   123] loss: 0.027\n",
            "[487,   124] loss: 0.036\n",
            "[487,   125] loss: 0.027\n",
            "[487,   126] loss: 0.031\n",
            "[487,   127] loss: 0.029\n",
            "[487,   128] loss: 0.026\n",
            "[487,   129] loss: 0.026\n",
            "[487,   130] loss: 0.028\n",
            "[487,   131] loss: 0.029\n",
            "[487,   132] loss: 0.027\n",
            "[487,   133] loss: 0.019\n",
            "[487,   134] loss: 0.026\n",
            "[487,   135] loss: 0.028\n",
            "[487,   136] loss: 0.025\n",
            "[487,   137] loss: 0.020\n",
            "[487,   138] loss: 0.024\n",
            "[487,   139] loss: 0.023\n",
            "[487,   140] loss: 0.031\n",
            "[487,   141] loss: 0.027\n",
            "[487,   142] loss: 0.027\n",
            "[487,   143] loss: 0.033\n",
            "[487,   144] loss: 0.023\n",
            "[487,   145] loss: 0.025\n",
            "[487,   146] loss: 0.022\n",
            "[487,   147] loss: 0.032\n",
            "[488,     1] loss: 0.021\n",
            "[488,     2] loss: 0.027\n",
            "[488,     3] loss: 0.022\n",
            "[488,     4] loss: 0.027\n",
            "[488,     5] loss: 0.028\n",
            "[488,     6] loss: 0.021\n",
            "[488,     7] loss: 0.026\n",
            "[488,     8] loss: 0.027\n",
            "[488,     9] loss: 0.027\n",
            "[488,    10] loss: 0.021\n",
            "[488,    11] loss: 0.024\n",
            "[488,    12] loss: 0.027\n",
            "[488,    13] loss: 0.029\n",
            "[488,    14] loss: 0.026\n",
            "[488,    15] loss: 0.019\n",
            "[488,    16] loss: 0.026\n",
            "[488,    17] loss: 0.023\n",
            "[488,    18] loss: 0.025\n",
            "[488,    19] loss: 0.029\n",
            "[488,    20] loss: 0.025\n",
            "[488,    21] loss: 0.027\n",
            "[488,    22] loss: 0.024\n",
            "[488,    23] loss: 0.029\n",
            "[488,    24] loss: 0.024\n",
            "[488,    25] loss: 0.029\n",
            "[488,    26] loss: 0.026\n",
            "[488,    27] loss: 0.023\n",
            "[488,    28] loss: 0.026\n",
            "[488,    29] loss: 0.027\n",
            "[488,    30] loss: 0.023\n",
            "[488,    31] loss: 0.024\n",
            "[488,    32] loss: 0.020\n",
            "[488,    33] loss: 0.026\n",
            "[488,    34] loss: 0.024\n",
            "[488,    35] loss: 0.027\n",
            "[488,    36] loss: 0.029\n",
            "[488,    37] loss: 0.025\n",
            "[488,    38] loss: 0.027\n",
            "[488,    39] loss: 0.027\n",
            "[488,    40] loss: 0.019\n",
            "[488,    41] loss: 0.030\n",
            "[488,    42] loss: 0.021\n",
            "[488,    43] loss: 0.023\n",
            "[488,    44] loss: 0.025\n",
            "[488,    45] loss: 0.023\n",
            "[488,    46] loss: 0.027\n",
            "[488,    47] loss: 0.023\n",
            "[488,    48] loss: 0.026\n",
            "[488,    49] loss: 0.027\n",
            "[488,    50] loss: 0.024\n",
            "[488,    51] loss: 0.023\n",
            "[488,    52] loss: 0.025\n",
            "[488,    53] loss: 0.025\n",
            "[488,    54] loss: 0.030\n",
            "[488,    55] loss: 0.020\n",
            "[488,    56] loss: 0.026\n",
            "[488,    57] loss: 0.029\n",
            "[488,    58] loss: 0.019\n",
            "[488,    59] loss: 0.025\n",
            "[488,    60] loss: 0.029\n",
            "[488,    61] loss: 0.020\n",
            "[488,    62] loss: 0.028\n",
            "[488,    63] loss: 0.024\n",
            "[488,    64] loss: 0.029\n",
            "[488,    65] loss: 0.020\n",
            "[488,    66] loss: 0.025\n",
            "[488,    67] loss: 0.031\n",
            "[488,    68] loss: 0.023\n",
            "[488,    69] loss: 0.023\n",
            "[488,    70] loss: 0.020\n",
            "[488,    71] loss: 0.025\n",
            "[488,    72] loss: 0.024\n",
            "[488,    73] loss: 0.026\n",
            "[488,    74] loss: 0.027\n",
            "[488,    75] loss: 0.028\n",
            "[488,    76] loss: 0.028\n",
            "[488,    77] loss: 0.024\n",
            "[488,    78] loss: 0.022\n",
            "[488,    79] loss: 0.030\n",
            "[488,    80] loss: 0.020\n",
            "[488,    81] loss: 0.020\n",
            "[488,    82] loss: 0.021\n",
            "[488,    83] loss: 0.028\n",
            "[488,    84] loss: 0.030\n",
            "[488,    85] loss: 0.027\n",
            "[488,    86] loss: 0.023\n",
            "[488,    87] loss: 0.026\n",
            "[488,    88] loss: 0.025\n",
            "[488,    89] loss: 0.020\n",
            "[488,    90] loss: 0.027\n",
            "[488,    91] loss: 0.030\n",
            "[488,    92] loss: 0.023\n",
            "[488,    93] loss: 0.023\n",
            "[488,    94] loss: 0.029\n",
            "[488,    95] loss: 0.025\n",
            "[488,    96] loss: 0.027\n",
            "[488,    97] loss: 0.027\n",
            "[488,    98] loss: 0.026\n",
            "[488,    99] loss: 0.028\n",
            "[488,   100] loss: 0.024\n",
            "[488,   101] loss: 0.027\n",
            "[488,   102] loss: 0.025\n",
            "[488,   103] loss: 0.027\n",
            "[488,   104] loss: 0.026\n",
            "[488,   105] loss: 0.028\n",
            "[488,   106] loss: 0.030\n",
            "[488,   107] loss: 0.025\n",
            "[488,   108] loss: 0.019\n",
            "[488,   109] loss: 0.026\n",
            "[488,   110] loss: 0.023\n",
            "[488,   111] loss: 0.027\n",
            "[488,   112] loss: 0.022\n",
            "[488,   113] loss: 0.026\n",
            "[488,   114] loss: 0.027\n",
            "[488,   115] loss: 0.030\n",
            "[488,   116] loss: 0.024\n",
            "[488,   117] loss: 0.029\n",
            "[488,   118] loss: 0.021\n",
            "[488,   119] loss: 0.023\n",
            "[488,   120] loss: 0.025\n",
            "[488,   121] loss: 0.026\n",
            "[488,   122] loss: 0.030\n",
            "[488,   123] loss: 0.031\n",
            "[488,   124] loss: 0.023\n",
            "[488,   125] loss: 0.020\n",
            "[488,   126] loss: 0.023\n",
            "[488,   127] loss: 0.027\n",
            "[488,   128] loss: 0.026\n",
            "[488,   129] loss: 0.028\n",
            "[488,   130] loss: 0.022\n",
            "[488,   131] loss: 0.025\n",
            "[488,   132] loss: 0.020\n",
            "[488,   133] loss: 0.023\n",
            "[488,   134] loss: 0.024\n",
            "[488,   135] loss: 0.020\n",
            "[488,   136] loss: 0.026\n",
            "[488,   137] loss: 0.029\n",
            "[488,   138] loss: 0.023\n",
            "[488,   139] loss: 0.022\n",
            "[488,   140] loss: 0.031\n",
            "[488,   141] loss: 0.023\n",
            "[488,   142] loss: 0.027\n",
            "[488,   143] loss: 0.027\n",
            "[488,   144] loss: 0.029\n",
            "[488,   145] loss: 0.024\n",
            "[488,   146] loss: 0.027\n",
            "[488,   147] loss: 0.018\n",
            "[489,     1] loss: 0.027\n",
            "[489,     2] loss: 0.020\n",
            "[489,     3] loss: 0.023\n",
            "[489,     4] loss: 0.030\n",
            "[489,     5] loss: 0.027\n",
            "[489,     6] loss: 0.030\n",
            "[489,     7] loss: 0.027\n",
            "[489,     8] loss: 0.023\n",
            "[489,     9] loss: 0.028\n",
            "[489,    10] loss: 0.023\n",
            "[489,    11] loss: 0.028\n",
            "[489,    12] loss: 0.025\n",
            "[489,    13] loss: 0.028\n",
            "[489,    14] loss: 0.028\n",
            "[489,    15] loss: 0.021\n",
            "[489,    16] loss: 0.024\n",
            "[489,    17] loss: 0.023\n",
            "[489,    18] loss: 0.029\n",
            "[489,    19] loss: 0.023\n",
            "[489,    20] loss: 0.021\n",
            "[489,    21] loss: 0.023\n",
            "[489,    22] loss: 0.027\n",
            "[489,    23] loss: 0.028\n",
            "[489,    24] loss: 0.023\n",
            "[489,    25] loss: 0.026\n",
            "[489,    26] loss: 0.032\n",
            "[489,    27] loss: 0.027\n",
            "[489,    28] loss: 0.023\n",
            "[489,    29] loss: 0.027\n",
            "[489,    30] loss: 0.022\n",
            "[489,    31] loss: 0.030\n",
            "[489,    32] loss: 0.019\n",
            "[489,    33] loss: 0.029\n",
            "[489,    34] loss: 0.022\n",
            "[489,    35] loss: 0.027\n",
            "[489,    36] loss: 0.025\n",
            "[489,    37] loss: 0.021\n",
            "[489,    38] loss: 0.030\n",
            "[489,    39] loss: 0.023\n",
            "[489,    40] loss: 0.025\n",
            "[489,    41] loss: 0.030\n",
            "[489,    42] loss: 0.022\n",
            "[489,    43] loss: 0.024\n",
            "[489,    44] loss: 0.026\n",
            "[489,    45] loss: 0.025\n",
            "[489,    46] loss: 0.023\n",
            "[489,    47] loss: 0.017\n",
            "[489,    48] loss: 0.026\n",
            "[489,    49] loss: 0.027\n",
            "[489,    50] loss: 0.023\n",
            "[489,    51] loss: 0.028\n",
            "[489,    52] loss: 0.028\n",
            "[489,    53] loss: 0.023\n",
            "[489,    54] loss: 0.034\n",
            "[489,    55] loss: 0.029\n",
            "[489,    56] loss: 0.016\n",
            "[489,    57] loss: 0.028\n",
            "[489,    58] loss: 0.026\n",
            "[489,    59] loss: 0.023\n",
            "[489,    60] loss: 0.025\n",
            "[489,    61] loss: 0.024\n",
            "[489,    62] loss: 0.029\n",
            "[489,    63] loss: 0.020\n",
            "[489,    64] loss: 0.027\n",
            "[489,    65] loss: 0.022\n",
            "[489,    66] loss: 0.020\n",
            "[489,    67] loss: 0.028\n",
            "[489,    68] loss: 0.019\n",
            "[489,    69] loss: 0.029\n",
            "[489,    70] loss: 0.030\n",
            "[489,    71] loss: 0.029\n",
            "[489,    72] loss: 0.026\n",
            "[489,    73] loss: 0.026\n",
            "[489,    74] loss: 0.027\n",
            "[489,    75] loss: 0.024\n",
            "[489,    76] loss: 0.027\n",
            "[489,    77] loss: 0.027\n",
            "[489,    78] loss: 0.020\n",
            "[489,    79] loss: 0.029\n",
            "[489,    80] loss: 0.021\n",
            "[489,    81] loss: 0.028\n",
            "[489,    82] loss: 0.027\n",
            "[489,    83] loss: 0.024\n",
            "[489,    84] loss: 0.026\n",
            "[489,    85] loss: 0.023\n",
            "[489,    86] loss: 0.024\n",
            "[489,    87] loss: 0.033\n",
            "[489,    88] loss: 0.026\n",
            "[489,    89] loss: 0.021\n",
            "[489,    90] loss: 0.026\n",
            "[489,    91] loss: 0.017\n",
            "[489,    92] loss: 0.026\n",
            "[489,    93] loss: 0.023\n",
            "[489,    94] loss: 0.026\n",
            "[489,    95] loss: 0.024\n",
            "[489,    96] loss: 0.026\n",
            "[489,    97] loss: 0.027\n",
            "[489,    98] loss: 0.027\n",
            "[489,    99] loss: 0.023\n",
            "[489,   100] loss: 0.022\n",
            "[489,   101] loss: 0.026\n",
            "[489,   102] loss: 0.025\n",
            "[489,   103] loss: 0.020\n",
            "[489,   104] loss: 0.026\n",
            "[489,   105] loss: 0.023\n",
            "[489,   106] loss: 0.029\n",
            "[489,   107] loss: 0.028\n",
            "[489,   108] loss: 0.023\n",
            "[489,   109] loss: 0.027\n",
            "[489,   110] loss: 0.023\n",
            "[489,   111] loss: 0.019\n",
            "[489,   112] loss: 0.025\n",
            "[489,   113] loss: 0.027\n",
            "[489,   114] loss: 0.026\n",
            "[489,   115] loss: 0.031\n",
            "[489,   116] loss: 0.025\n",
            "[489,   117] loss: 0.027\n",
            "[489,   118] loss: 0.024\n",
            "[489,   119] loss: 0.025\n",
            "[489,   120] loss: 0.024\n",
            "[489,   121] loss: 0.020\n",
            "[489,   122] loss: 0.027\n",
            "[489,   123] loss: 0.026\n",
            "[489,   124] loss: 0.029\n",
            "[489,   125] loss: 0.030\n",
            "[489,   126] loss: 0.027\n",
            "[489,   127] loss: 0.027\n",
            "[489,   128] loss: 0.023\n",
            "[489,   129] loss: 0.022\n",
            "[489,   130] loss: 0.028\n",
            "[489,   131] loss: 0.021\n",
            "[489,   132] loss: 0.028\n",
            "[489,   133] loss: 0.025\n",
            "[489,   134] loss: 0.030\n",
            "[489,   135] loss: 0.025\n",
            "[489,   136] loss: 0.027\n",
            "[489,   137] loss: 0.023\n",
            "[489,   138] loss: 0.022\n",
            "[489,   139] loss: 0.027\n",
            "[489,   140] loss: 0.029\n",
            "[489,   141] loss: 0.027\n",
            "[489,   142] loss: 0.023\n",
            "[489,   143] loss: 0.027\n",
            "[489,   144] loss: 0.024\n",
            "[489,   145] loss: 0.018\n",
            "[489,   146] loss: 0.024\n",
            "[489,   147] loss: 0.018\n",
            "[490,     1] loss: 0.018\n",
            "[490,     2] loss: 0.028\n",
            "[490,     3] loss: 0.025\n",
            "[490,     4] loss: 0.027\n",
            "[490,     5] loss: 0.023\n",
            "[490,     6] loss: 0.020\n",
            "[490,     7] loss: 0.023\n",
            "[490,     8] loss: 0.030\n",
            "[490,     9] loss: 0.029\n",
            "[490,    10] loss: 0.026\n",
            "[490,    11] loss: 0.023\n",
            "[490,    12] loss: 0.022\n",
            "[490,    13] loss: 0.025\n",
            "[490,    14] loss: 0.029\n",
            "[490,    15] loss: 0.030\n",
            "[490,    16] loss: 0.023\n",
            "[490,    17] loss: 0.030\n",
            "[490,    18] loss: 0.024\n",
            "[490,    19] loss: 0.023\n",
            "[490,    20] loss: 0.023\n",
            "[490,    21] loss: 0.018\n",
            "[490,    22] loss: 0.023\n",
            "[490,    23] loss: 0.022\n",
            "[490,    24] loss: 0.022\n",
            "[490,    25] loss: 0.028\n",
            "[490,    26] loss: 0.027\n",
            "[490,    27] loss: 0.032\n",
            "[490,    28] loss: 0.029\n",
            "[490,    29] loss: 0.022\n",
            "[490,    30] loss: 0.023\n",
            "[490,    31] loss: 0.024\n",
            "[490,    32] loss: 0.027\n",
            "[490,    33] loss: 0.024\n",
            "[490,    34] loss: 0.023\n",
            "[490,    35] loss: 0.026\n",
            "[490,    36] loss: 0.023\n",
            "[490,    37] loss: 0.025\n",
            "[490,    38] loss: 0.027\n",
            "[490,    39] loss: 0.021\n",
            "[490,    40] loss: 0.024\n",
            "[490,    41] loss: 0.023\n",
            "[490,    42] loss: 0.019\n",
            "[490,    43] loss: 0.022\n",
            "[490,    44] loss: 0.027\n",
            "[490,    45] loss: 0.020\n",
            "[490,    46] loss: 0.030\n",
            "[490,    47] loss: 0.027\n",
            "[490,    48] loss: 0.027\n",
            "[490,    49] loss: 0.022\n",
            "[490,    50] loss: 0.023\n",
            "[490,    51] loss: 0.024\n",
            "[490,    52] loss: 0.030\n",
            "[490,    53] loss: 0.024\n",
            "[490,    54] loss: 0.025\n",
            "[490,    55] loss: 0.025\n",
            "[490,    56] loss: 0.020\n",
            "[490,    57] loss: 0.026\n",
            "[490,    58] loss: 0.027\n",
            "[490,    59] loss: 0.027\n",
            "[490,    60] loss: 0.023\n",
            "[490,    61] loss: 0.030\n",
            "[490,    62] loss: 0.027\n",
            "[490,    63] loss: 0.023\n",
            "[490,    64] loss: 0.029\n",
            "[490,    65] loss: 0.031\n",
            "[490,    66] loss: 0.028\n",
            "[490,    67] loss: 0.023\n",
            "[490,    68] loss: 0.029\n",
            "[490,    69] loss: 0.031\n",
            "[490,    70] loss: 0.024\n",
            "[490,    71] loss: 0.024\n",
            "[490,    72] loss: 0.024\n",
            "[490,    73] loss: 0.021\n",
            "[490,    74] loss: 0.023\n",
            "[490,    75] loss: 0.029\n",
            "[490,    76] loss: 0.027\n",
            "[490,    77] loss: 0.027\n",
            "[490,    78] loss: 0.023\n",
            "[490,    79] loss: 0.027\n",
            "[490,    80] loss: 0.028\n",
            "[490,    81] loss: 0.024\n",
            "[490,    82] loss: 0.030\n",
            "[490,    83] loss: 0.027\n",
            "[490,    84] loss: 0.025\n",
            "[490,    85] loss: 0.026\n",
            "[490,    86] loss: 0.027\n",
            "[490,    87] loss: 0.029\n",
            "[490,    88] loss: 0.024\n",
            "[490,    89] loss: 0.023\n",
            "[490,    90] loss: 0.030\n",
            "[490,    91] loss: 0.030\n",
            "[490,    92] loss: 0.023\n",
            "[490,    93] loss: 0.023\n",
            "[490,    94] loss: 0.026\n",
            "[490,    95] loss: 0.027\n",
            "[490,    96] loss: 0.031\n",
            "[490,    97] loss: 0.024\n",
            "[490,    98] loss: 0.029\n",
            "[490,    99] loss: 0.027\n",
            "[490,   100] loss: 0.021\n",
            "[490,   101] loss: 0.020\n",
            "[490,   102] loss: 0.028\n",
            "[490,   103] loss: 0.026\n",
            "[490,   104] loss: 0.022\n",
            "[490,   105] loss: 0.023\n",
            "[490,   106] loss: 0.023\n",
            "[490,   107] loss: 0.024\n",
            "[490,   108] loss: 0.026\n",
            "[490,   109] loss: 0.024\n",
            "[490,   110] loss: 0.023\n",
            "[490,   111] loss: 0.023\n",
            "[490,   112] loss: 0.020\n",
            "[490,   113] loss: 0.026\n",
            "[490,   114] loss: 0.027\n",
            "[490,   115] loss: 0.030\n",
            "[490,   116] loss: 0.021\n",
            "[490,   117] loss: 0.027\n",
            "[490,   118] loss: 0.026\n",
            "[490,   119] loss: 0.027\n",
            "[490,   120] loss: 0.030\n",
            "[490,   121] loss: 0.022\n",
            "[490,   122] loss: 0.022\n",
            "[490,   123] loss: 0.027\n",
            "[490,   124] loss: 0.016\n",
            "[490,   125] loss: 0.027\n",
            "[490,   126] loss: 0.029\n",
            "[490,   127] loss: 0.033\n",
            "[490,   128] loss: 0.020\n",
            "[490,   129] loss: 0.023\n",
            "[490,   130] loss: 0.023\n",
            "[490,   131] loss: 0.022\n",
            "[490,   132] loss: 0.021\n",
            "[490,   133] loss: 0.022\n",
            "[490,   134] loss: 0.018\n",
            "[490,   135] loss: 0.028\n",
            "[490,   136] loss: 0.025\n",
            "[490,   137] loss: 0.027\n",
            "[490,   138] loss: 0.034\n",
            "[490,   139] loss: 0.024\n",
            "[490,   140] loss: 0.030\n",
            "[490,   141] loss: 0.031\n",
            "[490,   142] loss: 0.026\n",
            "[490,   143] loss: 0.024\n",
            "[490,   144] loss: 0.021\n",
            "[490,   145] loss: 0.022\n",
            "[490,   146] loss: 0.026\n",
            "[490,   147] loss: 0.032\n",
            "[491,     1] loss: 0.021\n",
            "[491,     2] loss: 0.027\n",
            "[491,     3] loss: 0.024\n",
            "[491,     4] loss: 0.027\n",
            "[491,     5] loss: 0.025\n",
            "[491,     6] loss: 0.027\n",
            "[491,     7] loss: 0.021\n",
            "[491,     8] loss: 0.021\n",
            "[491,     9] loss: 0.024\n",
            "[491,    10] loss: 0.020\n",
            "[491,    11] loss: 0.027\n",
            "[491,    12] loss: 0.026\n",
            "[491,    13] loss: 0.022\n",
            "[491,    14] loss: 0.023\n",
            "[491,    15] loss: 0.024\n",
            "[491,    16] loss: 0.027\n",
            "[491,    17] loss: 0.023\n",
            "[491,    18] loss: 0.023\n",
            "[491,    19] loss: 0.029\n",
            "[491,    20] loss: 0.023\n",
            "[491,    21] loss: 0.027\n",
            "[491,    22] loss: 0.023\n",
            "[491,    23] loss: 0.030\n",
            "[491,    24] loss: 0.023\n",
            "[491,    25] loss: 0.027\n",
            "[491,    26] loss: 0.023\n",
            "[491,    27] loss: 0.033\n",
            "[491,    28] loss: 0.026\n",
            "[491,    29] loss: 0.023\n",
            "[491,    30] loss: 0.027\n",
            "[491,    31] loss: 0.025\n",
            "[491,    32] loss: 0.026\n",
            "[491,    33] loss: 0.028\n",
            "[491,    34] loss: 0.028\n",
            "[491,    35] loss: 0.025\n",
            "[491,    36] loss: 0.025\n",
            "[491,    37] loss: 0.027\n",
            "[491,    38] loss: 0.024\n",
            "[491,    39] loss: 0.027\n",
            "[491,    40] loss: 0.027\n",
            "[491,    41] loss: 0.030\n",
            "[491,    42] loss: 0.020\n",
            "[491,    43] loss: 0.024\n",
            "[491,    44] loss: 0.021\n",
            "[491,    45] loss: 0.028\n",
            "[491,    46] loss: 0.023\n",
            "[491,    47] loss: 0.023\n",
            "[491,    48] loss: 0.023\n",
            "[491,    49] loss: 0.020\n",
            "[491,    50] loss: 0.024\n",
            "[491,    51] loss: 0.023\n",
            "[491,    52] loss: 0.027\n",
            "[491,    53] loss: 0.028\n",
            "[491,    54] loss: 0.019\n",
            "[491,    55] loss: 0.027\n",
            "[491,    56] loss: 0.030\n",
            "[491,    57] loss: 0.027\n",
            "[491,    58] loss: 0.028\n",
            "[491,    59] loss: 0.027\n",
            "[491,    60] loss: 0.031\n",
            "[491,    61] loss: 0.027\n",
            "[491,    62] loss: 0.023\n",
            "[491,    63] loss: 0.023\n",
            "[491,    64] loss: 0.020\n",
            "[491,    65] loss: 0.027\n",
            "[491,    66] loss: 0.030\n",
            "[491,    67] loss: 0.020\n",
            "[491,    68] loss: 0.030\n",
            "[491,    69] loss: 0.023\n",
            "[491,    70] loss: 0.027\n",
            "[491,    71] loss: 0.027\n",
            "[491,    72] loss: 0.027\n",
            "[491,    73] loss: 0.019\n",
            "[491,    74] loss: 0.027\n",
            "[491,    75] loss: 0.030\n",
            "[491,    76] loss: 0.029\n",
            "[491,    77] loss: 0.025\n",
            "[491,    78] loss: 0.026\n",
            "[491,    79] loss: 0.027\n",
            "[491,    80] loss: 0.029\n",
            "[491,    81] loss: 0.020\n",
            "[491,    82] loss: 0.023\n",
            "[491,    83] loss: 0.028\n",
            "[491,    84] loss: 0.022\n",
            "[491,    85] loss: 0.023\n",
            "[491,    86] loss: 0.026\n",
            "[491,    87] loss: 0.025\n",
            "[491,    88] loss: 0.027\n",
            "[491,    89] loss: 0.024\n",
            "[491,    90] loss: 0.027\n",
            "[491,    91] loss: 0.027\n",
            "[491,    92] loss: 0.024\n",
            "[491,    93] loss: 0.028\n",
            "[491,    94] loss: 0.022\n",
            "[491,    95] loss: 0.030\n",
            "[491,    96] loss: 0.029\n",
            "[491,    97] loss: 0.027\n",
            "[491,    98] loss: 0.019\n",
            "[491,    99] loss: 0.027\n",
            "[491,   100] loss: 0.027\n",
            "[491,   101] loss: 0.028\n",
            "[491,   102] loss: 0.023\n",
            "[491,   103] loss: 0.029\n",
            "[491,   104] loss: 0.027\n",
            "[491,   105] loss: 0.025\n",
            "[491,   106] loss: 0.023\n",
            "[491,   107] loss: 0.026\n",
            "[491,   108] loss: 0.025\n",
            "[491,   109] loss: 0.024\n",
            "[491,   110] loss: 0.025\n",
            "[491,   111] loss: 0.025\n",
            "[491,   112] loss: 0.025\n",
            "[491,   113] loss: 0.021\n",
            "[491,   114] loss: 0.024\n",
            "[491,   115] loss: 0.023\n",
            "[491,   116] loss: 0.025\n",
            "[491,   117] loss: 0.020\n",
            "[491,   118] loss: 0.025\n",
            "[491,   119] loss: 0.030\n",
            "[491,   120] loss: 0.026\n",
            "[491,   121] loss: 0.033\n",
            "[491,   122] loss: 0.025\n",
            "[491,   123] loss: 0.023\n",
            "[491,   124] loss: 0.021\n",
            "[491,   125] loss: 0.025\n",
            "[491,   126] loss: 0.023\n",
            "[491,   127] loss: 0.023\n",
            "[491,   128] loss: 0.023\n",
            "[491,   129] loss: 0.028\n",
            "[491,   130] loss: 0.028\n",
            "[491,   131] loss: 0.023\n",
            "[491,   132] loss: 0.021\n",
            "[491,   133] loss: 0.027\n",
            "[491,   134] loss: 0.027\n",
            "[491,   135] loss: 0.022\n",
            "[491,   136] loss: 0.027\n",
            "[491,   137] loss: 0.026\n",
            "[491,   138] loss: 0.023\n",
            "[491,   139] loss: 0.021\n",
            "[491,   140] loss: 0.020\n",
            "[491,   141] loss: 0.029\n",
            "[491,   142] loss: 0.028\n",
            "[491,   143] loss: 0.023\n",
            "[491,   144] loss: 0.027\n",
            "[491,   145] loss: 0.024\n",
            "[491,   146] loss: 0.023\n",
            "[491,   147] loss: 0.024\n",
            "[492,     1] loss: 0.025\n",
            "[492,     2] loss: 0.025\n",
            "[492,     3] loss: 0.027\n",
            "[492,     4] loss: 0.027\n",
            "[492,     5] loss: 0.018\n",
            "[492,     6] loss: 0.023\n",
            "[492,     7] loss: 0.023\n",
            "[492,     8] loss: 0.023\n",
            "[492,     9] loss: 0.029\n",
            "[492,    10] loss: 0.023\n",
            "[492,    11] loss: 0.022\n",
            "[492,    12] loss: 0.020\n",
            "[492,    13] loss: 0.027\n",
            "[492,    14] loss: 0.026\n",
            "[492,    15] loss: 0.026\n",
            "[492,    16] loss: 0.023\n",
            "[492,    17] loss: 0.027\n",
            "[492,    18] loss: 0.024\n",
            "[492,    19] loss: 0.020\n",
            "[492,    20] loss: 0.028\n",
            "[492,    21] loss: 0.027\n",
            "[492,    22] loss: 0.027\n",
            "[492,    23] loss: 0.024\n",
            "[492,    24] loss: 0.027\n",
            "[492,    25] loss: 0.025\n",
            "[492,    26] loss: 0.026\n",
            "[492,    27] loss: 0.024\n",
            "[492,    28] loss: 0.023\n",
            "[492,    29] loss: 0.029\n",
            "[492,    30] loss: 0.025\n",
            "[492,    31] loss: 0.029\n",
            "[492,    32] loss: 0.022\n",
            "[492,    33] loss: 0.023\n",
            "[492,    34] loss: 0.020\n",
            "[492,    35] loss: 0.024\n",
            "[492,    36] loss: 0.030\n",
            "[492,    37] loss: 0.021\n",
            "[492,    38] loss: 0.027\n",
            "[492,    39] loss: 0.027\n",
            "[492,    40] loss: 0.028\n",
            "[492,    41] loss: 0.028\n",
            "[492,    42] loss: 0.030\n",
            "[492,    43] loss: 0.027\n",
            "[492,    44] loss: 0.029\n",
            "[492,    45] loss: 0.025\n",
            "[492,    46] loss: 0.028\n",
            "[492,    47] loss: 0.024\n",
            "[492,    48] loss: 0.026\n",
            "[492,    49] loss: 0.023\n",
            "[492,    50] loss: 0.027\n",
            "[492,    51] loss: 0.025\n",
            "[492,    52] loss: 0.029\n",
            "[492,    53] loss: 0.028\n",
            "[492,    54] loss: 0.025\n",
            "[492,    55] loss: 0.026\n",
            "[492,    56] loss: 0.021\n",
            "[492,    57] loss: 0.020\n",
            "[492,    58] loss: 0.016\n",
            "[492,    59] loss: 0.025\n",
            "[492,    60] loss: 0.023\n",
            "[492,    61] loss: 0.033\n",
            "[492,    62] loss: 0.023\n",
            "[492,    63] loss: 0.026\n",
            "[492,    64] loss: 0.023\n",
            "[492,    65] loss: 0.025\n",
            "[492,    66] loss: 0.030\n",
            "[492,    67] loss: 0.024\n",
            "[492,    68] loss: 0.029\n",
            "[492,    69] loss: 0.027\n",
            "[492,    70] loss: 0.026\n",
            "[492,    71] loss: 0.026\n",
            "[492,    72] loss: 0.023\n",
            "[492,    73] loss: 0.023\n",
            "[492,    74] loss: 0.024\n",
            "[492,    75] loss: 0.023\n",
            "[492,    76] loss: 0.030\n",
            "[492,    77] loss: 0.030\n",
            "[492,    78] loss: 0.020\n",
            "[492,    79] loss: 0.027\n",
            "[492,    80] loss: 0.027\n",
            "[492,    81] loss: 0.024\n",
            "[492,    82] loss: 0.028\n",
            "[492,    83] loss: 0.032\n",
            "[492,    84] loss: 0.020\n",
            "[492,    85] loss: 0.024\n",
            "[492,    86] loss: 0.020\n",
            "[492,    87] loss: 0.029\n",
            "[492,    88] loss: 0.020\n",
            "[492,    89] loss: 0.028\n",
            "[492,    90] loss: 0.023\n",
            "[492,    91] loss: 0.025\n",
            "[492,    92] loss: 0.023\n",
            "[492,    93] loss: 0.027\n",
            "[492,    94] loss: 0.028\n",
            "[492,    95] loss: 0.027\n",
            "[492,    96] loss: 0.033\n",
            "[492,    97] loss: 0.028\n",
            "[492,    98] loss: 0.021\n",
            "[492,    99] loss: 0.027\n",
            "[492,   100] loss: 0.022\n",
            "[492,   101] loss: 0.024\n",
            "[492,   102] loss: 0.025\n",
            "[492,   103] loss: 0.027\n",
            "[492,   104] loss: 0.023\n",
            "[492,   105] loss: 0.023\n",
            "[492,   106] loss: 0.026\n",
            "[492,   107] loss: 0.021\n",
            "[492,   108] loss: 0.023\n",
            "[492,   109] loss: 0.027\n",
            "[492,   110] loss: 0.023\n",
            "[492,   111] loss: 0.030\n",
            "[492,   112] loss: 0.027\n",
            "[492,   113] loss: 0.026\n",
            "[492,   114] loss: 0.027\n",
            "[492,   115] loss: 0.025\n",
            "[492,   116] loss: 0.029\n",
            "[492,   117] loss: 0.027\n",
            "[492,   118] loss: 0.023\n",
            "[492,   119] loss: 0.024\n",
            "[492,   120] loss: 0.023\n",
            "[492,   121] loss: 0.027\n",
            "[492,   122] loss: 0.021\n",
            "[492,   123] loss: 0.021\n",
            "[492,   124] loss: 0.022\n",
            "[492,   125] loss: 0.027\n",
            "[492,   126] loss: 0.025\n",
            "[492,   127] loss: 0.026\n",
            "[492,   128] loss: 0.029\n",
            "[492,   129] loss: 0.027\n",
            "[492,   130] loss: 0.025\n",
            "[492,   131] loss: 0.020\n",
            "[492,   132] loss: 0.027\n",
            "[492,   133] loss: 0.022\n",
            "[492,   134] loss: 0.031\n",
            "[492,   135] loss: 0.027\n",
            "[492,   136] loss: 0.023\n",
            "[492,   137] loss: 0.021\n",
            "[492,   138] loss: 0.022\n",
            "[492,   139] loss: 0.029\n",
            "[492,   140] loss: 0.022\n",
            "[492,   141] loss: 0.024\n",
            "[492,   142] loss: 0.027\n",
            "[492,   143] loss: 0.021\n",
            "[492,   144] loss: 0.030\n",
            "[492,   145] loss: 0.027\n",
            "[492,   146] loss: 0.019\n",
            "[492,   147] loss: 0.021\n",
            "[493,     1] loss: 0.020\n",
            "[493,     2] loss: 0.026\n",
            "[493,     3] loss: 0.026\n",
            "[493,     4] loss: 0.027\n",
            "[493,     5] loss: 0.027\n",
            "[493,     6] loss: 0.022\n",
            "[493,     7] loss: 0.029\n",
            "[493,     8] loss: 0.025\n",
            "[493,     9] loss: 0.024\n",
            "[493,    10] loss: 0.028\n",
            "[493,    11] loss: 0.027\n",
            "[493,    12] loss: 0.027\n",
            "[493,    13] loss: 0.023\n",
            "[493,    14] loss: 0.028\n",
            "[493,    15] loss: 0.022\n",
            "[493,    16] loss: 0.023\n",
            "[493,    17] loss: 0.028\n",
            "[493,    18] loss: 0.029\n",
            "[493,    19] loss: 0.023\n",
            "[493,    20] loss: 0.025\n",
            "[493,    21] loss: 0.020\n",
            "[493,    22] loss: 0.027\n",
            "[493,    23] loss: 0.027\n",
            "[493,    24] loss: 0.024\n",
            "[493,    25] loss: 0.023\n",
            "[493,    26] loss: 0.024\n",
            "[493,    27] loss: 0.023\n",
            "[493,    28] loss: 0.027\n",
            "[493,    29] loss: 0.025\n",
            "[493,    30] loss: 0.019\n",
            "[493,    31] loss: 0.024\n",
            "[493,    32] loss: 0.023\n",
            "[493,    33] loss: 0.023\n",
            "[493,    34] loss: 0.026\n",
            "[493,    35] loss: 0.025\n",
            "[493,    36] loss: 0.017\n",
            "[493,    37] loss: 0.023\n",
            "[493,    38] loss: 0.023\n",
            "[493,    39] loss: 0.026\n",
            "[493,    40] loss: 0.028\n",
            "[493,    41] loss: 0.027\n",
            "[493,    42] loss: 0.025\n",
            "[493,    43] loss: 0.025\n",
            "[493,    44] loss: 0.030\n",
            "[493,    45] loss: 0.022\n",
            "[493,    46] loss: 0.029\n",
            "[493,    47] loss: 0.025\n",
            "[493,    48] loss: 0.018\n",
            "[493,    49] loss: 0.027\n",
            "[493,    50] loss: 0.030\n",
            "[493,    51] loss: 0.027\n",
            "[493,    52] loss: 0.024\n",
            "[493,    53] loss: 0.020\n",
            "[493,    54] loss: 0.028\n",
            "[493,    55] loss: 0.023\n",
            "[493,    56] loss: 0.020\n",
            "[493,    57] loss: 0.027\n",
            "[493,    58] loss: 0.027\n",
            "[493,    59] loss: 0.020\n",
            "[493,    60] loss: 0.023\n",
            "[493,    61] loss: 0.033\n",
            "[493,    62] loss: 0.021\n",
            "[493,    63] loss: 0.024\n",
            "[493,    64] loss: 0.028\n",
            "[493,    65] loss: 0.025\n",
            "[493,    66] loss: 0.027\n",
            "[493,    67] loss: 0.020\n",
            "[493,    68] loss: 0.027\n",
            "[493,    69] loss: 0.027\n",
            "[493,    70] loss: 0.023\n",
            "[493,    71] loss: 0.022\n",
            "[493,    72] loss: 0.024\n",
            "[493,    73] loss: 0.022\n",
            "[493,    74] loss: 0.024\n",
            "[493,    75] loss: 0.024\n",
            "[493,    76] loss: 0.023\n",
            "[493,    77] loss: 0.026\n",
            "[493,    78] loss: 0.030\n",
            "[493,    79] loss: 0.023\n",
            "[493,    80] loss: 0.027\n",
            "[493,    81] loss: 0.021\n",
            "[493,    82] loss: 0.020\n",
            "[493,    83] loss: 0.023\n",
            "[493,    84] loss: 0.023\n",
            "[493,    85] loss: 0.024\n",
            "[493,    86] loss: 0.028\n",
            "[493,    87] loss: 0.030\n",
            "[493,    88] loss: 0.027\n",
            "[493,    89] loss: 0.025\n",
            "[493,    90] loss: 0.026\n",
            "[493,    91] loss: 0.029\n",
            "[493,    92] loss: 0.020\n",
            "[493,    93] loss: 0.031\n",
            "[493,    94] loss: 0.023\n",
            "[493,    95] loss: 0.023\n",
            "[493,    96] loss: 0.024\n",
            "[493,    97] loss: 0.025\n",
            "[493,    98] loss: 0.021\n",
            "[493,    99] loss: 0.027\n",
            "[493,   100] loss: 0.023\n",
            "[493,   101] loss: 0.026\n",
            "[493,   102] loss: 0.027\n",
            "[493,   103] loss: 0.024\n",
            "[493,   104] loss: 0.020\n",
            "[493,   105] loss: 0.023\n",
            "[493,   106] loss: 0.024\n",
            "[493,   107] loss: 0.020\n",
            "[493,   108] loss: 0.028\n",
            "[493,   109] loss: 0.023\n",
            "[493,   110] loss: 0.030\n",
            "[493,   111] loss: 0.025\n",
            "[493,   112] loss: 0.023\n",
            "[493,   113] loss: 0.023\n",
            "[493,   114] loss: 0.022\n",
            "[493,   115] loss: 0.025\n",
            "[493,   116] loss: 0.024\n",
            "[493,   117] loss: 0.026\n",
            "[493,   118] loss: 0.027\n",
            "[493,   119] loss: 0.024\n",
            "[493,   120] loss: 0.023\n",
            "[493,   121] loss: 0.024\n",
            "[493,   122] loss: 0.030\n",
            "[493,   123] loss: 0.026\n",
            "[493,   124] loss: 0.031\n",
            "[493,   125] loss: 0.033\n",
            "[493,   126] loss: 0.023\n",
            "[493,   127] loss: 0.029\n",
            "[493,   128] loss: 0.022\n",
            "[493,   129] loss: 0.031\n",
            "[493,   130] loss: 0.023\n",
            "[493,   131] loss: 0.037\n",
            "[493,   132] loss: 0.023\n",
            "[493,   133] loss: 0.027\n",
            "[493,   134] loss: 0.034\n",
            "[493,   135] loss: 0.026\n",
            "[493,   136] loss: 0.027\n",
            "[493,   137] loss: 0.025\n",
            "[493,   138] loss: 0.027\n",
            "[493,   139] loss: 0.022\n",
            "[493,   140] loss: 0.030\n",
            "[493,   141] loss: 0.025\n",
            "[493,   142] loss: 0.027\n",
            "[493,   143] loss: 0.022\n",
            "[493,   144] loss: 0.025\n",
            "[493,   145] loss: 0.023\n",
            "[493,   146] loss: 0.033\n",
            "[493,   147] loss: 0.032\n",
            "[494,     1] loss: 0.023\n",
            "[494,     2] loss: 0.031\n",
            "[494,     3] loss: 0.027\n",
            "[494,     4] loss: 0.024\n",
            "[494,     5] loss: 0.030\n",
            "[494,     6] loss: 0.024\n",
            "[494,     7] loss: 0.021\n",
            "[494,     8] loss: 0.025\n",
            "[494,     9] loss: 0.028\n",
            "[494,    10] loss: 0.028\n",
            "[494,    11] loss: 0.024\n",
            "[494,    12] loss: 0.023\n",
            "[494,    13] loss: 0.034\n",
            "[494,    14] loss: 0.023\n",
            "[494,    15] loss: 0.028\n",
            "[494,    16] loss: 0.027\n",
            "[494,    17] loss: 0.027\n",
            "[494,    18] loss: 0.026\n",
            "[494,    19] loss: 0.023\n",
            "[494,    20] loss: 0.022\n",
            "[494,    21] loss: 0.029\n",
            "[494,    22] loss: 0.027\n",
            "[494,    23] loss: 0.027\n",
            "[494,    24] loss: 0.020\n",
            "[494,    25] loss: 0.030\n",
            "[494,    26] loss: 0.030\n",
            "[494,    27] loss: 0.023\n",
            "[494,    28] loss: 0.029\n",
            "[494,    29] loss: 0.018\n",
            "[494,    30] loss: 0.023\n",
            "[494,    31] loss: 0.024\n",
            "[494,    32] loss: 0.030\n",
            "[494,    33] loss: 0.030\n",
            "[494,    34] loss: 0.027\n",
            "[494,    35] loss: 0.015\n",
            "[494,    36] loss: 0.027\n",
            "[494,    37] loss: 0.023\n",
            "[494,    38] loss: 0.027\n",
            "[494,    39] loss: 0.027\n",
            "[494,    40] loss: 0.025\n",
            "[494,    41] loss: 0.023\n",
            "[494,    42] loss: 0.028\n",
            "[494,    43] loss: 0.027\n",
            "[494,    44] loss: 0.023\n",
            "[494,    45] loss: 0.027\n",
            "[494,    46] loss: 0.025\n",
            "[494,    47] loss: 0.026\n",
            "[494,    48] loss: 0.025\n",
            "[494,    49] loss: 0.024\n",
            "[494,    50] loss: 0.029\n",
            "[494,    51] loss: 0.021\n",
            "[494,    52] loss: 0.025\n",
            "[494,    53] loss: 0.026\n",
            "[494,    54] loss: 0.023\n",
            "[494,    55] loss: 0.025\n",
            "[494,    56] loss: 0.023\n",
            "[494,    57] loss: 0.030\n",
            "[494,    58] loss: 0.026\n",
            "[494,    59] loss: 0.023\n",
            "[494,    60] loss: 0.025\n",
            "[494,    61] loss: 0.020\n",
            "[494,    62] loss: 0.020\n",
            "[494,    63] loss: 0.024\n",
            "[494,    64] loss: 0.020\n",
            "[494,    65] loss: 0.028\n",
            "[494,    66] loss: 0.026\n",
            "[494,    67] loss: 0.027\n",
            "[494,    68] loss: 0.030\n",
            "[494,    69] loss: 0.022\n",
            "[494,    70] loss: 0.025\n",
            "[494,    71] loss: 0.027\n",
            "[494,    72] loss: 0.026\n",
            "[494,    73] loss: 0.027\n",
            "[494,    74] loss: 0.027\n",
            "[494,    75] loss: 0.029\n",
            "[494,    76] loss: 0.027\n",
            "[494,    77] loss: 0.023\n",
            "[494,    78] loss: 0.027\n",
            "[494,    79] loss: 0.023\n",
            "[494,    80] loss: 0.027\n",
            "[494,    81] loss: 0.026\n",
            "[494,    82] loss: 0.030\n",
            "[494,    83] loss: 0.022\n",
            "[494,    84] loss: 0.027\n",
            "[494,    85] loss: 0.024\n",
            "[494,    86] loss: 0.022\n",
            "[494,    87] loss: 0.024\n",
            "[494,    88] loss: 0.021\n",
            "[494,    89] loss: 0.029\n",
            "[494,    90] loss: 0.030\n",
            "[494,    91] loss: 0.027\n",
            "[494,    92] loss: 0.027\n",
            "[494,    93] loss: 0.023\n",
            "[494,    94] loss: 0.023\n",
            "[494,    95] loss: 0.021\n",
            "[494,    96] loss: 0.025\n",
            "[494,    97] loss: 0.025\n",
            "[494,    98] loss: 0.020\n",
            "[494,    99] loss: 0.027\n",
            "[494,   100] loss: 0.028\n",
            "[494,   101] loss: 0.025\n",
            "[494,   102] loss: 0.022\n",
            "[494,   103] loss: 0.021\n",
            "[494,   104] loss: 0.027\n",
            "[494,   105] loss: 0.023\n",
            "[494,   106] loss: 0.025\n",
            "[494,   107] loss: 0.031\n",
            "[494,   108] loss: 0.023\n",
            "[494,   109] loss: 0.027\n",
            "[494,   110] loss: 0.032\n",
            "[494,   111] loss: 0.025\n",
            "[494,   112] loss: 0.030\n",
            "[494,   113] loss: 0.021\n",
            "[494,   114] loss: 0.025\n",
            "[494,   115] loss: 0.024\n",
            "[494,   116] loss: 0.027\n",
            "[494,   117] loss: 0.022\n",
            "[494,   118] loss: 0.024\n",
            "[494,   119] loss: 0.028\n",
            "[494,   120] loss: 0.023\n",
            "[494,   121] loss: 0.024\n",
            "[494,   122] loss: 0.025\n",
            "[494,   123] loss: 0.027\n",
            "[494,   124] loss: 0.027\n",
            "[494,   125] loss: 0.026\n",
            "[494,   126] loss: 0.024\n",
            "[494,   127] loss: 0.023\n",
            "[494,   128] loss: 0.030\n",
            "[494,   129] loss: 0.021\n",
            "[494,   130] loss: 0.022\n",
            "[494,   131] loss: 0.023\n",
            "[494,   132] loss: 0.026\n",
            "[494,   133] loss: 0.024\n",
            "[494,   134] loss: 0.022\n",
            "[494,   135] loss: 0.023\n",
            "[494,   136] loss: 0.026\n",
            "[494,   137] loss: 0.023\n",
            "[494,   138] loss: 0.023\n",
            "[494,   139] loss: 0.020\n",
            "[494,   140] loss: 0.028\n",
            "[494,   141] loss: 0.026\n",
            "[494,   142] loss: 0.024\n",
            "[494,   143] loss: 0.023\n",
            "[494,   144] loss: 0.021\n",
            "[494,   145] loss: 0.029\n",
            "[494,   146] loss: 0.023\n",
            "[494,   147] loss: 0.021\n",
            "[495,     1] loss: 0.023\n",
            "[495,     2] loss: 0.021\n",
            "[495,     3] loss: 0.028\n",
            "[495,     4] loss: 0.027\n",
            "[495,     5] loss: 0.027\n",
            "[495,     6] loss: 0.025\n",
            "[495,     7] loss: 0.026\n",
            "[495,     8] loss: 0.019\n",
            "[495,     9] loss: 0.023\n",
            "[495,    10] loss: 0.028\n",
            "[495,    11] loss: 0.027\n",
            "[495,    12] loss: 0.030\n",
            "[495,    13] loss: 0.022\n",
            "[495,    14] loss: 0.029\n",
            "[495,    15] loss: 0.017\n",
            "[495,    16] loss: 0.025\n",
            "[495,    17] loss: 0.016\n",
            "[495,    18] loss: 0.026\n",
            "[495,    19] loss: 0.032\n",
            "[495,    20] loss: 0.023\n",
            "[495,    21] loss: 0.027\n",
            "[495,    22] loss: 0.027\n",
            "[495,    23] loss: 0.025\n",
            "[495,    24] loss: 0.023\n",
            "[495,    25] loss: 0.027\n",
            "[495,    26] loss: 0.023\n",
            "[495,    27] loss: 0.026\n",
            "[495,    28] loss: 0.030\n",
            "[495,    29] loss: 0.024\n",
            "[495,    30] loss: 0.022\n",
            "[495,    31] loss: 0.021\n",
            "[495,    32] loss: 0.025\n",
            "[495,    33] loss: 0.023\n",
            "[495,    34] loss: 0.023\n",
            "[495,    35] loss: 0.023\n",
            "[495,    36] loss: 0.030\n",
            "[495,    37] loss: 0.027\n",
            "[495,    38] loss: 0.027\n",
            "[495,    39] loss: 0.028\n",
            "[495,    40] loss: 0.030\n",
            "[495,    41] loss: 0.022\n",
            "[495,    42] loss: 0.027\n",
            "[495,    43] loss: 0.028\n",
            "[495,    44] loss: 0.030\n",
            "[495,    45] loss: 0.026\n",
            "[495,    46] loss: 0.028\n",
            "[495,    47] loss: 0.025\n",
            "[495,    48] loss: 0.020\n",
            "[495,    49] loss: 0.025\n",
            "[495,    50] loss: 0.030\n",
            "[495,    51] loss: 0.030\n",
            "[495,    52] loss: 0.023\n",
            "[495,    53] loss: 0.030\n",
            "[495,    54] loss: 0.022\n",
            "[495,    55] loss: 0.019\n",
            "[495,    56] loss: 0.022\n",
            "[495,    57] loss: 0.023\n",
            "[495,    58] loss: 0.027\n",
            "[495,    59] loss: 0.025\n",
            "[495,    60] loss: 0.027\n",
            "[495,    61] loss: 0.023\n",
            "[495,    62] loss: 0.025\n",
            "[495,    63] loss: 0.024\n",
            "[495,    64] loss: 0.024\n",
            "[495,    65] loss: 0.023\n",
            "[495,    66] loss: 0.029\n",
            "[495,    67] loss: 0.023\n",
            "[495,    68] loss: 0.025\n",
            "[495,    69] loss: 0.027\n",
            "[495,    70] loss: 0.022\n",
            "[495,    71] loss: 0.031\n",
            "[495,    72] loss: 0.023\n",
            "[495,    73] loss: 0.025\n",
            "[495,    74] loss: 0.025\n",
            "[495,    75] loss: 0.023\n",
            "[495,    76] loss: 0.021\n",
            "[495,    77] loss: 0.028\n",
            "[495,    78] loss: 0.023\n",
            "[495,    79] loss: 0.026\n",
            "[495,    80] loss: 0.023\n",
            "[495,    81] loss: 0.034\n",
            "[495,    82] loss: 0.023\n",
            "[495,    83] loss: 0.023\n",
            "[495,    84] loss: 0.024\n",
            "[495,    85] loss: 0.027\n",
            "[495,    86] loss: 0.029\n",
            "[495,    87] loss: 0.028\n",
            "[495,    88] loss: 0.024\n",
            "[495,    89] loss: 0.027\n",
            "[495,    90] loss: 0.025\n",
            "[495,    91] loss: 0.025\n",
            "[495,    92] loss: 0.029\n",
            "[495,    93] loss: 0.016\n",
            "[495,    94] loss: 0.028\n",
            "[495,    95] loss: 0.024\n",
            "[495,    96] loss: 0.033\n",
            "[495,    97] loss: 0.023\n",
            "[495,    98] loss: 0.023\n",
            "[495,    99] loss: 0.020\n",
            "[495,   100] loss: 0.027\n",
            "[495,   101] loss: 0.024\n",
            "[495,   102] loss: 0.027\n",
            "[495,   103] loss: 0.025\n",
            "[495,   104] loss: 0.017\n",
            "[495,   105] loss: 0.027\n",
            "[495,   106] loss: 0.020\n",
            "[495,   107] loss: 0.029\n",
            "[495,   108] loss: 0.023\n",
            "[495,   109] loss: 0.020\n",
            "[495,   110] loss: 0.025\n",
            "[495,   111] loss: 0.027\n",
            "[495,   112] loss: 0.025\n",
            "[495,   113] loss: 0.023\n",
            "[495,   114] loss: 0.030\n",
            "[495,   115] loss: 0.022\n",
            "[495,   116] loss: 0.021\n",
            "[495,   117] loss: 0.034\n",
            "[495,   118] loss: 0.027\n",
            "[495,   119] loss: 0.027\n",
            "[495,   120] loss: 0.029\n",
            "[495,   121] loss: 0.026\n",
            "[495,   122] loss: 0.025\n",
            "[495,   123] loss: 0.026\n",
            "[495,   124] loss: 0.026\n",
            "[495,   125] loss: 0.025\n",
            "[495,   126] loss: 0.025\n",
            "[495,   127] loss: 0.031\n",
            "[495,   128] loss: 0.027\n",
            "[495,   129] loss: 0.027\n",
            "[495,   130] loss: 0.026\n",
            "[495,   131] loss: 0.027\n",
            "[495,   132] loss: 0.029\n",
            "[495,   133] loss: 0.026\n",
            "[495,   134] loss: 0.022\n",
            "[495,   135] loss: 0.023\n",
            "[495,   136] loss: 0.020\n",
            "[495,   137] loss: 0.023\n",
            "[495,   138] loss: 0.022\n",
            "[495,   139] loss: 0.026\n",
            "[495,   140] loss: 0.023\n",
            "[495,   141] loss: 0.023\n",
            "[495,   142] loss: 0.021\n",
            "[495,   143] loss: 0.028\n",
            "[495,   144] loss: 0.026\n",
            "[495,   145] loss: 0.027\n",
            "[495,   146] loss: 0.027\n",
            "[495,   147] loss: 0.018\n",
            "[496,     1] loss: 0.024\n",
            "[496,     2] loss: 0.025\n",
            "[496,     3] loss: 0.027\n",
            "[496,     4] loss: 0.028\n",
            "[496,     5] loss: 0.026\n",
            "[496,     6] loss: 0.022\n",
            "[496,     7] loss: 0.021\n",
            "[496,     8] loss: 0.026\n",
            "[496,     9] loss: 0.023\n",
            "[496,    10] loss: 0.025\n",
            "[496,    11] loss: 0.022\n",
            "[496,    12] loss: 0.025\n",
            "[496,    13] loss: 0.027\n",
            "[496,    14] loss: 0.030\n",
            "[496,    15] loss: 0.018\n",
            "[496,    16] loss: 0.022\n",
            "[496,    17] loss: 0.025\n",
            "[496,    18] loss: 0.030\n",
            "[496,    19] loss: 0.026\n",
            "[496,    20] loss: 0.023\n",
            "[496,    21] loss: 0.024\n",
            "[496,    22] loss: 0.027\n",
            "[496,    23] loss: 0.029\n",
            "[496,    24] loss: 0.024\n",
            "[496,    25] loss: 0.020\n",
            "[496,    26] loss: 0.023\n",
            "[496,    27] loss: 0.025\n",
            "[496,    28] loss: 0.022\n",
            "[496,    29] loss: 0.030\n",
            "[496,    30] loss: 0.032\n",
            "[496,    31] loss: 0.030\n",
            "[496,    32] loss: 0.027\n",
            "[496,    33] loss: 0.022\n",
            "[496,    34] loss: 0.027\n",
            "[496,    35] loss: 0.030\n",
            "[496,    36] loss: 0.022\n",
            "[496,    37] loss: 0.027\n",
            "[496,    38] loss: 0.026\n",
            "[496,    39] loss: 0.027\n",
            "[496,    40] loss: 0.022\n",
            "[496,    41] loss: 0.024\n",
            "[496,    42] loss: 0.023\n",
            "[496,    43] loss: 0.020\n",
            "[496,    44] loss: 0.025\n",
            "[496,    45] loss: 0.024\n",
            "[496,    46] loss: 0.025\n",
            "[496,    47] loss: 0.026\n",
            "[496,    48] loss: 0.031\n",
            "[496,    49] loss: 0.030\n",
            "[496,    50] loss: 0.026\n",
            "[496,    51] loss: 0.024\n",
            "[496,    52] loss: 0.027\n",
            "[496,    53] loss: 0.024\n",
            "[496,    54] loss: 0.025\n",
            "[496,    55] loss: 0.029\n",
            "[496,    56] loss: 0.021\n",
            "[496,    57] loss: 0.025\n",
            "[496,    58] loss: 0.020\n",
            "[496,    59] loss: 0.028\n",
            "[496,    60] loss: 0.025\n",
            "[496,    61] loss: 0.027\n",
            "[496,    62] loss: 0.026\n",
            "[496,    63] loss: 0.021\n",
            "[496,    64] loss: 0.025\n",
            "[496,    65] loss: 0.021\n",
            "[496,    66] loss: 0.025\n",
            "[496,    67] loss: 0.023\n",
            "[496,    68] loss: 0.023\n",
            "[496,    69] loss: 0.023\n",
            "[496,    70] loss: 0.024\n",
            "[496,    71] loss: 0.029\n",
            "[496,    72] loss: 0.024\n",
            "[496,    73] loss: 0.022\n",
            "[496,    74] loss: 0.021\n",
            "[496,    75] loss: 0.018\n",
            "[496,    76] loss: 0.031\n",
            "[496,    77] loss: 0.022\n",
            "[496,    78] loss: 0.022\n",
            "[496,    79] loss: 0.021\n",
            "[496,    80] loss: 0.025\n",
            "[496,    81] loss: 0.023\n",
            "[496,    82] loss: 0.020\n",
            "[496,    83] loss: 0.027\n",
            "[496,    84] loss: 0.030\n",
            "[496,    85] loss: 0.027\n",
            "[496,    86] loss: 0.023\n",
            "[496,    87] loss: 0.023\n",
            "[496,    88] loss: 0.029\n",
            "[496,    89] loss: 0.025\n",
            "[496,    90] loss: 0.018\n",
            "[496,    91] loss: 0.029\n",
            "[496,    92] loss: 0.020\n",
            "[496,    93] loss: 0.030\n",
            "[496,    94] loss: 0.029\n",
            "[496,    95] loss: 0.023\n",
            "[496,    96] loss: 0.027\n",
            "[496,    97] loss: 0.031\n",
            "[496,    98] loss: 0.029\n",
            "[496,    99] loss: 0.023\n",
            "[496,   100] loss: 0.025\n",
            "[496,   101] loss: 0.032\n",
            "[496,   102] loss: 0.021\n",
            "[496,   103] loss: 0.026\n",
            "[496,   104] loss: 0.026\n",
            "[496,   105] loss: 0.030\n",
            "[496,   106] loss: 0.028\n",
            "[496,   107] loss: 0.023\n",
            "[496,   108] loss: 0.022\n",
            "[496,   109] loss: 0.030\n",
            "[496,   110] loss: 0.023\n",
            "[496,   111] loss: 0.026\n",
            "[496,   112] loss: 0.029\n",
            "[496,   113] loss: 0.025\n",
            "[496,   114] loss: 0.023\n",
            "[496,   115] loss: 0.026\n",
            "[496,   116] loss: 0.024\n",
            "[496,   117] loss: 0.021\n",
            "[496,   118] loss: 0.027\n",
            "[496,   119] loss: 0.022\n",
            "[496,   120] loss: 0.030\n",
            "[496,   121] loss: 0.023\n",
            "[496,   122] loss: 0.023\n",
            "[496,   123] loss: 0.030\n",
            "[496,   124] loss: 0.027\n",
            "[496,   125] loss: 0.027\n",
            "[496,   126] loss: 0.023\n",
            "[496,   127] loss: 0.020\n",
            "[496,   128] loss: 0.026\n",
            "[496,   129] loss: 0.024\n",
            "[496,   130] loss: 0.028\n",
            "[496,   131] loss: 0.025\n",
            "[496,   132] loss: 0.031\n",
            "[496,   133] loss: 0.027\n",
            "[496,   134] loss: 0.019\n",
            "[496,   135] loss: 0.030\n",
            "[496,   136] loss: 0.025\n",
            "[496,   137] loss: 0.027\n",
            "[496,   138] loss: 0.028\n",
            "[496,   139] loss: 0.024\n",
            "[496,   140] loss: 0.029\n",
            "[496,   141] loss: 0.026\n",
            "[496,   142] loss: 0.027\n",
            "[496,   143] loss: 0.026\n",
            "[496,   144] loss: 0.026\n",
            "[496,   145] loss: 0.023\n",
            "[496,   146] loss: 0.026\n",
            "[496,   147] loss: 0.021\n",
            "[497,     1] loss: 0.028\n",
            "[497,     2] loss: 0.023\n",
            "[497,     3] loss: 0.030\n",
            "[497,     4] loss: 0.024\n",
            "[497,     5] loss: 0.029\n",
            "[497,     6] loss: 0.024\n",
            "[497,     7] loss: 0.027\n",
            "[497,     8] loss: 0.026\n",
            "[497,     9] loss: 0.026\n",
            "[497,    10] loss: 0.030\n",
            "[497,    11] loss: 0.029\n",
            "[497,    12] loss: 0.032\n",
            "[497,    13] loss: 0.025\n",
            "[497,    14] loss: 0.028\n",
            "[497,    15] loss: 0.023\n",
            "[497,    16] loss: 0.024\n",
            "[497,    17] loss: 0.030\n",
            "[497,    18] loss: 0.020\n",
            "[497,    19] loss: 0.027\n",
            "[497,    20] loss: 0.016\n",
            "[497,    21] loss: 0.024\n",
            "[497,    22] loss: 0.029\n",
            "[497,    23] loss: 0.022\n",
            "[497,    24] loss: 0.027\n",
            "[497,    25] loss: 0.027\n",
            "[497,    26] loss: 0.029\n",
            "[497,    27] loss: 0.023\n",
            "[497,    28] loss: 0.025\n",
            "[497,    29] loss: 0.028\n",
            "[497,    30] loss: 0.025\n",
            "[497,    31] loss: 0.027\n",
            "[497,    32] loss: 0.024\n",
            "[497,    33] loss: 0.025\n",
            "[497,    34] loss: 0.025\n",
            "[497,    35] loss: 0.020\n",
            "[497,    36] loss: 0.023\n",
            "[497,    37] loss: 0.021\n",
            "[497,    38] loss: 0.027\n",
            "[497,    39] loss: 0.027\n",
            "[497,    40] loss: 0.020\n",
            "[497,    41] loss: 0.027\n",
            "[497,    42] loss: 0.026\n",
            "[497,    43] loss: 0.025\n",
            "[497,    44] loss: 0.020\n",
            "[497,    45] loss: 0.025\n",
            "[497,    46] loss: 0.028\n",
            "[497,    47] loss: 0.023\n",
            "[497,    48] loss: 0.027\n",
            "[497,    49] loss: 0.030\n",
            "[497,    50] loss: 0.020\n",
            "[497,    51] loss: 0.026\n",
            "[497,    52] loss: 0.027\n",
            "[497,    53] loss: 0.022\n",
            "[497,    54] loss: 0.023\n",
            "[497,    55] loss: 0.029\n",
            "[497,    56] loss: 0.026\n",
            "[497,    57] loss: 0.031\n",
            "[497,    58] loss: 0.022\n",
            "[497,    59] loss: 0.026\n",
            "[497,    60] loss: 0.022\n",
            "[497,    61] loss: 0.028\n",
            "[497,    62] loss: 0.026\n",
            "[497,    63] loss: 0.022\n",
            "[497,    64] loss: 0.024\n",
            "[497,    65] loss: 0.023\n",
            "[497,    66] loss: 0.023\n",
            "[497,    67] loss: 0.020\n",
            "[497,    68] loss: 0.024\n",
            "[497,    69] loss: 0.034\n",
            "[497,    70] loss: 0.027\n",
            "[497,    71] loss: 0.030\n",
            "[497,    72] loss: 0.021\n",
            "[497,    73] loss: 0.020\n",
            "[497,    74] loss: 0.023\n",
            "[497,    75] loss: 0.021\n",
            "[497,    76] loss: 0.026\n",
            "[497,    77] loss: 0.023\n",
            "[497,    78] loss: 0.029\n",
            "[497,    79] loss: 0.022\n",
            "[497,    80] loss: 0.025\n",
            "[497,    81] loss: 0.026\n",
            "[497,    82] loss: 0.029\n",
            "[497,    83] loss: 0.032\n",
            "[497,    84] loss: 0.030\n",
            "[497,    85] loss: 0.024\n",
            "[497,    86] loss: 0.028\n",
            "[497,    87] loss: 0.021\n",
            "[497,    88] loss: 0.026\n",
            "[497,    89] loss: 0.024\n",
            "[497,    90] loss: 0.032\n",
            "[497,    91] loss: 0.023\n",
            "[497,    92] loss: 0.020\n",
            "[497,    93] loss: 0.023\n",
            "[497,    94] loss: 0.027\n",
            "[497,    95] loss: 0.030\n",
            "[497,    96] loss: 0.029\n",
            "[497,    97] loss: 0.023\n",
            "[497,    98] loss: 0.023\n",
            "[497,    99] loss: 0.024\n",
            "[497,   100] loss: 0.028\n",
            "[497,   101] loss: 0.020\n",
            "[497,   102] loss: 0.019\n",
            "[497,   103] loss: 0.023\n",
            "[497,   104] loss: 0.026\n",
            "[497,   105] loss: 0.018\n",
            "[497,   106] loss: 0.027\n",
            "[497,   107] loss: 0.031\n",
            "[497,   108] loss: 0.032\n",
            "[497,   109] loss: 0.023\n",
            "[497,   110] loss: 0.026\n",
            "[497,   111] loss: 0.020\n",
            "[497,   112] loss: 0.019\n",
            "[497,   113] loss: 0.024\n",
            "[497,   114] loss: 0.022\n",
            "[497,   115] loss: 0.022\n",
            "[497,   116] loss: 0.023\n",
            "[497,   117] loss: 0.027\n",
            "[497,   118] loss: 0.031\n",
            "[497,   119] loss: 0.027\n",
            "[497,   120] loss: 0.023\n",
            "[497,   121] loss: 0.025\n",
            "[497,   122] loss: 0.021\n",
            "[497,   123] loss: 0.016\n",
            "[497,   124] loss: 0.031\n",
            "[497,   125] loss: 0.027\n",
            "[497,   126] loss: 0.024\n",
            "[497,   127] loss: 0.020\n",
            "[497,   128] loss: 0.023\n",
            "[497,   129] loss: 0.021\n",
            "[497,   130] loss: 0.023\n",
            "[497,   131] loss: 0.027\n",
            "[497,   132] loss: 0.026\n",
            "[497,   133] loss: 0.029\n",
            "[497,   134] loss: 0.028\n",
            "[497,   135] loss: 0.027\n",
            "[497,   136] loss: 0.026\n",
            "[497,   137] loss: 0.027\n",
            "[497,   138] loss: 0.024\n",
            "[497,   139] loss: 0.023\n",
            "[497,   140] loss: 0.027\n",
            "[497,   141] loss: 0.020\n",
            "[497,   142] loss: 0.032\n",
            "[497,   143] loss: 0.029\n",
            "[497,   144] loss: 0.027\n",
            "[497,   145] loss: 0.025\n",
            "[497,   146] loss: 0.026\n",
            "[497,   147] loss: 0.029\n",
            "[498,     1] loss: 0.027\n",
            "[498,     2] loss: 0.023\n",
            "[498,     3] loss: 0.028\n",
            "[498,     4] loss: 0.027\n",
            "[498,     5] loss: 0.023\n",
            "[498,     6] loss: 0.017\n",
            "[498,     7] loss: 0.023\n",
            "[498,     8] loss: 0.020\n",
            "[498,     9] loss: 0.023\n",
            "[498,    10] loss: 0.024\n",
            "[498,    11] loss: 0.027\n",
            "[498,    12] loss: 0.023\n",
            "[498,    13] loss: 0.024\n",
            "[498,    14] loss: 0.028\n",
            "[498,    15] loss: 0.023\n",
            "[498,    16] loss: 0.022\n",
            "[498,    17] loss: 0.028\n",
            "[498,    18] loss: 0.021\n",
            "[498,    19] loss: 0.023\n",
            "[498,    20] loss: 0.028\n",
            "[498,    21] loss: 0.020\n",
            "[498,    22] loss: 0.031\n",
            "[498,    23] loss: 0.030\n",
            "[498,    24] loss: 0.027\n",
            "[498,    25] loss: 0.028\n",
            "[498,    26] loss: 0.027\n",
            "[498,    27] loss: 0.025\n",
            "[498,    28] loss: 0.027\n",
            "[498,    29] loss: 0.028\n",
            "[498,    30] loss: 0.020\n",
            "[498,    31] loss: 0.027\n",
            "[498,    32] loss: 0.023\n",
            "[498,    33] loss: 0.023\n",
            "[498,    34] loss: 0.027\n",
            "[498,    35] loss: 0.023\n",
            "[498,    36] loss: 0.029\n",
            "[498,    37] loss: 0.027\n",
            "[498,    38] loss: 0.021\n",
            "[498,    39] loss: 0.027\n",
            "[498,    40] loss: 0.024\n",
            "[498,    41] loss: 0.027\n",
            "[498,    42] loss: 0.023\n",
            "[498,    43] loss: 0.030\n",
            "[498,    44] loss: 0.028\n",
            "[498,    45] loss: 0.023\n",
            "[498,    46] loss: 0.029\n",
            "[498,    47] loss: 0.023\n",
            "[498,    48] loss: 0.028\n",
            "[498,    49] loss: 0.030\n",
            "[498,    50] loss: 0.028\n",
            "[498,    51] loss: 0.026\n",
            "[498,    52] loss: 0.030\n",
            "[498,    53] loss: 0.023\n",
            "[498,    54] loss: 0.028\n",
            "[498,    55] loss: 0.023\n",
            "[498,    56] loss: 0.027\n",
            "[498,    57] loss: 0.025\n",
            "[498,    58] loss: 0.033\n",
            "[498,    59] loss: 0.024\n",
            "[498,    60] loss: 0.017\n",
            "[498,    61] loss: 0.022\n",
            "[498,    62] loss: 0.020\n",
            "[498,    63] loss: 0.028\n",
            "[498,    64] loss: 0.022\n",
            "[498,    65] loss: 0.026\n",
            "[498,    66] loss: 0.027\n",
            "[498,    67] loss: 0.028\n",
            "[498,    68] loss: 0.024\n",
            "[498,    69] loss: 0.027\n",
            "[498,    70] loss: 0.024\n",
            "[498,    71] loss: 0.025\n",
            "[498,    72] loss: 0.022\n",
            "[498,    73] loss: 0.019\n",
            "[498,    74] loss: 0.025\n",
            "[498,    75] loss: 0.030\n",
            "[498,    76] loss: 0.028\n",
            "[498,    77] loss: 0.026\n",
            "[498,    78] loss: 0.028\n",
            "[498,    79] loss: 0.027\n",
            "[498,    80] loss: 0.024\n",
            "[498,    81] loss: 0.027\n",
            "[498,    82] loss: 0.021\n",
            "[498,    83] loss: 0.027\n",
            "[498,    84] loss: 0.030\n",
            "[498,    85] loss: 0.020\n",
            "[498,    86] loss: 0.029\n",
            "[498,    87] loss: 0.025\n",
            "[498,    88] loss: 0.023\n",
            "[498,    89] loss: 0.025\n",
            "[498,    90] loss: 0.018\n",
            "[498,    91] loss: 0.033\n",
            "[498,    92] loss: 0.029\n",
            "[498,    93] loss: 0.023\n",
            "[498,    94] loss: 0.026\n",
            "[498,    95] loss: 0.028\n",
            "[498,    96] loss: 0.023\n",
            "[498,    97] loss: 0.023\n",
            "[498,    98] loss: 0.019\n",
            "[498,    99] loss: 0.024\n",
            "[498,   100] loss: 0.023\n",
            "[498,   101] loss: 0.027\n",
            "[498,   102] loss: 0.026\n",
            "[498,   103] loss: 0.027\n",
            "[498,   104] loss: 0.026\n",
            "[498,   105] loss: 0.024\n",
            "[498,   106] loss: 0.028\n",
            "[498,   107] loss: 0.033\n",
            "[498,   108] loss: 0.024\n",
            "[498,   109] loss: 0.026\n",
            "[498,   110] loss: 0.033\n",
            "[498,   111] loss: 0.021\n",
            "[498,   112] loss: 0.026\n",
            "[498,   113] loss: 0.028\n",
            "[498,   114] loss: 0.023\n",
            "[498,   115] loss: 0.021\n",
            "[498,   116] loss: 0.024\n",
            "[498,   117] loss: 0.021\n",
            "[498,   118] loss: 0.024\n",
            "[498,   119] loss: 0.029\n",
            "[498,   120] loss: 0.028\n",
            "[498,   121] loss: 0.025\n",
            "[498,   122] loss: 0.017\n",
            "[498,   123] loss: 0.022\n",
            "[498,   124] loss: 0.027\n",
            "[498,   125] loss: 0.027\n",
            "[498,   126] loss: 0.026\n",
            "[498,   127] loss: 0.030\n",
            "[498,   128] loss: 0.024\n",
            "[498,   129] loss: 0.024\n",
            "[498,   130] loss: 0.021\n",
            "[498,   131] loss: 0.024\n",
            "[498,   132] loss: 0.030\n",
            "[498,   133] loss: 0.020\n",
            "[498,   134] loss: 0.023\n",
            "[498,   135] loss: 0.030\n",
            "[498,   136] loss: 0.021\n",
            "[498,   137] loss: 0.027\n",
            "[498,   138] loss: 0.020\n",
            "[498,   139] loss: 0.025\n",
            "[498,   140] loss: 0.025\n",
            "[498,   141] loss: 0.018\n",
            "[498,   142] loss: 0.030\n",
            "[498,   143] loss: 0.022\n",
            "[498,   144] loss: 0.025\n",
            "[498,   145] loss: 0.027\n",
            "[498,   146] loss: 0.024\n",
            "[498,   147] loss: 0.026\n",
            "[499,     1] loss: 0.029\n",
            "[499,     2] loss: 0.027\n",
            "[499,     3] loss: 0.023\n",
            "[499,     4] loss: 0.022\n",
            "[499,     5] loss: 0.025\n",
            "[499,     6] loss: 0.025\n",
            "[499,     7] loss: 0.027\n",
            "[499,     8] loss: 0.024\n",
            "[499,     9] loss: 0.021\n",
            "[499,    10] loss: 0.024\n",
            "[499,    11] loss: 0.022\n",
            "[499,    12] loss: 0.022\n",
            "[499,    13] loss: 0.026\n",
            "[499,    14] loss: 0.021\n",
            "[499,    15] loss: 0.028\n",
            "[499,    16] loss: 0.027\n",
            "[499,    17] loss: 0.027\n",
            "[499,    18] loss: 0.026\n",
            "[499,    19] loss: 0.023\n",
            "[499,    20] loss: 0.027\n",
            "[499,    21] loss: 0.020\n",
            "[499,    22] loss: 0.023\n",
            "[499,    23] loss: 0.023\n",
            "[499,    24] loss: 0.019\n",
            "[499,    25] loss: 0.025\n",
            "[499,    26] loss: 0.026\n",
            "[499,    27] loss: 0.027\n",
            "[499,    28] loss: 0.027\n",
            "[499,    29] loss: 0.027\n",
            "[499,    30] loss: 0.026\n",
            "[499,    31] loss: 0.025\n",
            "[499,    32] loss: 0.022\n",
            "[499,    33] loss: 0.025\n",
            "[499,    34] loss: 0.030\n",
            "[499,    35] loss: 0.033\n",
            "[499,    36] loss: 0.022\n",
            "[499,    37] loss: 0.028\n",
            "[499,    38] loss: 0.025\n",
            "[499,    39] loss: 0.022\n",
            "[499,    40] loss: 0.023\n",
            "[499,    41] loss: 0.022\n",
            "[499,    42] loss: 0.025\n",
            "[499,    43] loss: 0.023\n",
            "[499,    44] loss: 0.024\n",
            "[499,    45] loss: 0.030\n",
            "[499,    46] loss: 0.025\n",
            "[499,    47] loss: 0.025\n",
            "[499,    48] loss: 0.021\n",
            "[499,    49] loss: 0.020\n",
            "[499,    50] loss: 0.020\n",
            "[499,    51] loss: 0.025\n",
            "[499,    52] loss: 0.028\n",
            "[499,    53] loss: 0.024\n",
            "[499,    54] loss: 0.024\n",
            "[499,    55] loss: 0.027\n",
            "[499,    56] loss: 0.023\n",
            "[499,    57] loss: 0.027\n",
            "[499,    58] loss: 0.029\n",
            "[499,    59] loss: 0.020\n",
            "[499,    60] loss: 0.022\n",
            "[499,    61] loss: 0.024\n",
            "[499,    62] loss: 0.024\n",
            "[499,    63] loss: 0.025\n",
            "[499,    64] loss: 0.030\n",
            "[499,    65] loss: 0.020\n",
            "[499,    66] loss: 0.028\n",
            "[499,    67] loss: 0.029\n",
            "[499,    68] loss: 0.027\n",
            "[499,    69] loss: 0.027\n",
            "[499,    70] loss: 0.027\n",
            "[499,    71] loss: 0.027\n",
            "[499,    72] loss: 0.027\n",
            "[499,    73] loss: 0.028\n",
            "[499,    74] loss: 0.020\n",
            "[499,    75] loss: 0.023\n",
            "[499,    76] loss: 0.027\n",
            "[499,    77] loss: 0.024\n",
            "[499,    78] loss: 0.025\n",
            "[499,    79] loss: 0.029\n",
            "[499,    80] loss: 0.027\n",
            "[499,    81] loss: 0.017\n",
            "[499,    82] loss: 0.022\n",
            "[499,    83] loss: 0.027\n",
            "[499,    84] loss: 0.030\n",
            "[499,    85] loss: 0.027\n",
            "[499,    86] loss: 0.023\n",
            "[499,    87] loss: 0.030\n",
            "[499,    88] loss: 0.020\n",
            "[499,    89] loss: 0.024\n",
            "[499,    90] loss: 0.025\n",
            "[499,    91] loss: 0.026\n",
            "[499,    92] loss: 0.027\n",
            "[499,    93] loss: 0.028\n",
            "[499,    94] loss: 0.023\n",
            "[499,    95] loss: 0.027\n",
            "[499,    96] loss: 0.023\n",
            "[499,    97] loss: 0.025\n",
            "[499,    98] loss: 0.027\n",
            "[499,    99] loss: 0.027\n",
            "[499,   100] loss: 0.029\n",
            "[499,   101] loss: 0.027\n",
            "[499,   102] loss: 0.023\n",
            "[499,   103] loss: 0.028\n",
            "[499,   104] loss: 0.021\n",
            "[499,   105] loss: 0.026\n",
            "[499,   106] loss: 0.023\n",
            "[499,   107] loss: 0.028\n",
            "[499,   108] loss: 0.027\n",
            "[499,   109] loss: 0.030\n",
            "[499,   110] loss: 0.024\n",
            "[499,   111] loss: 0.031\n",
            "[499,   112] loss: 0.024\n",
            "[499,   113] loss: 0.023\n",
            "[499,   114] loss: 0.026\n",
            "[499,   115] loss: 0.027\n",
            "[499,   116] loss: 0.020\n",
            "[499,   117] loss: 0.020\n",
            "[499,   118] loss: 0.025\n",
            "[499,   119] loss: 0.027\n",
            "[499,   120] loss: 0.029\n",
            "[499,   121] loss: 0.021\n",
            "[499,   122] loss: 0.027\n",
            "[499,   123] loss: 0.025\n",
            "[499,   124] loss: 0.023\n",
            "[499,   125] loss: 0.021\n",
            "[499,   126] loss: 0.027\n",
            "[499,   127] loss: 0.027\n",
            "[499,   128] loss: 0.029\n",
            "[499,   129] loss: 0.025\n",
            "[499,   130] loss: 0.027\n",
            "[499,   131] loss: 0.027\n",
            "[499,   132] loss: 0.024\n",
            "[499,   133] loss: 0.028\n",
            "[499,   134] loss: 0.027\n",
            "[499,   135] loss: 0.022\n",
            "[499,   136] loss: 0.020\n",
            "[499,   137] loss: 0.023\n",
            "[499,   138] loss: 0.030\n",
            "[499,   139] loss: 0.025\n",
            "[499,   140] loss: 0.028\n",
            "[499,   141] loss: 0.023\n",
            "[499,   142] loss: 0.025\n",
            "[499,   143] loss: 0.030\n",
            "[499,   144] loss: 0.025\n",
            "[499,   145] loss: 0.024\n",
            "[499,   146] loss: 0.030\n",
            "[499,   147] loss: 0.021\n",
            "[500,     1] loss: 0.027\n",
            "[500,     2] loss: 0.030\n",
            "[500,     3] loss: 0.023\n",
            "[500,     4] loss: 0.028\n",
            "[500,     5] loss: 0.030\n",
            "[500,     6] loss: 0.026\n",
            "[500,     7] loss: 0.031\n",
            "[500,     8] loss: 0.025\n",
            "[500,     9] loss: 0.020\n",
            "[500,    10] loss: 0.025\n",
            "[500,    11] loss: 0.024\n",
            "[500,    12] loss: 0.022\n",
            "[500,    13] loss: 0.023\n",
            "[500,    14] loss: 0.029\n",
            "[500,    15] loss: 0.025\n",
            "[500,    16] loss: 0.026\n",
            "[500,    17] loss: 0.024\n",
            "[500,    18] loss: 0.024\n",
            "[500,    19] loss: 0.026\n",
            "[500,    20] loss: 0.022\n",
            "[500,    21] loss: 0.023\n",
            "[500,    22] loss: 0.027\n",
            "[500,    23] loss: 0.021\n",
            "[500,    24] loss: 0.024\n",
            "[500,    25] loss: 0.020\n",
            "[500,    26] loss: 0.029\n",
            "[500,    27] loss: 0.029\n",
            "[500,    28] loss: 0.030\n",
            "[500,    29] loss: 0.027\n",
            "[500,    30] loss: 0.033\n",
            "[500,    31] loss: 0.030\n",
            "[500,    32] loss: 0.021\n",
            "[500,    33] loss: 0.029\n",
            "[500,    34] loss: 0.028\n",
            "[500,    35] loss: 0.027\n",
            "[500,    36] loss: 0.025\n",
            "[500,    37] loss: 0.020\n",
            "[500,    38] loss: 0.024\n",
            "[500,    39] loss: 0.023\n",
            "[500,    40] loss: 0.027\n",
            "[500,    41] loss: 0.023\n",
            "[500,    42] loss: 0.023\n",
            "[500,    43] loss: 0.023\n",
            "[500,    44] loss: 0.023\n",
            "[500,    45] loss: 0.025\n",
            "[500,    46] loss: 0.024\n",
            "[500,    47] loss: 0.024\n",
            "[500,    48] loss: 0.024\n",
            "[500,    49] loss: 0.027\n",
            "[500,    50] loss: 0.027\n",
            "[500,    51] loss: 0.030\n",
            "[500,    52] loss: 0.025\n",
            "[500,    53] loss: 0.023\n",
            "[500,    54] loss: 0.027\n",
            "[500,    55] loss: 0.023\n",
            "[500,    56] loss: 0.026\n",
            "[500,    57] loss: 0.028\n",
            "[500,    58] loss: 0.023\n",
            "[500,    59] loss: 0.025\n",
            "[500,    60] loss: 0.028\n",
            "[500,    61] loss: 0.023\n",
            "[500,    62] loss: 0.028\n",
            "[500,    63] loss: 0.020\n",
            "[500,    64] loss: 0.018\n",
            "[500,    65] loss: 0.025\n",
            "[500,    66] loss: 0.027\n",
            "[500,    67] loss: 0.028\n",
            "[500,    68] loss: 0.024\n",
            "[500,    69] loss: 0.022\n",
            "[500,    70] loss: 0.030\n",
            "[500,    71] loss: 0.027\n",
            "[500,    72] loss: 0.024\n",
            "[500,    73] loss: 0.027\n",
            "[500,    74] loss: 0.023\n",
            "[500,    75] loss: 0.026\n",
            "[500,    76] loss: 0.023\n",
            "[500,    77] loss: 0.031\n",
            "[500,    78] loss: 0.025\n",
            "[500,    79] loss: 0.019\n",
            "[500,    80] loss: 0.030\n",
            "[500,    81] loss: 0.023\n",
            "[500,    82] loss: 0.030\n",
            "[500,    83] loss: 0.020\n",
            "[500,    84] loss: 0.026\n",
            "[500,    85] loss: 0.034\n",
            "[500,    86] loss: 0.026\n",
            "[500,    87] loss: 0.024\n",
            "[500,    88] loss: 0.022\n",
            "[500,    89] loss: 0.020\n",
            "[500,    90] loss: 0.027\n",
            "[500,    91] loss: 0.028\n",
            "[500,    92] loss: 0.030\n",
            "[500,    93] loss: 0.025\n",
            "[500,    94] loss: 0.022\n",
            "[500,    95] loss: 0.029\n",
            "[500,    96] loss: 0.029\n",
            "[500,    97] loss: 0.027\n",
            "[500,    98] loss: 0.023\n",
            "[500,    99] loss: 0.022\n",
            "[500,   100] loss: 0.026\n",
            "[500,   101] loss: 0.022\n",
            "[500,   102] loss: 0.023\n",
            "[500,   103] loss: 0.023\n",
            "[500,   104] loss: 0.027\n",
            "[500,   105] loss: 0.023\n",
            "[500,   106] loss: 0.022\n",
            "[500,   107] loss: 0.019\n",
            "[500,   108] loss: 0.017\n",
            "[500,   109] loss: 0.031\n",
            "[500,   110] loss: 0.027\n",
            "[500,   111] loss: 0.020\n",
            "[500,   112] loss: 0.024\n",
            "[500,   113] loss: 0.029\n",
            "[500,   114] loss: 0.027\n",
            "[500,   115] loss: 0.026\n",
            "[500,   116] loss: 0.025\n",
            "[500,   117] loss: 0.026\n",
            "[500,   118] loss: 0.030\n",
            "[500,   119] loss: 0.023\n",
            "[500,   120] loss: 0.028\n",
            "[500,   121] loss: 0.023\n",
            "[500,   122] loss: 0.025\n",
            "[500,   123] loss: 0.023\n",
            "[500,   124] loss: 0.020\n",
            "[500,   125] loss: 0.023\n",
            "[500,   126] loss: 0.025\n",
            "[500,   127] loss: 0.024\n",
            "[500,   128] loss: 0.027\n",
            "[500,   129] loss: 0.026\n",
            "[500,   130] loss: 0.023\n",
            "[500,   131] loss: 0.027\n",
            "[500,   132] loss: 0.023\n",
            "[500,   133] loss: 0.023\n",
            "[500,   134] loss: 0.027\n",
            "[500,   135] loss: 0.023\n",
            "[500,   136] loss: 0.025\n",
            "[500,   137] loss: 0.019\n",
            "[500,   138] loss: 0.022\n",
            "[500,   139] loss: 0.023\n",
            "[500,   140] loss: 0.027\n",
            "[500,   141] loss: 0.032\n",
            "[500,   142] loss: 0.030\n",
            "[500,   143] loss: 0.020\n",
            "[500,   144] loss: 0.025\n",
            "[500,   145] loss: 0.030\n",
            "[500,   146] loss: 0.026\n",
            "[500,   147] loss: 0.026\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = NeuralNetwork_Model1(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYZLHvuiWAsC",
        "outputId": "16e80b07-14d3-4b50-e036-a31e27622cee"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 50 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Not the best accuracy I know :( first time using pytorch"
      ],
      "metadata": {
        "id": "Oy1CzU_cksYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keras"
      ],
      "metadata": {
        "id": "y0B7-pmnQ3E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "ea-Ai8_6U4Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "CKGr-NpPX83U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateModel(hidden_layer1, hidden_layer2):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(hidden_layer1,activation='relu'))\n",
        "  model.add(Dense(hidden_layer2, activation='relu'))\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "  return model\n",
        "        "
      ],
      "metadata": {
        "id": "h7_ZCzHKV36Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Neural_Network1 = CreateModel(16,32)\n",
        "Neural_Network1.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
        "Neural_Network1.fit(X_train,y_train_encoded,epochs=300,batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKraoNIBVdvh",
        "outputId": "451ecd13-fb45-4ec4-b26c-68738bec7d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "937/937 [==============================] - 3s 2ms/step - loss: 0.7806 - accuracy: 0.6618\n",
            "Epoch 2/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.5744 - accuracy: 0.7197\n",
            "Epoch 3/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.5397 - accuracy: 0.7361\n",
            "Epoch 4/300\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.5086 - accuracy: 0.7515\n",
            "Epoch 5/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4924 - accuracy: 0.7634\n",
            "Epoch 6/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4814 - accuracy: 0.7693\n",
            "Epoch 7/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4759 - accuracy: 0.7715\n",
            "Epoch 8/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4620 - accuracy: 0.7819\n",
            "Epoch 9/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4621 - accuracy: 0.7822\n",
            "Epoch 10/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4589 - accuracy: 0.7860\n",
            "Epoch 11/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4507 - accuracy: 0.7838\n",
            "Epoch 12/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4492 - accuracy: 0.7899\n",
            "Epoch 13/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4446 - accuracy: 0.7906\n",
            "Epoch 14/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4401 - accuracy: 0.7995\n",
            "Epoch 15/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4377 - accuracy: 0.7953\n",
            "Epoch 16/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4384 - accuracy: 0.7979\n",
            "Epoch 17/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4372 - accuracy: 0.8002\n",
            "Epoch 18/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4346 - accuracy: 0.8035\n",
            "Epoch 19/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4321 - accuracy: 0.7977\n",
            "Epoch 20/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4312 - accuracy: 0.7987\n",
            "Epoch 21/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4314 - accuracy: 0.8031\n",
            "Epoch 22/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4316 - accuracy: 0.8019\n",
            "Epoch 23/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4298 - accuracy: 0.8029\n",
            "Epoch 24/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4294 - accuracy: 0.8055\n",
            "Epoch 25/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4318 - accuracy: 0.8009\n",
            "Epoch 26/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4312 - accuracy: 0.8019\n",
            "Epoch 27/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4300 - accuracy: 0.8009\n",
            "Epoch 28/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4305 - accuracy: 0.8037\n",
            "Epoch 29/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4313 - accuracy: 0.8038\n",
            "Epoch 30/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4318 - accuracy: 0.8070\n",
            "Epoch 31/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4268 - accuracy: 0.8050\n",
            "Epoch 32/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4285 - accuracy: 0.8022\n",
            "Epoch 33/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4280 - accuracy: 0.8036\n",
            "Epoch 34/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4343 - accuracy: 0.8031\n",
            "Epoch 35/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4315 - accuracy: 0.8053\n",
            "Epoch 36/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4300 - accuracy: 0.8016\n",
            "Epoch 37/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4285 - accuracy: 0.8035\n",
            "Epoch 38/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4321 - accuracy: 0.8033\n",
            "Epoch 39/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4291 - accuracy: 0.8068\n",
            "Epoch 40/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4323 - accuracy: 0.8047\n",
            "Epoch 41/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4333 - accuracy: 0.8025\n",
            "Epoch 42/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4318 - accuracy: 0.8023\n",
            "Epoch 43/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4366 - accuracy: 0.8028\n",
            "Epoch 44/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4363 - accuracy: 0.8009\n",
            "Epoch 45/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4387 - accuracy: 0.8075\n",
            "Epoch 46/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4358 - accuracy: 0.8006\n",
            "Epoch 47/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4344 - accuracy: 0.8041\n",
            "Epoch 48/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4355 - accuracy: 0.8018\n",
            "Epoch 49/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4421 - accuracy: 0.8034\n",
            "Epoch 50/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4352 - accuracy: 0.8035\n",
            "Epoch 51/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4385 - accuracy: 0.8063\n",
            "Epoch 52/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4380 - accuracy: 0.8013\n",
            "Epoch 53/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4387 - accuracy: 0.8034\n",
            "Epoch 54/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4385 - accuracy: 0.8015\n",
            "Epoch 55/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4428 - accuracy: 0.7979\n",
            "Epoch 56/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4412 - accuracy: 0.8035\n",
            "Epoch 57/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4391 - accuracy: 0.8049\n",
            "Epoch 58/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4435 - accuracy: 0.8016\n",
            "Epoch 59/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4457 - accuracy: 0.8004\n",
            "Epoch 60/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4429 - accuracy: 0.7992\n",
            "Epoch 61/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4453 - accuracy: 0.7996\n",
            "Epoch 62/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4461 - accuracy: 0.7992\n",
            "Epoch 63/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4439 - accuracy: 0.7992\n",
            "Epoch 64/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4465 - accuracy: 0.8013\n",
            "Epoch 65/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4444 - accuracy: 0.8018\n",
            "Epoch 66/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4374 - accuracy: 0.8034\n",
            "Epoch 67/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4466 - accuracy: 0.7987\n",
            "Epoch 68/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4410 - accuracy: 0.8011\n",
            "Epoch 69/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4446 - accuracy: 0.7997\n",
            "Epoch 70/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4453 - accuracy: 0.7975\n",
            "Epoch 71/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4424 - accuracy: 0.7991\n",
            "Epoch 72/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4399 - accuracy: 0.7972\n",
            "Epoch 73/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4443 - accuracy: 0.8031\n",
            "Epoch 74/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4475 - accuracy: 0.7966\n",
            "Epoch 75/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4421 - accuracy: 0.8018\n",
            "Epoch 76/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4359 - accuracy: 0.8049\n",
            "Epoch 77/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4348 - accuracy: 0.8023\n",
            "Epoch 78/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4348 - accuracy: 0.8032\n",
            "Epoch 79/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4351 - accuracy: 0.8065\n",
            "Epoch 80/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4392 - accuracy: 0.8056\n",
            "Epoch 81/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4370 - accuracy: 0.8003\n",
            "Epoch 82/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4366 - accuracy: 0.8066\n",
            "Epoch 83/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4433 - accuracy: 0.8020\n",
            "Epoch 84/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4368 - accuracy: 0.8009\n",
            "Epoch 85/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4417 - accuracy: 0.8023\n",
            "Epoch 86/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4400 - accuracy: 0.8029\n",
            "Epoch 87/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4408 - accuracy: 0.8045\n",
            "Epoch 88/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4414 - accuracy: 0.8016\n",
            "Epoch 89/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4421 - accuracy: 0.8023\n",
            "Epoch 90/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4387 - accuracy: 0.8028\n",
            "Epoch 91/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4335 - accuracy: 0.8040\n",
            "Epoch 92/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4363 - accuracy: 0.8060\n",
            "Epoch 93/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4372 - accuracy: 0.8051\n",
            "Epoch 94/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4339 - accuracy: 0.8032\n",
            "Epoch 95/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4307 - accuracy: 0.8052\n",
            "Epoch 96/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4379 - accuracy: 0.8059\n",
            "Epoch 97/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4412 - accuracy: 0.8072\n",
            "Epoch 98/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4354 - accuracy: 0.8035\n",
            "Epoch 99/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4322 - accuracy: 0.8066\n",
            "Epoch 100/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4401 - accuracy: 0.8080\n",
            "Epoch 101/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4341 - accuracy: 0.8054\n",
            "Epoch 102/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4306 - accuracy: 0.8116\n",
            "Epoch 103/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4336 - accuracy: 0.8060\n",
            "Epoch 104/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4299 - accuracy: 0.8094\n",
            "Epoch 105/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4284 - accuracy: 0.8073\n",
            "Epoch 106/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4277 - accuracy: 0.8058\n",
            "Epoch 107/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4277 - accuracy: 0.8096\n",
            "Epoch 108/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4331 - accuracy: 0.8127\n",
            "Epoch 109/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4245 - accuracy: 0.8068\n",
            "Epoch 110/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4246 - accuracy: 0.8132\n",
            "Epoch 111/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4222 - accuracy: 0.8135\n",
            "Epoch 112/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4296 - accuracy: 0.8118\n",
            "Epoch 113/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4208 - accuracy: 0.8123\n",
            "Epoch 114/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4216 - accuracy: 0.8117\n",
            "Epoch 115/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4211 - accuracy: 0.8089\n",
            "Epoch 116/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4286 - accuracy: 0.8132\n",
            "Epoch 117/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4267 - accuracy: 0.8119\n",
            "Epoch 118/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4231 - accuracy: 0.8106\n",
            "Epoch 119/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4208 - accuracy: 0.8086\n",
            "Epoch 120/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4289 - accuracy: 0.8117\n",
            "Epoch 121/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4219 - accuracy: 0.8143\n",
            "Epoch 122/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4208 - accuracy: 0.8178\n",
            "Epoch 123/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4221 - accuracy: 0.8136\n",
            "Epoch 124/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4206 - accuracy: 0.8116\n",
            "Epoch 125/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4261 - accuracy: 0.8158\n",
            "Epoch 126/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4202 - accuracy: 0.8142\n",
            "Epoch 127/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4274 - accuracy: 0.8148\n",
            "Epoch 128/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4226 - accuracy: 0.8141\n",
            "Epoch 129/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4221 - accuracy: 0.8174\n",
            "Epoch 130/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4195 - accuracy: 0.8170\n",
            "Epoch 131/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4229 - accuracy: 0.8158\n",
            "Epoch 132/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4169 - accuracy: 0.8131\n",
            "Epoch 133/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4188 - accuracy: 0.8160\n",
            "Epoch 134/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4238 - accuracy: 0.8139\n",
            "Epoch 135/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4204 - accuracy: 0.8150\n",
            "Epoch 136/300\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.4268 - accuracy: 0.8136\n",
            "Epoch 137/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4234 - accuracy: 0.8154\n",
            "Epoch 138/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4233 - accuracy: 0.8111\n",
            "Epoch 139/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4264 - accuracy: 0.8126\n",
            "Epoch 140/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4303 - accuracy: 0.8166\n",
            "Epoch 141/300\n",
            "937/937 [==============================] - 1s 2ms/step - loss: 0.4213 - accuracy: 0.8127\n",
            "Epoch 142/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4236 - accuracy: 0.8158\n",
            "Epoch 143/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4326 - accuracy: 0.8160\n",
            "Epoch 144/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4196 - accuracy: 0.8156\n",
            "Epoch 145/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4166 - accuracy: 0.8176\n",
            "Epoch 146/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4154 - accuracy: 0.8181\n",
            "Epoch 147/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4253 - accuracy: 0.8143\n",
            "Epoch 148/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4261 - accuracy: 0.8170\n",
            "Epoch 149/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4290 - accuracy: 0.8168\n",
            "Epoch 150/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4383 - accuracy: 0.8126\n",
            "Epoch 151/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4305 - accuracy: 0.8160\n",
            "Epoch 152/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4269 - accuracy: 0.8142\n",
            "Epoch 153/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4331 - accuracy: 0.8150\n",
            "Epoch 154/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4175 - accuracy: 0.8164\n",
            "Epoch 155/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4277 - accuracy: 0.8177\n",
            "Epoch 156/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4244 - accuracy: 0.8173\n",
            "Epoch 157/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4218 - accuracy: 0.8162\n",
            "Epoch 158/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4162 - accuracy: 0.8162\n",
            "Epoch 159/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4268 - accuracy: 0.8172\n",
            "Epoch 160/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4182 - accuracy: 0.8182\n",
            "Epoch 161/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4271 - accuracy: 0.8175\n",
            "Epoch 162/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4190 - accuracy: 0.8182\n",
            "Epoch 163/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4244 - accuracy: 0.8161\n",
            "Epoch 164/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4170 - accuracy: 0.8204\n",
            "Epoch 165/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4216 - accuracy: 0.8172\n",
            "Epoch 166/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4197 - accuracy: 0.8158\n",
            "Epoch 167/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4268 - accuracy: 0.8175\n",
            "Epoch 168/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4202 - accuracy: 0.8209\n",
            "Epoch 169/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4252 - accuracy: 0.8176\n",
            "Epoch 170/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4284 - accuracy: 0.8173\n",
            "Epoch 171/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4220 - accuracy: 0.8146\n",
            "Epoch 172/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4274 - accuracy: 0.8183\n",
            "Epoch 173/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4358 - accuracy: 0.8207\n",
            "Epoch 174/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4220 - accuracy: 0.8170\n",
            "Epoch 175/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4237 - accuracy: 0.8224\n",
            "Epoch 176/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4252 - accuracy: 0.8189\n",
            "Epoch 177/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4222 - accuracy: 0.8168\n",
            "Epoch 178/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4279 - accuracy: 0.8173\n",
            "Epoch 179/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4221 - accuracy: 0.8168\n",
            "Epoch 180/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4225 - accuracy: 0.8177\n",
            "Epoch 181/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4178 - accuracy: 0.8176\n",
            "Epoch 182/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4199 - accuracy: 0.8156\n",
            "Epoch 183/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4278 - accuracy: 0.8194\n",
            "Epoch 184/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4086 - accuracy: 0.8196\n",
            "Epoch 185/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4289 - accuracy: 0.8161\n",
            "Epoch 186/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4148 - accuracy: 0.8235\n",
            "Epoch 187/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4210 - accuracy: 0.8174\n",
            "Epoch 188/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4155 - accuracy: 0.8198\n",
            "Epoch 189/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4153 - accuracy: 0.8185\n",
            "Epoch 190/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4186 - accuracy: 0.8227\n",
            "Epoch 191/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4255 - accuracy: 0.8191\n",
            "Epoch 192/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4205 - accuracy: 0.8228\n",
            "Epoch 193/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4251 - accuracy: 0.8177\n",
            "Epoch 194/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4218 - accuracy: 0.8151\n",
            "Epoch 195/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4135 - accuracy: 0.8166\n",
            "Epoch 196/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4316 - accuracy: 0.8207\n",
            "Epoch 197/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4213 - accuracy: 0.8160\n",
            "Epoch 198/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4257 - accuracy: 0.8188\n",
            "Epoch 199/300\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 0.4235 - accuracy: 0.8198\n",
            "Epoch 200/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4256 - accuracy: 0.8192\n",
            "Epoch 201/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4337 - accuracy: 0.8231\n",
            "Epoch 202/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4249 - accuracy: 0.8215\n",
            "Epoch 203/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4156 - accuracy: 0.8174\n",
            "Epoch 204/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4237 - accuracy: 0.8168\n",
            "Epoch 205/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4226 - accuracy: 0.8197\n",
            "Epoch 206/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4156 - accuracy: 0.8128\n",
            "Epoch 207/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4252 - accuracy: 0.8177\n",
            "Epoch 208/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4162 - accuracy: 0.8167\n",
            "Epoch 209/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4266 - accuracy: 0.8205\n",
            "Epoch 210/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4217 - accuracy: 0.8204\n",
            "Epoch 211/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4229 - accuracy: 0.8188\n",
            "Epoch 212/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4178 - accuracy: 0.8170\n",
            "Epoch 213/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4206 - accuracy: 0.8181\n",
            "Epoch 214/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4168 - accuracy: 0.8181\n",
            "Epoch 215/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4300 - accuracy: 0.8219\n",
            "Epoch 216/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8225\n",
            "Epoch 217/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4198 - accuracy: 0.8237\n",
            "Epoch 218/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4371 - accuracy: 0.8205\n",
            "Epoch 219/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4196 - accuracy: 0.8205\n",
            "Epoch 220/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4265 - accuracy: 0.8182\n",
            "Epoch 221/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4198 - accuracy: 0.8197\n",
            "Epoch 222/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4190 - accuracy: 0.8196\n",
            "Epoch 223/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4216 - accuracy: 0.8214\n",
            "Epoch 224/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4251 - accuracy: 0.8211\n",
            "Epoch 225/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4181 - accuracy: 0.8213\n",
            "Epoch 226/300\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 0.4208 - accuracy: 0.8201\n",
            "Epoch 227/300\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 0.4159 - accuracy: 0.8191\n",
            "Epoch 228/300\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 0.4218 - accuracy: 0.8205\n",
            "Epoch 229/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4180 - accuracy: 0.8190\n",
            "Epoch 230/300\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 0.4188 - accuracy: 0.8193\n",
            "Epoch 231/300\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.4351 - accuracy: 0.8199\n",
            "Epoch 232/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4172 - accuracy: 0.8242\n",
            "Epoch 233/300\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.4138 - accuracy: 0.8216\n",
            "Epoch 234/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4159 - accuracy: 0.8204\n",
            "Epoch 235/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4198 - accuracy: 0.8183\n",
            "Epoch 236/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4179 - accuracy: 0.8272\n",
            "Epoch 237/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4187 - accuracy: 0.8230\n",
            "Epoch 238/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4117 - accuracy: 0.8241\n",
            "Epoch 239/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4120 - accuracy: 0.8206\n",
            "Epoch 240/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4219 - accuracy: 0.8197\n",
            "Epoch 241/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4223 - accuracy: 0.8212\n",
            "Epoch 242/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4226 - accuracy: 0.8197\n",
            "Epoch 243/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4147 - accuracy: 0.8247\n",
            "Epoch 244/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4210 - accuracy: 0.8195\n",
            "Epoch 245/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4189 - accuracy: 0.8256\n",
            "Epoch 246/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4152 - accuracy: 0.8240\n",
            "Epoch 247/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4091 - accuracy: 0.8214\n",
            "Epoch 248/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4127 - accuracy: 0.8223\n",
            "Epoch 249/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4129 - accuracy: 0.8242\n",
            "Epoch 250/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4215 - accuracy: 0.8224\n",
            "Epoch 251/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4124 - accuracy: 0.8238\n",
            "Epoch 252/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4209 - accuracy: 0.8211\n",
            "Epoch 253/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4217 - accuracy: 0.8226\n",
            "Epoch 254/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4175 - accuracy: 0.8241\n",
            "Epoch 255/300\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.4207 - accuracy: 0.8214\n",
            "Epoch 256/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4069 - accuracy: 0.8243\n",
            "Epoch 257/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4163 - accuracy: 0.8224\n",
            "Epoch 258/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4250 - accuracy: 0.8202\n",
            "Epoch 259/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4136 - accuracy: 0.8248\n",
            "Epoch 260/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4196 - accuracy: 0.8271\n",
            "Epoch 261/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4187 - accuracy: 0.8217\n",
            "Epoch 262/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4111 - accuracy: 0.8249\n",
            "Epoch 263/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4225 - accuracy: 0.8229\n",
            "Epoch 264/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4235 - accuracy: 0.8253\n",
            "Epoch 265/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4144 - accuracy: 0.8251\n",
            "Epoch 266/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4189 - accuracy: 0.8245\n",
            "Epoch 267/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4084 - accuracy: 0.8256\n",
            "Epoch 268/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4143 - accuracy: 0.8259\n",
            "Epoch 269/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4109 - accuracy: 0.8211\n",
            "Epoch 270/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4226 - accuracy: 0.8224\n",
            "Epoch 271/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4171 - accuracy: 0.8253\n",
            "Epoch 272/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4087 - accuracy: 0.8233\n",
            "Epoch 273/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4100 - accuracy: 0.8254\n",
            "Epoch 274/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4095 - accuracy: 0.8284\n",
            "Epoch 275/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4171 - accuracy: 0.8235\n",
            "Epoch 276/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4149 - accuracy: 0.8199\n",
            "Epoch 277/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4082 - accuracy: 0.8233\n",
            "Epoch 278/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4105 - accuracy: 0.8249\n",
            "Epoch 279/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4106 - accuracy: 0.8253\n",
            "Epoch 280/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4052 - accuracy: 0.8260\n",
            "Epoch 281/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4080 - accuracy: 0.8258\n",
            "Epoch 282/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4153 - accuracy: 0.8224\n",
            "Epoch 283/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4150 - accuracy: 0.8268\n",
            "Epoch 284/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4048 - accuracy: 0.8217\n",
            "Epoch 285/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4169 - accuracy: 0.8220\n",
            "Epoch 286/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4123 - accuracy: 0.8249\n",
            "Epoch 287/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4108 - accuracy: 0.8257\n",
            "Epoch 288/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4209 - accuracy: 0.8239\n",
            "Epoch 289/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4136 - accuracy: 0.8254\n",
            "Epoch 290/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4029 - accuracy: 0.8273\n",
            "Epoch 291/300\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 0.4078 - accuracy: 0.8232\n",
            "Epoch 292/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4122 - accuracy: 0.8226\n",
            "Epoch 293/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4033 - accuracy: 0.8266\n",
            "Epoch 294/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4095 - accuracy: 0.8235\n",
            "Epoch 295/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4093 - accuracy: 0.8244\n",
            "Epoch 296/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4064 - accuracy: 0.8233\n",
            "Epoch 297/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4129 - accuracy: 0.8296\n",
            "Epoch 298/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4152 - accuracy: 0.8237\n",
            "Epoch 299/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4094 - accuracy: 0.8276\n",
            "Epoch 300/300\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.4091 - accuracy: 0.8248\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbbe422c990>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Neural_Network1.predict(X_test)"
      ],
      "metadata": {
        "id": "dszilygZdiCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = np.sum(y_pred == y_test_encoded) / len(y_test_encoded)"
      ],
      "metadata": {
        "id": "HD8f7NoUZuLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jByuoYuqdw52",
        "outputId": "cdd4f665-94a1-48c1-cfb8-457f9e2b06bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96.6982307500623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Neural_Network2 = CreateModel(32,64)\n",
        "Neural_Network2.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
        "Neural_Network2.fit(X_train,y_train_encoded,epochs=300,batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqouFRN1d11U",
        "outputId": "c7740782-2c77-44b0-987b-936fdd7ca7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "147/147 [==============================] - 1s 2ms/step - loss: 1.4383 - accuracy: 0.6361\n",
            "Epoch 2/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.7911 - accuracy: 0.6632\n",
            "Epoch 3/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.7370 - accuracy: 0.6698\n",
            "Epoch 4/300\n",
            "147/147 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.6869\n",
            "Epoch 5/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.6364 - accuracy: 0.7000\n",
            "Epoch 6/300\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 0.5987 - accuracy: 0.7122\n",
            "Epoch 7/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.5726 - accuracy: 0.7183\n",
            "Epoch 8/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.5497 - accuracy: 0.7317\n",
            "Epoch 9/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7365\n",
            "Epoch 10/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7425\n",
            "Epoch 11/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.5190 - accuracy: 0.7452\n",
            "Epoch 12/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7485\n",
            "Epoch 13/300\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 0.4932 - accuracy: 0.7588\n",
            "Epoch 14/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.4963 - accuracy: 0.7571\n",
            "Epoch 15/300\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 0.4791 - accuracy: 0.7641\n",
            "Epoch 16/300\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 0.4744 - accuracy: 0.7676\n",
            "Epoch 17/300\n",
            "147/147 [==============================] - 1s 5ms/step - loss: 0.4703 - accuracy: 0.7711\n",
            "Epoch 18/300\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 0.4710 - accuracy: 0.7697\n",
            "Epoch 19/300\n",
            "147/147 [==============================] - 1s 7ms/step - loss: 0.4591 - accuracy: 0.7772\n",
            "Epoch 20/300\n",
            "147/147 [==============================] - 1s 9ms/step - loss: 0.4599 - accuracy: 0.7751\n",
            "Epoch 21/300\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 0.4559 - accuracy: 0.7759\n",
            "Epoch 22/300\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 0.4564 - accuracy: 0.7785\n",
            "Epoch 23/300\n",
            "147/147 [==============================] - 1s 4ms/step - loss: 0.4486 - accuracy: 0.7835\n",
            "Epoch 24/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7825\n",
            "Epoch 25/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7891\n",
            "Epoch 26/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.7892\n",
            "Epoch 27/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.7923\n",
            "Epoch 28/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.7905\n",
            "Epoch 29/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.7897\n",
            "Epoch 30/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.7942\n",
            "Epoch 31/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.7954\n",
            "Epoch 32/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.7978\n",
            "Epoch 33/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4240 - accuracy: 0.7985\n",
            "Epoch 34/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.7952\n",
            "Epoch 35/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.7993\n",
            "Epoch 36/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8017\n",
            "Epoch 37/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8012\n",
            "Epoch 38/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8032\n",
            "Epoch 39/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8022\n",
            "Epoch 40/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.8015\n",
            "Epoch 41/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8075\n",
            "Epoch 42/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8053\n",
            "Epoch 43/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8053\n",
            "Epoch 44/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8047\n",
            "Epoch 45/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8034\n",
            "Epoch 46/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8060\n",
            "Epoch 47/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8069\n",
            "Epoch 48/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8072\n",
            "Epoch 49/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8088\n",
            "Epoch 50/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8109\n",
            "Epoch 51/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8039\n",
            "Epoch 52/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8085\n",
            "Epoch 53/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8080\n",
            "Epoch 54/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8089\n",
            "Epoch 55/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8152\n",
            "Epoch 56/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8118\n",
            "Epoch 57/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8101\n",
            "Epoch 58/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8135\n",
            "Epoch 59/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8164\n",
            "Epoch 60/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8176\n",
            "Epoch 61/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8109\n",
            "Epoch 62/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8163\n",
            "Epoch 63/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3877 - accuracy: 0.8127\n",
            "Epoch 64/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8128\n",
            "Epoch 65/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8144\n",
            "Epoch 66/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8136\n",
            "Epoch 67/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8179\n",
            "Epoch 68/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3877 - accuracy: 0.8132\n",
            "Epoch 69/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8149\n",
            "Epoch 70/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8199\n",
            "Epoch 71/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8154\n",
            "Epoch 72/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.8175\n",
            "Epoch 73/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8178\n",
            "Epoch 74/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8150\n",
            "Epoch 75/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8196\n",
            "Epoch 76/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3779 - accuracy: 0.8220\n",
            "Epoch 77/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8191\n",
            "Epoch 78/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8189\n",
            "Epoch 79/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.8184\n",
            "Epoch 80/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8216\n",
            "Epoch 81/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8192\n",
            "Epoch 82/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8205\n",
            "Epoch 83/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8188\n",
            "Epoch 84/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.8168\n",
            "Epoch 85/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8197\n",
            "Epoch 86/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8197\n",
            "Epoch 87/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8277\n",
            "Epoch 88/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8232\n",
            "Epoch 89/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8233\n",
            "Epoch 90/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8257\n",
            "Epoch 91/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8230\n",
            "Epoch 92/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3659 - accuracy: 0.8262\n",
            "Epoch 93/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8221\n",
            "Epoch 94/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.8241\n",
            "Epoch 95/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.8241\n",
            "Epoch 96/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8210\n",
            "Epoch 97/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3641 - accuracy: 0.8262\n",
            "Epoch 98/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3632 - accuracy: 0.8242\n",
            "Epoch 99/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8245\n",
            "Epoch 100/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.8272\n",
            "Epoch 101/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.3652 - accuracy: 0.8269\n",
            "Epoch 102/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.8199\n",
            "Epoch 103/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8289\n",
            "Epoch 104/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8307\n",
            "Epoch 105/300\n",
            "147/147 [==============================] - 0s 3ms/step - loss: 0.3616 - accuracy: 0.8323\n",
            "Epoch 106/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.8289\n",
            "Epoch 107/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8316\n",
            "Epoch 108/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8268\n",
            "Epoch 109/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8300\n",
            "Epoch 110/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8336\n",
            "Epoch 111/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3575 - accuracy: 0.8309\n",
            "Epoch 112/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3564 - accuracy: 0.8304\n",
            "Epoch 113/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8266\n",
            "Epoch 114/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8291\n",
            "Epoch 115/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8334\n",
            "Epoch 116/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8275\n",
            "Epoch 117/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8347\n",
            "Epoch 118/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.8287\n",
            "Epoch 119/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.8340\n",
            "Epoch 120/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8279\n",
            "Epoch 121/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.8360\n",
            "Epoch 122/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8294\n",
            "Epoch 123/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8324\n",
            "Epoch 124/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.8324\n",
            "Epoch 125/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8315\n",
            "Epoch 126/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8284\n",
            "Epoch 127/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8339\n",
            "Epoch 128/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8356\n",
            "Epoch 129/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8339\n",
            "Epoch 130/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8339\n",
            "Epoch 131/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8366\n",
            "Epoch 132/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8354\n",
            "Epoch 133/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8372\n",
            "Epoch 134/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.8376\n",
            "Epoch 135/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8389\n",
            "Epoch 136/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8367\n",
            "Epoch 137/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8400\n",
            "Epoch 138/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8395\n",
            "Epoch 139/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8382\n",
            "Epoch 140/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8355\n",
            "Epoch 141/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8377\n",
            "Epoch 142/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8386\n",
            "Epoch 143/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8371\n",
            "Epoch 144/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8374\n",
            "Epoch 145/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8361\n",
            "Epoch 146/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3410 - accuracy: 0.8396\n",
            "Epoch 147/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8369\n",
            "Epoch 148/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.8403\n",
            "Epoch 149/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8428\n",
            "Epoch 150/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3419 - accuracy: 0.8419\n",
            "Epoch 151/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8393\n",
            "Epoch 152/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8432\n",
            "Epoch 153/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8360\n",
            "Epoch 154/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8443\n",
            "Epoch 155/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8390\n",
            "Epoch 156/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8404\n",
            "Epoch 157/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8426\n",
            "Epoch 158/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8434\n",
            "Epoch 159/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8417\n",
            "Epoch 160/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8440\n",
            "Epoch 161/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8420\n",
            "Epoch 162/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8426\n",
            "Epoch 163/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8457\n",
            "Epoch 164/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8453\n",
            "Epoch 165/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8481\n",
            "Epoch 166/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8437\n",
            "Epoch 167/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8429\n",
            "Epoch 168/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8419\n",
            "Epoch 169/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8410\n",
            "Epoch 170/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8415\n",
            "Epoch 171/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8470\n",
            "Epoch 172/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8455\n",
            "Epoch 173/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8471\n",
            "Epoch 174/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8505\n",
            "Epoch 175/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8448\n",
            "Epoch 176/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8416\n",
            "Epoch 177/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8426\n",
            "Epoch 178/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8468\n",
            "Epoch 179/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8451\n",
            "Epoch 180/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8448\n",
            "Epoch 181/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8497\n",
            "Epoch 182/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3321 - accuracy: 0.8472\n",
            "Epoch 183/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8451\n",
            "Epoch 184/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8448\n",
            "Epoch 185/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.8475\n",
            "Epoch 186/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8429\n",
            "Epoch 187/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8496\n",
            "Epoch 188/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8449\n",
            "Epoch 189/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.8499\n",
            "Epoch 190/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8458\n",
            "Epoch 191/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.8500\n",
            "Epoch 192/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8476\n",
            "Epoch 193/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.8484\n",
            "Epoch 194/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8458\n",
            "Epoch 195/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8492\n",
            "Epoch 196/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8510\n",
            "Epoch 197/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8498\n",
            "Epoch 198/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.8486\n",
            "Epoch 199/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8528\n",
            "Epoch 200/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8470\n",
            "Epoch 201/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8472\n",
            "Epoch 202/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8453\n",
            "Epoch 203/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8494\n",
            "Epoch 204/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8505\n",
            "Epoch 205/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8523\n",
            "Epoch 206/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8508\n",
            "Epoch 207/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8508\n",
            "Epoch 208/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8494\n",
            "Epoch 209/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8496\n",
            "Epoch 210/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.8487\n",
            "Epoch 211/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8513\n",
            "Epoch 212/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3250 - accuracy: 0.8528\n",
            "Epoch 213/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8494\n",
            "Epoch 214/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8500\n",
            "Epoch 215/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8523\n",
            "Epoch 216/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8540\n",
            "Epoch 217/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3263 - accuracy: 0.8513\n",
            "Epoch 218/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8534\n",
            "Epoch 219/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 0.8506\n",
            "Epoch 220/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8533\n",
            "Epoch 221/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8524\n",
            "Epoch 222/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3260 - accuracy: 0.8527\n",
            "Epoch 223/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8482\n",
            "Epoch 224/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8544\n",
            "Epoch 225/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.8497\n",
            "Epoch 226/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8520\n",
            "Epoch 227/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8520\n",
            "Epoch 228/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8510\n",
            "Epoch 229/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.8554\n",
            "Epoch 230/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.8505\n",
            "Epoch 231/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8508\n",
            "Epoch 232/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8511\n",
            "Epoch 233/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8558\n",
            "Epoch 234/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8525\n",
            "Epoch 235/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3234 - accuracy: 0.8539\n",
            "Epoch 236/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3256 - accuracy: 0.8511\n",
            "Epoch 237/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8523\n",
            "Epoch 238/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8556\n",
            "Epoch 239/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8538\n",
            "Epoch 240/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8526\n",
            "Epoch 241/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8539\n",
            "Epoch 242/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8549\n",
            "Epoch 243/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8537\n",
            "Epoch 244/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8510\n",
            "Epoch 245/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8545\n",
            "Epoch 246/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3260 - accuracy: 0.8512\n",
            "Epoch 247/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8568\n",
            "Epoch 248/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3237 - accuracy: 0.8523\n",
            "Epoch 249/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8527\n",
            "Epoch 250/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8537\n",
            "Epoch 251/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8551\n",
            "Epoch 252/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8571\n",
            "Epoch 253/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.8560\n",
            "Epoch 254/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3234 - accuracy: 0.8518\n",
            "Epoch 255/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8498\n",
            "Epoch 256/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8550\n",
            "Epoch 257/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8543\n",
            "Epoch 258/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8547\n",
            "Epoch 259/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8566\n",
            "Epoch 260/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8568\n",
            "Epoch 261/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8546\n",
            "Epoch 262/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8556\n",
            "Epoch 263/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3204 - accuracy: 0.8577\n",
            "Epoch 264/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8575\n",
            "Epoch 265/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.8543\n",
            "Epoch 266/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8578\n",
            "Epoch 267/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8530\n",
            "Epoch 268/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8549\n",
            "Epoch 269/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8561\n",
            "Epoch 270/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8567\n",
            "Epoch 271/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8523\n",
            "Epoch 272/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8543\n",
            "Epoch 273/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8538\n",
            "Epoch 274/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.8530\n",
            "Epoch 275/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8573\n",
            "Epoch 276/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.8544\n",
            "Epoch 277/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8529\n",
            "Epoch 278/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.8529\n",
            "Epoch 279/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8550\n",
            "Epoch 280/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.8578\n",
            "Epoch 281/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8567\n",
            "Epoch 282/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8544\n",
            "Epoch 283/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8550\n",
            "Epoch 284/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8533\n",
            "Epoch 285/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8538\n",
            "Epoch 286/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.8581\n",
            "Epoch 287/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8538\n",
            "Epoch 288/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3171 - accuracy: 0.8590\n",
            "Epoch 289/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8576\n",
            "Epoch 290/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3172 - accuracy: 0.8559\n",
            "Epoch 291/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8536\n",
            "Epoch 292/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8569\n",
            "Epoch 293/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8567\n",
            "Epoch 294/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.8589\n",
            "Epoch 295/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8561\n",
            "Epoch 296/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8586\n",
            "Epoch 297/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8536\n",
            "Epoch 298/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3162 - accuracy: 0.8559\n",
            "Epoch 299/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8554\n",
            "Epoch 300/300\n",
            "147/147 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbbe3473790>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZbYcExneTY2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}